# Linear algebra

## Variance Formula (Vector Form)

**Front:** What is the formula for sample variance when data is in vector form?
**Back:**
Given $n$ samples of a single feature in vector $\mathbf{x} = [x_1, x_2, ..., x_n]^T$ with mean $\mu = \frac{1}{n}\sum_{i=1}^n x_i$:

$$
\text{Var}(\mathbf{x}) = \frac{1}{n-1} (\mathbf{x} - \mu\mathbf{1})^T (\mathbf{x} - \mu\mathbf{1}) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu)^2
$$

**Dimensions:** $\mathbf{x}$ is $(n \times 1)$, $\mathbf{1}$ is $(n \times 1)$, $(\mathbf{x} - \mu\mathbf{1})$ is $(n \times 1)$, so result is scalar $(1 \times 1)$.

## Covariance Formula (Two Variables)

**Front:** What is the formula for sample covariance between two variables?
**Back:**
Given two vectors $\mathbf{x} = [x_1, ..., x_n]^T$ and $\mathbf{y} = [y_1, ..., y_n]^T$ with means $\mu_x$ and $\mu_y$:

$$
\text{Cov}(\mathbf{x}, \mathbf{y}) = \frac{1}{n-1} (\mathbf{x} - \mu_x\mathbf{1})^T (\mathbf{y} - \mu_y\mathbf{1}) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu_x)(y_i - \mu_y)
$$

**Dimensions:** Both centered vectors are $(n \times 1)$, their dot product gives scalar $(1 \times 1)$.

## Covariance Matrix (Multiple Features)

**Front:** How do you compute the sample covariance matrix for $m$ features?
**Back:**
Given data matrix $\mathbf{X}$ of size $(n \times m)$ where columns are features, with column means $\mathbf{\mu} = [\mu_1, \mu_2, ..., \mu_m]$:

1. Center the data: $\mathbf{\bar{X}} = \mathbf{X} - \mathbf{1}_n \mathbf{\mu}^T$
2. Compute covariance: $\mathbf{S} = \frac{1}{n-1} \mathbf{\bar{X}}^T \mathbf{\bar{X}}$

**Dimensions Critical:**

- $\mathbf{X}$: $(n \times m)$ (n samples, m features)
- $\mathbf{1}_n$: $(n \times 1)$ column vector of ones
- $\mathbf{\mu}$: $(1 \times m)$ row vector of means
- $\mathbf{\bar{X}}$: $(n \times m)$ centered data
- $\mathbf{\bar{X}}^T$: $(m \times n)$ transpose
- $\mathbf{S} = \mathbf{\bar{X}}^T \mathbf{\bar{X}}$: $(m \times m)$ covariance matrix

**Key Insight:** $\mathbf{\bar{X}}^T \mathbf{\bar{X}}$ gives feature-by-feature relationships, not sample-by-sample.

## Dimensionality Example: 5×3 Data

**Front:** For a 5×3 data matrix (5 samples, 3 features), what are the dimensions at each step?
**Back:**

1. $\mathbf{X}$: $(5 \times 3)$
2. $\mathbf{\mu}$ (row means): $(1 \times 3)$ or as column vector: $(3 \times 1)$
3. $\mathbf{1}_5$: $(5 \times 1)$ column of ones
4. $\mathbf{1}_5 \mathbf{\mu}^T$: $(5 \times 1) \times (1 \times 3) = (5 \times 3)$
5. $\mathbf{\bar{X}} = \mathbf{X} - \mathbf{1}_5 \mathbf{\mu}^T$: $(5 \times 3) - (5 \times 3) = (5 \times 3)$
6. $\mathbf{\bar{X}}^T$: $(3 \times 5)$
7. $\mathbf{\bar{X}}^T \mathbf{\bar{X}}$: $(3 \times 5) \times (5 \times 3) = (3 \times 3)$ covariance matrix
8. $\frac{1}{n-1}\mathbf{\bar{X}}^T \mathbf{\bar{X}}$: $(3 \times 3)$ sample covariance matrix

**Wrong Order Warning:** $\mathbf{\bar{X}} \mathbf{\bar{X}}^T$ gives $(5 \times 5)$ sample similarity matrix, NOT covariance!

# LDA

## Use Cases	

**Front:** What are the two primary use cases of Linear Discriminant Analysis (LDA)?
**Back:**

1. **Supervised Dimensionality Reduction:** Projects data onto a lower-dimensional space while preserving as much class-discriminatory information as possible. The new axes are linear combinations of original features.
2. **Classification:** Provides a simple, linear classifier by defining a decision boundary (hyperplane) based on the projected class means and shared covariance.

## LDA Definition & Goal

**Front:** What is the core goal of Linear Discriminant Analysis (LDA)?
**Back:**
LDA is a supervised method. Its goal is to find a projection (a direction or set of axes) that **maximizes the separation between the means of different classes** while **minimizing the variance (scatter) within each class** in the projected space.

$$
\text{Goal: Find } \mathbf{w} \text{ that maximizes class separation relative to class compactness.}
$$

## Geometric Concept: Projection in Detail

**Front:** In LDA, given $\mathbf{x} \in \mathbb{R}^d$ and $\mathbf{w} \in \mathbb{R}^d$, what does the operation $\mathbf{w}^T\mathbf{x}$ compute geometrically? What are the dimensions of the input and output?
**Back:**
Geometrically, $\mathbf{w}^T\mathbf{x}$ computes the **scalar projection** of the data point $\mathbf{x}$ onto the vector $\mathbf{w}$. This operation:

1. Takes a high-dimensional point $\mathbf{x}$ in the original feature space (dimension $d$).
2. Projects it onto the 1-dimensional line defined by the direction of $\mathbf{w}$.
3. Outputs a **single scalar value** representing the coordinate of that projection along the line.

- **Dimensions:** $\mathbf{x}$: $(d \times 1)$, $\mathbf{w}$: $(d \times 1)$, $\mathbf{w}^T\mathbf{x}$: $(1 \times 1)$ (a scalar)
- If $\mathbf{w}$ is a unit vector ($\|\mathbf{w}\| = 1$), then $\mathbf{w}^T\mathbf{x}$ equals the **signed length/magnitude** of the orthogonal projection.
- The sign indicates on which "side" of the origin (relative to $\mathbf{w}$'s direction) the projection lies.
- This projection reduces dimensionality from $d$ to $1$, which is the core of LDA's dimensionality reduction aspect.

**Visual:** Imagine a 2D cloud of points. $\mathbf{w}$ defines a line through the origin. Each point $\mathbf{x}$ is "dropped" perpendicularly onto this line. $\mathbf{w}^T\mathbf{x}$ is the coordinate of that landing point along the line.

$$
\text{Projection: } \mathbf{w}^T\mathbf{x} = \|\mathbf{w}\|\|\mathbf{x}\|\cos\theta \quad\text{(where } \theta \text{ is the angle between } \mathbf{w} \text{ and } \mathbf{x})
$$

## Visualizing the Optimal W Vector

**Front:** In a 2D scatter plot of two classes, how would you visually identify a good projection vector $\mathbf{w}$ for LDA?
**Back:**
A good $\mathbf{w}$ vector, when the data is projected onto the line in its direction, will result in: 1) Two clusters of projected points (one per class) that are **far apart from each other** (large inter-class distance). 2) Each cluster being as **tight or compact** as possible (small intra-class spread).

## First Rule: Maximizing Mean Separation

**Front:** What is the first, naive objective function for LDA, and what is its major flaw?
**Back:**
The naive rule aims to **maximize the squared distance between projected class means**.

$$
\underset{\mathbf{w}}{\argmax} (\tilde{\mu}_1 - \tilde{\mu}_2)^2 \quad \text{, where } \quad \tilde{\mu}_c = \mathbf{w}^T\mathbf{\mu}_c
$$

**Major Flaw:** It doesn't consider within-class scatter. The solution could simply blow up the scale of $\mathbf{w}$, and it's sensitive to classes with large internal variance.

$$
\underset{\mathbf{w}}{\arg\max} (\mathbf{w}^T\mathbf{\mu}_1 - \mathbf{w}^T\mathbf{\mu}_2)^2 = \underset{\mathbf{w}}{\arg\max} [\mathbf{w}^T(\mathbf{\mu}_1-\mathbf{\mu}_2)(\mathbf{\mu}_1-\mathbf{\mu}_2)^T\mathbf{w}]
$$

## Pros and Cons of the First Rule

**Front:** List one pro and one critical con of the LDA objective $\arg\max (\tilde{\mu}_1 - \tilde{\mu}_2)^2$.
**Back:**
**Pro:** Simple and intuitive. Directly targets class separation.
**Critical Con:** It is **not scale-invariant**. Multiplying $\mathbf{w}$ by a scalar increases the objective without actually improving the separation relative to the data's natural spread. It ignores within-class variance, leading to poor projections if classes have high scatter.

## Second Rule: Fisher's Criterion

**Front:** What is Fisher's Linear Discriminant criterion (J(w)), and how does it fix the first rule's flaw?
**Back:**
Fisher's criterion introduces a normalization term using within-class scatter.

$$
J(\mathbf{w}) = \frac{(\tilde{\mu}_1 - \tilde{\mu}_2)^2}{\tilde{s}_1^2 + \tilde{s}_2^2}
$$

It fixes the scale problem by maximizing the ratio of **between-class variance** to **within-class variance**. The denominator $\tilde{s}_c^2$ measures the scatter (variance) of projected points within class $c$.

## Path to the Generalized Objective Function

**Front:** How do we get from $J(\mathbf{w}) = \frac{(\tilde{\mu}_1 - \tilde{\mu}_2)^2}{\tilde{s}_1^2 + \tilde{s}_2^2}$ to the standard form $J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}$?
**Back:**

1. **Between-class scatter (numerator):** $(\mathbf{w}^T\mathbf{\mu}_1 - \mathbf{w}^T\mathbf{\mu}_2)^2 = \mathbf{w}^T(\mathbf{\mu}_1-\mathbf{\mu}_2)(\mathbf{\mu}_1-\mathbf{\mu}_2)^T\mathbf{w} = \mathbf{w}^T \mathbf{S}_B \mathbf{w}$, where $\mathbf{S}_B$ is the between-class scatter matrix.
2. **Within-class scatter (denominator):** $\tilde{s}_1^2 + \tilde{s}_2^2 = \sum_{n \in C_1}(\mathbf{w}^T\mathbf{x}_n - \tilde{\mu}_1)^2 + \sum_{n \in C_2}(\mathbf{w}^T\mathbf{x}_n - \tilde{\mu}_2)^2 = \mathbf{w}^T \mathbf{S}_W \mathbf{w}$, where $\mathbf{S}_W$ is the total within-class scatter matrix.

## Fisher's Objective Function

**Front:** What is the standard matrix form of Fisher's Linear Discriminant objective function?
**Back:**
The objective is to maximize the ratio of between-class scatter to within-class scatter in the projected space:

* *make the projections on w as **far** as possible and as **compact** as possible*

$$
J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}
$$

where $\mathbf{S}_B$ is the between-class scatter matrix and $\mathbf{S}_W$ is the within-class scatter matrix.

## Optimizing the Ratio

**Front:** To find the optimal projection vector $\mathbf{w}$, what mathematical problem arises from maximizing $J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}$?
**Back:**
Taking the derivative and setting it to zero leads to the generalized eigenvalue problem:

$$
\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w} \\
(\mathbf{S}_W^{-1} \mathbf{S}_B) \mathbf{w} = \lambda \mathbf{w}
$$

* This means the optimal $\mathbf{w}$ is an eigenvector of $\mathbf{S}_W^{-1}\mathbf{S}_B$.

## Two-Class Special Case

**Front:** In the two-class case, what simpler form does the between-class scatter matrix $\mathbf{S}_B$ take?
**Back:**
For two classes, $\mathbf{S}_B$ simplifies to an outer product:

$$
\mathbf{S}_B = (\mathbf{\mu}_1 - \mathbf{\mu}_2)(\mathbf{\mu}_1 - \mathbf{\mu}_2)^T
$$

where $\mathbf{\mu}_1$ and $\mathbf{\mu}_2$ are the class mean vectors.

## Key LDA Solution

**Front:** What is the closed-form solution for the optimal LDA projection direction $\mathbf{w}$ in the two-class case?
**Back:**
For two classes, the solution to $\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w}$ gives:

$$
\mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)
$$

The optimal direction is proportional to $\mathbf{S}_W^{-1}$ times the difference between class means.

## Geometric Interpretation of Solution

**Front:** Why isn't the optimal LDA direction simply the line connecting the class means $(\mathbf{\mu}_1 - \mathbf{\mu}_2)$?
**Back:**
The multiplication by $\mathbf{S}_W^{-1}$ accounts for the shape and correlation of the data. If features are correlated or have different variances, $\mathbf{S}_W^{-1}$ "whitens" the space, stretching directions of low variance and compressing directions of high variance to create spherical classes before finding the best separation.

# Algorithm

## 5-Step Algorithm for 2D Binary LDA

**Front:** Outline the 5 key steps to compute the LDA projection vector $\mathbf{w}$ for a 2D, two-class problem.
**Back:**

1. **Compute Class Means:**
   $\mathbf{\mu}_1 = \frac{1}{N_1}\sum_{n \in C_1} \mathbf{x}_n$, $\mathbf{\mu}_2 = \frac{1}{N_2}\sum_{n \in C_2} \mathbf{x}_n$.
2. **Compute Within-Class Scatter Matrices(unnormalized sum of squares):**
   $\mathbf{S}_1 = \sum_{n \in C_1} (\mathbf{x}_n - \mathbf{\mu}_1)(\mathbf{x}_n - \mathbf{\mu}_1)^T$, $\mathbf{S}_2$ similarly.
3. **Compute Total Within-Class Scatter:**
   $\mathbf{S}_W = \mathbf{S}_1 + \mathbf{S}_2$.
4. **Compute Between-Class Scatter:**
   $\mathbf{S}_B = (\mathbf{\mu}_1 - \mathbf{\mu}_2)(\mathbf{\mu}_1 - \mathbf{\mu}_2)^T$.
   * we would only need $(\mathbf{\mu}_1 - \mathbf{\mu}_2)$
5. **Solve for $\mathbf{w}$:**
   $\mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)$.
   * In practice, compute $\mathbf{w} = \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)$ and then normalize it (e.g., to unit length).

## Algorithm Step 0: Input Data Format

**Front:** What is the input data format for binary LDA with $n$ samples and $m$ features?
**Back:**
We have two classes ($C_1$ and $C_2$) with:

- $\mathbf{X}_1$: $(n_1 \times m)$ matrix for class 1
- $\mathbf{X}_2$: $(n_2 \times m)$ matrix for class 2
- Each row is a sample, each column is a feature
- Column means are computed separately for each class

## Algorithm Step 1: Center Each Class

**Front:** How do you center the data for each class, preserving dimensions?
**Back:**
For each class $k \in \{1, 2\}$:

1. Compute row vector of means (means of features for all the datas):
   $\mathbf{\mu}_k = \frac{1}{n_k} \mathbf{1}_{n_k}^T \mathbf{X}_k$ (size: $(1 \times m)$)
2. Center the data(each data's feature values subtracted by relative mean for that feature):
   $\mathbf{\bar{X}}_k = \mathbf{X}_k - \mathbf{1}_{n_k} \mathbf{\mu}_k$
   where $\mathbf{1}_{n_k}$ is $(n_k \times 1)$ column vector
   **Dimensions Preserved:** $\mathbf{\bar{X}}_k$ remains $(n_k \times m)$

## Algorithm Step 2: Compute Scatter Matrices

**Front:** How do you compute scatter matrices with correct dimensionality?
**Back:**
For each centered class matrix $\mathbf{\bar{X}}_k$ (n data * m features):

$$
\mathbf{S}_k = \mathbf{\bar{X}}_k^T \mathbf{\bar{X}}_k
$$

**Dimensions:** $\mathbf{\bar{X}}_k^T$ is $(m \times n_k)$, $\mathbf{\bar{X}}_k$ is $(n_k \times m)$, so $\mathbf{S}_k$ is $(m \times m)$.
**Note:** This is the **scatter matrix** (unnormalized sum of squares).

* Covariance would be $\frac{1}{n_k-1}\mathbf{S}_k$.

**Critical:** The order $\mathbf{\bar{X}}_k^T \mathbf{\bar{X}}_k$ yields feature covariance. Reverse order gives sample similarity.

## Algorithm Step 3: Total Within-Class Scatter

**Front:** How do you compute $\mathbf{S}_W$ and what's the dimensional requirement?
**Back:**

$$
\mathbf{S}_W = \mathbf{S}_1 + \mathbf{S}_2
$$

**Dimensions:** $(m \times m)$ matrix.
**Invertibility Requirement:** For $\mathbf{S}_W^{-1}$ to exist, need $n_1 + n_2 - 2 \geq m$ (more samples than features minus 2).
**If $m > n_1 + n_2 - 2$:** $\mathbf{S}_W$ is singular. Use regularization: $\mathbf{S}_W + \lambda\mathbf{I}_m$.

## Algorithm Step 4: Mean Difference Vector

**Front:** How do you compute the mean difference, watching dimensions?
**Back:**
From Step 1 means $\mathbf{\mu}_1$ and $\mathbf{\mu}_2$ (each $(1 \times m)$ row vectors):

$$
\mathbf{d} = (\mathbf{\mu}_1 - \mathbf{\mu}_2)^T
$$

**Dimensions:** Transpose gives $(m \times 1)$ column vector.
**Alternative:** If means computed as column vectors $(m \times 1)$, then $\mathbf{d} = \mathbf{\mu}_1 - \mathbf{\mu}_2$ directly.
**Consistency is key:** Ensure means and $\mathbf{d}$ are column vectors for $\mathbf{S}_W^{-1}\mathbf{d}$ multiplication.

## Algorithm Step 5: Compute Projection Vector

**Front:** What's the final computation with dimension verification?
**Back:**
Solve:

$$
\mathbf{w} = \mathbf{S}_W^{-1} \mathbf{d}
$$

**Dimension Check:**

- $\mathbf{S}_W^{-1}$: $(m \times m)$
- $\mathbf{d}$: $(m \times 1)$
- $\mathbf{w}$: $(m \times 1)$ ✓
  **Practical:** Solve $\mathbf{S}_W \mathbf{w} = \mathbf{d}$ (avoid explicit inverse).
  **Normalize:** $\mathbf{w} \leftarrow \frac{\mathbf{w}}{\|\mathbf{w}\|}$ (direction matters, magnitude doesn't).

## Common Dimension Error

**Front:** What dimension errors occur with $\mathbf{X} - \mathbf{\mu}$ (without broadcasting)?
**Back:**
**Error:** $\mathbf{X}$ is $(n \times m)$, $\mathbf{\mu}$ is $(1 \times m)$ or $(m \times 1)$.
Subtraction fails: $(n \times m) - (1 \times m)$ is not defined in standard matrix algebra.
**Correct:** Use broadcasting: $\mathbf{X} - \mathbf{1}_n \mathbf{\mu}$

- $\mathbf{1}_n$: $(n \times 1)$ column vector
- $\mathbf{\mu}$: $(1 \times m)$ row vector
- $\mathbf{1}_n \mathbf{\mu}$: $(n \times m)$ replicated means
  **Programming:** In NumPy, $\mathbf{X} - \mathbf{\mu}$ works with broadcasting if $\mathbf{\mu}$ is $(1 \times m)$.

## Pitfalls & Special Considerations

**Front:** What are two critical pitfalls or considerations when applying LDA?
**Back:**

1. **Singular $\mathbf{S}_W$ Problem:** If the number of samples $N$ is less than the number of features $d$, or if features are linearly dependent, $\mathbf{S}_W$ is singular and cannot be inverted. **Solution:** Use PCA first for dimensionality reduction, or apply regularization (e.g., $\mathbf{S}_W + \lambda\mathbf{I}$).
2. **Assumption of Common Covariance:** LDA assumes all classes share the same covariance structure (i.e., $\mathbf{S}_1 \approx \mathbf{S}_2$). If this is severely violated (classes have very different shapes/spreads), the projection may be suboptimal. QDA (Quadratic DA) relaxes this assumption.
3. **Normalization:** The magnitude of $\mathbf{w}$ from $\mathbf{S}_W^{-1}(\mathbf{\mu}_1-\mathbf{\mu}_2)$ is arbitrary. The *direction* is what matters. For stability, often normalize $\mathbf{w}$ to unit length.
