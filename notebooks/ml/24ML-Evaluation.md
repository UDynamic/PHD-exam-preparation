## 01. Basic Definition of a Classifier

**Front:** What is a classifier in pattern recognition?
**Back:**
A function or algorithm that assigns an input feature vector $\mathbf{x}$ to a discrete class label $y$ from a set of classes $\mathcal{C}_k$. It maps the feature space to a decision boundary.

$$
y = f(\mathbf{x}), \quad y \in \{\mathcal{C}_1, \mathcal{C}_2, ..., \mathcal{C}_K\}
$$

## 02. Binary Classification Setup

**Front:** What are the standard class labels in binary classification?
**Back:**
Typically defined as Positive (P, 1) and Negative (N, 0). The goal is to distinguish between the presence (positive) and absence (negative) of a condition.

$$
\text{Class Labels: } \{1, 0\} \text{ or } \{\text{Positive, Negative}\}
$$

## 03. Confusion Matrix Structure

**Front:** How is a 2x2 confusion matrix structured for binary classification?
**Back:**
Rows represent actual classes; columns represent predicted classes.

|                 | Predicted Positive  | Predicted Negative  |
| --------------- | ------------------- | ------------------- |
| Actual Positive | True Positive (TP)  | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TN)  |

$$
N = TP + TN + FP + FN
$$

## 04. True Positive Rate (TPR) / Sensitivity / Recall

**Front:** What is True Positive Rate (TPR) and how is it calculated?
**Back:**

* True rate is over itself

The proportion of actual positives that are correctly identified by the classifier. Also called Sensitivity or Recall.

$$
\text{TPR} = \frac{TP}{TP + FN} = \frac{TP}{P}
$$

## 05. False Positive Rate (FPR) / Fall-out

**Front:** What is False Positive Rate (FPR) and how is it calculated?
**Back:**

* False rate is over the other

The proportion of actual negatives that are incorrectly classified as positives. Also called Fall-out.

$$
\text{FPR} = \frac{FP}{FP + TN} = \frac{FP}{N}
$$

## 06. True Negative Rate (TNR) / Specificity

**Front:** What is True Negative Rate (TNR) and how is it calculated?
**Back:**

* True rate is over itself

The proportion of actual negatives that are correctly identified. Also called Specificity.

$$
\text{TNR} = \frac{TN}{FP + TN} = \frac{TN}{N}
$$

## 36. False Negative Rate (FNR) / Miss Rate

**Front:** What is False Negative Rate (FNR) and how is it calculated?
**Back:**

* False rate is over the other

The proportion of actual positives that are incorrectly classified as negatives. Also called Miss Rate.

$$
\text{FNR} = \frac{FN}{TP + FN} = \frac{FN}{P}
$$

## 07. Precision / Positive Predictive Value (PPV)

**Front:** What is Precision and how is it calculated?
**Back:**
The proportion of positive predictions that are actually correct.

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

## 37. Symmetry of Error Rates

**Front:** What are the complementary relationships between the four fundamental rates?
**Back:**
The four basic rates form complementary pairs:

* opposing rates some to 1

$$
\text{TPR} + \text{FNR} = 1 \quad \text{(Positives)}
$$

$$
\text{TNR} + \text{FPR} = 1 \quad \text{(Negatives)}
$$

## 08. Classifier Scoring vs. Hard Classification

**Front:** What is the difference between a hard classifier and a probabilistic/score-based classifier?
**Back:**

- **Hard classifier:** Directly outputs a discrete class label.
- **Score-based classifier:** Outputs a continuous value (probability or discriminant score) $s(\mathbf{x})$ representing confidence. A threshold $\tau$ is applied to convert the score to a label.

$$
\hat{y} = \begin{cases} 1 & \text{if } s(\mathbf{x}) \ge \tau \\ 0 & \text{if } s(\mathbf{x}) < \tau \end{cases}
$$

## 09. The ROC Graph Definition

**Front:** What is a Receiver Operating Characteristic (ROC) graph?
**Back:**

* it's about positive rates.

A 2D graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold $\tau$ is varied. It plots TPR (Sensitivity) on the y-axis against FPR (1-Specificity) on the x-axis.

$$
\text{ROC Space: } (FPR, TPR) \in [0,1] \times [0,1]
$$

## 10. Operating Point on ROC Curve

**Front:** What does a single point in ROC space represent?
**Back:**
A single point (FPR, TPR) corresponds to a specific confusion matrix generated by applying a single threshold $\tau$ to the classifier's scores.

## 11. The ROC Curve Construction

**Front:** How is the ROC curve constructed from a score-based classifier?
**Back:**

1. Sort test instances by their score $s(\mathbf{x})$ in descending order.
2. Sweep the threshold $\tau$ from $+\infty$ to $-\infty$.
3. For each threshold, calculate the TPR and FPR.
4. Plot the resulting (FPR, TPR) pairs and connect them.

## 12. Effect of Extreme Thresholds on ROC

**Front:** What are the coordinates of the extreme points on an ROC curve?
**Back:**

- **Threshold $= +\infty$:** No positives predicted. (FPR = 0, TPR = 0). Bottom-left corner.
- **Threshold $= -\infty$:** All instances predicted as positive. (FPR = 1, TPR = 1). Top-right corner.

## 13. Diagonal Line in ROC Space

**Front:** What does the diagonal line $y = x$ represent in ROC space?
**Back:**
Represents a random classifier. A classifier on this line has no discriminatory power (TPR = FPR). Any classifier below the diagonal performs worse than random guessing.

## 14. Perfect Classifier in ROC Space

**Front:** What point represents a perfect classifier in ROC space?
**Back:**
The top-left corner (FPR = 0, TPR = 1). This indicates all positives are correctly identified and no negatives are incorrectly flagged as positive.

## 15. Area Under the ROC Curve (AUC) Definition

**Front:** What is the Area Under the ROC Curve (AUC)?
**Back:**
A single scalar value summarizing the entire ROC curve. It measures the overall performance of a classifier across all possible thresholds. Ranges from 0 to 1.

$$
\text{AUC} \in [0, 1]
$$

## 18. AUC Scale Interpretation

**Front:** How is the AUC value interpreted?
**Back:**

- **AUC = 1.0:** Perfect separation (all positive scores > all negative scores).
- **AUC = 0.9 - 1.0:** Excellent classifier.
- **AUC = 0.8 - 0.9:** Good classifier.
- **AUC = 0.7 - 0.8:** Fair classifier.
- **AUC = 0.6 - 0.7:** Poor classifier.
- **AUC = 0.5:** Random classifier (no discrimination).
- **AUC < 0.5:** Worse than random (systematic error; invert decision rule).

## 19. AUC Invariance Properties

**Front:** What properties is AUC invariant to?
**Back:**

- **Monotonic transformations:** AUC is unchanged if the scores are transformed by any strictly increasing function (e.g., log, exponential, scaling).
- **Threshold invariance:** Summarizes performance independent of a specific threshold.

## 20. ROC vs. PR Curve Comparison

**Front:** When should you use a Precision-Recall (PR) curve instead of an ROC curve?
**Back:**
ROC curves can be overly optimistic on highly imbalanced datasets because FPR (FP/N) becomes very small when N is large, even with many false positives. PR curves (Precision vs. Recall) focus on the positive class and are more informative for imbalanced data.

## 21. Effect of Class Imbalance on ROC

**Front:** How does severe class imbalance affect the ROC curve?
**Back:**
[! special consideration regarding pitfalls]
The ROC curve remains visually optimistic. A large absolute number of False Positives results in a small FPR if the total Negatives (N) is huge. The curve can look excellent (high TPR, low FPR) while Precision is actually very low.

## 22. ROC for Multi-class Classification

**Front:** How is ROC analysis extended to multi-class classification?
**Back:**
**One-vs-All (OvA) strategy:**

1. Treat each class $C_k$ as "positive" and all other classes as "negative".
2. Plot separate ROC curves for each class.
3. Calculate Macro-AUC (average of AUCs) or Micro-AUC (pool all decisions together).

## 35. Pitfall: Leakage and Overfitting

**Front:** How does overfitting affect ROC and AUC?
**Back:**
[! pitfalls and special considerations]
A classifier that memorizes the training data will achieve AUC = 1.0 on the training set but significantly lower AUC on the validation set. Always evaluate ROC/AUC on held-out test data.
