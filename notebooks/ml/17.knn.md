definition and distinction of different groups of classification methods including parametric vs none parametric and discreminative vs generative and Estimator's and calculators; including but not only these concepts:

1. regressions (linear and logistic) are parametric.
2. parametrics need training data for learning parameters to decide for the new data (for test almost all or some training data is used)
3. none parametrics the training is not necessary and the training data is directly used.(almost all the work is done in test time)
   1. analysis on train vs test computational cost
4. K-NN is none parametric.
5. both supervised and unsupervised has parametric and none parametric methods
6. if the distribution is known but parameters are unknown we use MLE or MAP
7. Bayes classifier will give the optimal classification (p(\\theta) and p(x|\theta)
8. none parametrics are memory based or instance based
9. none parametrics memorize the training data and predicts \hat{y} = f(x|training data) wich is usually declared according to the similarity of test data x to the training data [elaborate on this one.
10. the best use scenario for none parametric: modeling parametric is hard or the distribution is unknown



focusing on main subject of K-NN and Parzen window:[! you write the correct ones for each of the two and elaborate on diffeneces if there are any]

1. definition of the problem and formulation and parameters : p_n(x) = \frac{k}{nv} & v = (h^d)
2. definition of the window around the samples
3. calculation of v and k:
   1. fixed window (Parzen window): v_n = \frac{1}{\sqert{n}}
   2. fixed number of samples (K-NN): k-n = \sqert{n}
4. the best use scenario for none parametric: modeling parametric is hard or the distribution is unknown
5. this methods deliver flexible none parametric way of estimating the distributy functions
6. these two methods are specially effective when dealing with none conventional or very complex distributions for data
7. these two methods are example based learnings.
8. these two methodsare Instance based or memory based?
9. these two methods are lazy. what does it mean?
10. for K-NN if not specifically mentioned, use Euclidian distance for 	finding K Nearest Neighbours.
11. saves training data ?
12. complete steps for k-nn algorithm: including j^* = \argmax_{j=1, \dots, c} k_j
13. these two methods could be considered discriminative
    1. with K-NN we could find none linear decision boundaries
14. these two are sensative to noise especially when : small data sets, low dimensions or features[which is odd according to high dimensionality being more prone to overfit], small number of K like 1.
15. K is typically an odd number
16. main requirements for making instance based learner:
    1. distance criteria
    2. number of nearest neighbours
    3. weighting function (Optional) [! compare Euclidian distance with weighted Euclidian distance with formulation]
       1. cosine distance complete formula: 1 - cosine similarity
17. effect of magnitude of K on these:
    1. smaller k means more complex model => low training error, low bias, high variance.
       1. k=1 is at the most overfitt configuration.
       2. more complex decision boundary [! what does decision boundary even mean for K-NN?]
    2. bigger k means more simple model => higher bias
       1. smoother decision boundary
    3. if N is big the K must be big too [! analyse this]
    4. elbow method for choosing best k
    5. analysis on computational cost
    6. analysis on noise sensitivity ( [! is there difference betwean feature noise and classification noise?])
    7. analysis on number of effective features
18. effect of magnitude of N as number of data on the k-nn accuracy
19. regression with K-NN
    1. \hat{y} = \frac{1}{k} \sum_{j=1}^k y'^j
    2. analysis on magnitude of k for regression
20. is K-nn appropriate for regression?
    1. what does it even mean to use another method for another method?
    2. what is the general rule for answering these kind of questions?


parzen window:

1. complete definition and formulation.
   1. v = h^d
   2. labeling algorithm (complete algorithm mentioning \sum \varphi (\frac{x-x^i}{h_n}) and decision cases for \phi \leq 0.5 to be labeled 1 and 0 otherwise)
   3. solving an example for p(x) for x =1 for data set D={2,3,4,8,10,11,12} with h=3 for rectangular window
2. it's known as kernel densitiy estimation. what it means?
3. uses different shapes for windows. rectangular, Gaussian, triangular and cubic
4. soft window
   1. complete definition and elaboration on density estimate and kernel funcitons
   2. analysis on magnitude of h in soft window (visually and conceptually)
   3. using Gaussian distribution for an example with these data = {1,1.2,1.4,1.5,1.6,2,2.1,2.15,4,4.3,4.7,4.75,5} elaborating on \hat p formulation.
      1. analysis over magnitude of h=\sigma (both visually and conceptually)
5. analysis of window size effect on training error and generalization factors
6. pros of parzen window:
   1. no prior assumption about data is needed
   2. with sufficient data converges to any distribution function
7. cons:
   1. exponential need for data relative to dimensions [! elaborate]
   2. may require high memory and computation time
