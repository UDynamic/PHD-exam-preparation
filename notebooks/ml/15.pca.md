# Lagrange multiplyer

## Lagrange Multipliers - Core Purpose

**Front:** What is the fundamental purpose of the method of Lagrange multipliers in optimization?
**Back:**
It is a strategy for finding the local maxima and minima of a function subject to equality constraints (e.g., $g(x) = 0$), without needing to solve the constraint explicitly for one variable.

## Lagrange Function (Lagrangian) - Definition

**Front:** For a scalar optimization problem: maximize $f(x)$ subject to $g(x) = 0$, how is the Lagrangian function $\mathcal{L}$ defined?
**Back:**
Introduce a new scalar variable $\lambda$, called the Lagrange multiplier. The Lagrangian is:

$$
\mathcal{L}(x, \lambda) = f(x) - \lambda g(x)
$$

The term $-\lambda g(x)$ incorporates the constraint into the objective function.

## Lagrange Condition - Partial Derivatives

**Front:** What are the necessary conditions for a point $(x^*, \lambda^*)$ to be a constrained extremum of $f$ subject to $g(x)=0$?
**Back:**
Take the partial derivatives of the Lagrangian $\mathcal{L}(x, \lambda) = f(x) - \lambda g(x)$ and set them to zero:

1. $\frac{\partial \mathcal{L}}{\partial x} = 0 \quad \Rightarrow \quad f'(x) - \lambda g'(x) = 0$
2. $\frac{\partial \mathcal{L}}{\partial \lambda} = 0 \quad \Rightarrow \quad g(x) = 0$

This yields a system of equations to solve for $x^*$ and $\lambda^*$.

## Vector Case - General Formulation

**Front:** For optimizing $f(\mathbf{x})$ with $m$ constraints $g_i(\mathbf{x})=0$, how is the Lagrangian $\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda})$ defined ($\mathbf{x} \in \mathbb{R}^D$)?
**Back:**
Introduce a vector of multipliers $\boldsymbol{\lambda} = (\lambda_1, ..., \lambda_m)$. The Lagrangian is the sum:

$$
\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}) = f(\mathbf{x}) - \sum_{i=1}^m \lambda_i g_i(\mathbf{x})
$$

The solution satisfies $\nabla_{\mathbf{x}} \mathcal{L} = \mathbf{0}$ and $\nabla_{\boldsymbol{\lambda}} \mathcal{L} = \mathbf{0}$.

## General Solution Workflow

**Front:** Summarize the 3-step workflow for solving a constrained optimization problem using Lagrange multipliers.
**Back:**

1. **Form Lagrangian:** $\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}) = f(\mathbf{x}) - \sum_i \lambda_i g_i(\mathbf{x})$.
2. **Take Partial Derivatives:** Set $\nabla_{\mathbf{x}} \mathcal{L} = 0$ and $\nabla_{\boldsymbol{\lambda}} \mathcal{L} = 0$ (i.e., $g_i(\mathbf{x})=0$).
3. **Solve System:** Solve the resulting (often nonlinear) system of equations for $\mathbf{x}^*$ and $\boldsymbol{\lambda}^*$.

## Lagrange - Geometric Interpretation

**Front:** At a constrained optimum, what is the geometric relationship between $\nabla f$ and $\nabla g$?
**Back:**
Their gradients are parallel: $\nabla f = \lambda \nabla g$. The Lagrange multiplier $\lambda$ is the scaling factor. This means the level set of $f$ is tangent to the constraint surface defined by $g(x)=0$.

## Lagrange - Pitfall: Critical Points

**Front:** Does satisfying the Lagrange conditions ($\nabla \mathcal{L}=0$) guarantee a global maximum or minimum?
**Back:**
No. It only identifies **stationary points** (critical points) of the Lagrangian, which include constrained maxima, minima, and saddle points. Further tests (e.g., second-order conditions, checking boundaries) are needed to determine the nature of the extremum.

---

# PCA

## Dimensionality Reduction - Core Goal

**Front:** For a dataset $X$ of $N$ samples in $D$ dimensions, what is the goal of dimensionality reduction?
**Back:**
Find a representation in $M$ dimensions (where $M < D$) that preserves the essential structure/information (e.g., variance, class separability) of the original data. This combats the curse of dimensionality.

## PCA - Definition & Core Idea

**Front:** What is Principal Component Analysis (PCA)? State its core linear algebra idea.
**Back:**
PCA is an unsupervised linear transformation. It finds a new orthogonal coordinate system (principal components) for the centered data such that the first axis (PC1) captures the maximum variance, the second (PC2) captures the next maximum variance orthogonal to PC1, and so on.

## Feature Selection vs. Extraction - Dimensionality Context

**Front:** In reducing dimensions from $D$ to $M$, contrast Feature Selection and Feature Extraction.
**Back:**

- **Feature Selection:** Selects $M$ of the original $D$ features. The new feature space is a subset of the original axes.
- **Feature Extraction (like PCA):** Creates $M$ new features, each a function (linear combination) of all $D$ original features. The new axes lie in the original $D$-dimensional space.

## PCA - Visual Interpretation (Projection & Error)

**Front:** For 2D data, PCA finds a 1D subspace (line). What two equivalent criteria define the optimal line?
**Back:**

1. **Maximize Variance:** Maximize the variance of the projected data points.
2. **Minimize Error:** Minimize the mean squared distance (reconstruction error) between the original points and their projections onto the line.

## PCA Formulation - Centering & Notation

**Front:** Let $X$ be an $N \times D$ data matrix ($N$ samples, $D$ features). What is the first step of PCA and what does $\bar{X}$ represent?
**Back:**
**Center the data:** Subtract the mean of each feature (column) from all samples. $\bar{X}$ is the $N \times D$ centered data matrix, where $\sum_{n=1}^N \bar{x}_{n,j} = 0$ for each feature $j$.

## PCA Formulation - Maximizing Projection Variance

**Front:** For centered data $\bar{X}$, we project onto a unit vector $\mathbf{v} \in \mathbb{R}^D$. The projection of sample $n$ is $z_n = \mathbf{v}^T \bar{\mathbf{x}}_n$. Formulate the variance maximization problem.
**Back:**
Maximize the sample variance of the projections $\{z_1,...,z_N\}$:

$$
\text{Maximize } \frac{1}{N} \sum_{n=1}^N (z_n)^2 = \frac{1}{N} \sum_{n=1}^N (\mathbf{v}^T \bar{\mathbf{x}}_n)^2 = \mathbf{v}^T \left( \frac{1}{N} \bar{X}^T \bar{X} \right) \mathbf{v}
$$

subject to the constraint $\mathbf{v}^T \mathbf{v} = 1$. The matrix $\frac{1}{N}\bar{X}^T \bar{X}$ is the $D \times D$ sample covariance matrix $C$.

## Lagrange Multiplier in PCA - Setting Up

**Front:** The PCA objective is $\max \mathbf{v}^T C \mathbf{v}$ s.t. $\mathbf{v}^T \mathbf{v}=1$. How is the Lagrange function $\mathcal{L}$ defined?
**Back:**
Introduce a Lagrange multiplier $\lambda$ for the equality constraint. The Lagrangian function is:

$$
\mathcal{L}(\mathbf{v}, \lambda) = \mathbf{v}^T C \mathbf{v} - \lambda (\mathbf{v}^T \mathbf{v} - 1)
$$

We find the stationary points by taking derivatives w.r.t. $\mathbf{v}$ and $\lambda$ and setting them to zero.

## Lagrange Multiplier in PCA - Taking Derivative

**Front:** Taking the derivative of $\mathcal{L}(\mathbf{v}, \lambda) = \mathbf{v}^T C \mathbf{v} - \lambda (\mathbf{v}^T \mathbf{v} - 1)$ with respect to the vector $\mathbf{v}$, what equation do we get?
**Back:**
Using vector calculus, $\frac{\partial \mathcal{L}}{\partial \mathbf{v}} = 2C\mathbf{v} - 2\lambda \mathbf{v}$. Setting this to zero yields the critical condition:

$$
C \mathbf{v} = \lambda \mathbf{v}
$$

This is the eigenvector equation for the covariance matrix $C$.

## PCA Solution - Eigenvalue Problem

**Front:** From $C \mathbf{v} = \lambda \mathbf{v}$, what is the direct interpretation for $\lambda$ and $\mathbf{v}$ in the context of PCA?
**Back:**

- $\mathbf{v}$ is an eigenvector of the covariance matrix $C$.
- $\lambda$ is its corresponding eigenvalue, which equals the variance of the data projected onto $\mathbf{v}$.
- The optimal $\mathbf{v}$ (first principal component) is the eigenvector corresponding to the **largest eigenvalue**.

## Eigenvector Algorithm (5 Steps for One PC)

**Front:** Given a $D \times D$ covariance matrix $C$, list the 5-step process to find its dominant eigenpair $(\lambda_1, \mathbf{v}_1)$.
**Back:**

1. Write eigen-equation: $C\mathbf{v} = \lambda \mathbf{v}$.
2. Rearrange: $(C - \lambda I) \mathbf{v} = 0$.
3. Solve for $\lambda$: Require non-trivial $\mathbf{v}$, so $\det(C - \lambda I) = 0$. Find largest root $\lambda_1$.
4. Solve for $\mathbf{v}$: Substitute $\lambda_1$ into $(C - \lambda_1 I) \mathbf{v} = 0$ and solve for $\mathbf{v}_1$.
5. Normalize: Set $\mathbf{v}_1 \leftarrow \mathbf{v}_1 / \|\mathbf{v}_1\|$.

## Full PCA Algorithm (Step-by-Step)

**Front:** For an $N \times D$ data matrix $X$, list the complete steps to perform PCA and obtain $M$-dimensional projections.
**Back:**

1. **Center:** Compute column means $\bar{\mathbf{x}}$, form $\bar{X} = X - \mathbf{1}_N \bar{\mathbf{x}}^T$.
2. **Covariance:** Compute $D \times D$ covariance $C = \frac{1}{N} \bar{X}^T \bar{X}$.
3. **Eigendecomposition:** Solve $C V = V \Lambda$, where $V$ is $D \times D$ orthogonal matrix of eigenvectors (columns), $\Lambda$ is diagonal of eigenvalues $\lambda_1 \ge ... \ge \lambda_D$.
4. **Select:** Choose first $M$ columns of $V$ as $V_M$ ($D \times M$).
5. **Project:** The reduced data is $Z = \bar{X} V_M$ ($N \times M$). Columns of $Z$ are principal component scores.

## Projection and Reconstruction Formulas

**Front:** Given centered data $\bar{X}$ ($N \times D$) and a PCA projection matrix $V_M$ ($D \times M$), what are the formulas for projection and approximate reconstruction?
**Back:**

- **Projection (to lower $M$-D):** $Z = \bar{X} V_M$  ($N \times M$).
- **Reconstruction (back to original $D$-D):** $\tilde{X} = Z V_M^T = \bar{X} V_M V_M^T$.
  Note: $\bar{X} \approx \tilde{X}$ if $M$ is close to $D$. Perfect reconstruction if $M = D$.

## PCA vs. LDA - Conceptual Comparison

**Front:** For a labeled dataset with $K$ classes, contrast the fundamental objectives of PCA and LDA.
**Back:**

- **PCA (Unsupervised):** Maximizes total **variance** of projected data. Finds directions of greatest spread, ignoring class labels. Goal: preserve data structure.
- **LDA (Supervised):** Maximizes **between-class scatter** relative to **within-class scatter**. Finds directions that best separate class means. Goal: improve classification.

## PCA vs. LDA - Visual from Scatter Plots

**Front:** Sketch 2D data from two classes forming two elongated, overlapping clouds. Draw the likely first PC and first LD axis.
**Back:**

- **PCA PC1:** Aligns with the direction of greatest overall data spread, which likely follows the elongation of the combined clouds.
- **LDA LD1:** Aligns perpendicular to the decision boundary, aiming to pull apart the class centroids while squeezing each class's scatter. It often ignores directions of high variance within a class.

## PCA Pitfall - Scaling & Unit Variance

**Front:** Why must you often standardize data (mean=0, variance=1 per feature) before PCA? Give an example.
**Back:**
PCA is variance-sensitive. A feature with large scale (e.g., salary in $10,000s) dominates one with small scale (e.g., age). PC1 will align nearly with that high-variance feature, potentially missing important structure. Standardizing puts all features on equal footing.

## PCA Pitfall - Linearity & Interpretability

**Front:** What are two key conceptual limitations of standard PCA?
**Back:**

1. **Linear Assumption:** It only captures linear correlations. Nonlinear manifold structures are lost (use Kernel PCA).
2. **Interpretability:** Principal components are linear combos of all original features, making them harder to explain than individual selected features.
