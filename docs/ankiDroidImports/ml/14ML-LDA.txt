#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

14ML0001	Basic	14ML-LDA	What is the formula for sample variance when data is in vector form?	Given \(n\) samples of a single feature in vector \(\mathbf{x} = [x_1, x_2, ..., x_n]^T\) with mean \(\mu = \frac{1}{n}\sum_{i=1}^n x_i\):<br><br> \(\text{Var}(\mathbf{x}) = \frac{1}{n-1} (\mathbf{x} - \mu\mathbf{1})^T (\mathbf{x} - \mu\mathbf{1}) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu)^2\)<br><br><strong>Dimensions:</strong> \(\mathbf{x}\) is \((n \times 1)\), \(\mathbf{1}\) is \((n \times 1)\), \((\mathbf{x} - \mu\mathbf{1})\) is \((n \times 1)\), so result is scalar \((1 \times 1)\).
14ML0002	Basic	14ML-LDA	What is the formula for sample covariance between two variables?	Given two vectors \(\mathbf{x} = [x_1, ..., x_n]^T\) and \(\mathbf{y} = [y_1, ..., y_n]^T\) with means \(\mu_x\) and \(\mu_y\):<br><br> \(\text{Cov}(\mathbf{x}, \mathbf{y}) = \frac{1}{n-1} (\mathbf{x} - \mu_x\mathbf{1})^T (\mathbf{y} - \mu_y\mathbf{1}) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu_x)(y_i - \mu_y)\)<br><br><strong>Dimensions:</strong> Both centered vectors are \((n \times 1)\), their dot product gives scalar \((1 \times 1)\).
14ML0003	Basic	14ML-LDA	How do you compute the sample covariance matrix for \(m\) features?	Given data matrix \(\mathbf{X}\) of size \((n \times m)\) where columns are features, with column means \(\mathbf{\mu} = [\mu_1, \mu_2, ..., \mu_m]\):<br><br>1. Center the data: \(\mathbf{\bar{X}} = \mathbf{X} - \mathbf{1}_n \mathbf{\mu}^T\)<br>2. Compute covariance: \(\mathbf{S} = \frac{1}{n-1} \mathbf{\bar{X}}^T \mathbf{\bar{X}}\)<br><br><strong>Dimensions Critical:</strong><br>- \(\mathbf{X}\): \((n \times m)\) (n samples, m features)<br>- \(\mathbf{1}_n\): \((n \times 1)\) column vector of ones<br>- \(\mathbf{\mu}\): \((1 \times m)\) row vector of means<br>- \(\mathbf{\bar{X}}\): \((n \times m)\) centered data<br>- \(\mathbf{\bar{X}}^T\): \((m \times n)\) transpose<br>- \(\mathbf{S} = \mathbf{\bar{X}}^T \mathbf{\bar{X}}\): \((m \times m)\) covariance matrix<br><br><strong>Key Insight:</strong> \(\mathbf{\bar{X}}^T \mathbf{\bar{X}}\) gives feature-by-feature relationships, not sample-by-sample.
14ML0004	Basic	14ML-LDA	For a 5×3 data matrix (5 samples, 3 features), what are the dimensions at each step?	1. \(\mathbf{X}\): \((5 \times 3)\)<br>2. \(\mathbf{\mu}\) (row means): \((1 \times 3)\) or as column vector: \((3 \times 1)\)<br>3. \(\mathbf{1}_5\): \((5 \times 1)\) column of ones<br>4. \(\mathbf{1}_5 \mathbf{\mu}^T\): \((5 \times 1) \times (1 \times 3) = (5 \times 3)\)<br>5. \(\mathbf{\bar{X}} = \mathbf{X} - \mathbf{1}_5 \mathbf{\mu}^T\): \((5 \times 3) - (5 \times 3) = (5 \times 3)\)<br>6. \(\mathbf{\bar{X}}^T\): \((3 \times 5)\)<br>7. \(\mathbf{\bar{X}}^T \mathbf{\bar{X}}\): \((3 \times 5) \times (5 \times 3) = (3 \times 3)\) covariance matrix<br>8. \(\frac{1}{n-1}\mathbf{\bar{X}}^T \mathbf{\bar{X}}\): \((3 \times 3)\) sample covariance matrix<br><br><strong>Wrong Order Warning:</strong> \(\mathbf{\bar{X}} \mathbf{\bar{X}}^T\) gives \((5 \times 5)\) sample similarity matrix, NOT covariance!
14ML0005	Basic	14ML-LDA	What are the two primary use cases of Linear Discriminant Analysis (LDA)?	1. <strong>Supervised Dimensionality Reduction:</strong> Projects data onto a lower-dimensional space while preserving as much class-discriminatory information as possible. The new axes are linear combinations of original features.<br>2. <strong>Classification:</strong> Provides a simple, linear classifier by defining a decision boundary (hyperplane) based on the projected class means and shared covariance.
14ML0006	Basic	14ML-LDA	What is the core goal of Linear Discriminant Analysis (LDA)?	LDA is a supervised method. Its goal is to find a projection (a direction or set of axes) that <strong>maximizes the separation between the means of different classes</strong> while <strong>minimizing the variance (scatter) within each class</strong> in the projected space.<br><br> \(\text{Goal: Find } \mathbf{w} \text{ that maximizes class separation relative to class compactness.}\)
14ML0007	Basic	14ML-LDA	In LDA, given \(\mathbf{x} \in \mathbb{R}^d\) and \(\mathbf{w} \in \mathbb{R}^d\), what does the operation \(\mathbf{w}^T\mathbf{x}\) compute geometrically? What are the dimensions of the input and output?	Geometrically, \(\mathbf{w}^T\mathbf{x}\) computes the <strong>scalar projection</strong> of the data point \(\mathbf{x}\) onto the vector \(\mathbf{w}\). This operation:<br><br>1. Takes a high-dimensional point \(\mathbf{x}\) in the original feature space (dimension \(d\)).<br>2. Projects it onto the 1-dimensional line defined by the direction of \(\mathbf{w}\).<br>3. Outputs a <strong>single scalar value</strong> representing the coordinate of that projection along the line.<br><br>- <strong>Dimensions:</strong> \(\mathbf{x}\): \((d \times 1)\), \(\mathbf{w}\): \((d \times 1)\), \(\mathbf{w}^T\mathbf{x}\): \((1 \times 1)\) (a scalar)<br>- If \(\mathbf{w}\) is a unit vector (\(\|\mathbf{w}\| = 1\)), then \(\mathbf{w}^T\mathbf{x}\) equals the <strong>signed length/magnitude</strong> of the orthogonal projection.<br>- The sign indicates on which "side" of the origin (relative to \(\mathbf{w}\)'s direction) the projection lies.<br>- This projection reduces dimensionality from \(d\) to \(1\), which is the core of LDA's dimensionality reduction aspect.<br><br><strong>Visual:</strong> Imagine a 2D cloud of points. \(\mathbf{w}\) defines a line through the origin. Each point \(\mathbf{x}\) is "dropped" perpendicularly onto this line. \(\mathbf{w}^T\mathbf{x}\) is the coordinate of that landing point along the line.<br><br> \(\text{Projection: } \mathbf{w}^T\mathbf{x} = \|\mathbf{w}\|\|\mathbf{x}\|\cos\theta \quad\text{(where } \theta \text{ is the angle between } \mathbf{w} \text{ and } \mathbf{x})\)
14ML0008	Basic	14ML-LDA	In a 2D scatter plot of two classes, how would you visually identify a good projection vector \(\mathbf{w}\) for LDA?	A good \(\mathbf{w}\) vector, when the data is projected onto the line in its direction, will result in: 1) Two clusters of projected points (one per class) that are <strong>far apart from each other</strong> (large inter-class distance). 2) Each cluster being as <strong>tight or compact</strong> as possible (small intra-class spread).
14ML0009	Basic	14ML-LDA	What is the first, naive objective function for LDA, and what is its major flaw?	The naive rule aims to <strong>maximize the squared distance between projected class means</strong>.<br><br> \(\underset{\mathbf{w}}{\argmax} (\tilde{\mu}_1 - \tilde{\mu}_2)^2 \quad \text{, where } \quad \tilde{\mu}_c = \mathbf{w}^T\mathbf{\mu}_c\)<br><br><strong>Major Flaw:</strong> It doesn't consider within-class scatter. The solution could simply blow up the scale of \(\mathbf{w}\), and it's sensitive to classes with large internal variance.<br><br> \(\underset{\mathbf{w}}{\arg\max} (\mathbf{w}^T\mathbf{\mu}_1 - \mathbf{w}^T\mathbf{\mu}_2)^2 = \underset{\mathbf{w}}{\arg\max} [\mathbf{w}^T(\mathbf{\mu}_1-\mathbf{\mu}_2)(\mathbf{\mu}_1-\mathbf{\mu}_2)^T\mathbf{w}]\)
14ML0010	Basic	14ML-LDA	List one pro and one critical con of the LDA objective \(\arg\max (\tilde{\mu}_1 - \tilde{\mu}_2)^2\).	<strong>Pro:</strong> Simple and intuitive. Directly targets class separation.<br><strong>Critical Con:</strong> It is <strong>not scale-invariant</strong>. Multiplying \(\mathbf{w}\) by a scalar increases the objective without actually improving the separation relative to the data's natural spread. It ignores within-class variance, leading to poor projections if classes have high scatter.
14ML0011	Basic	14ML-LDA	What is Fisher's Linear Discriminant criterion (J(w)), and how does it fix the first rule's flaw?	Fisher's criterion introduces a normalization term using within-class scatter.<br><br> \(J(\mathbf{w}) = \frac{(\tilde{\mu}_1 - \tilde{\mu}_2)^2}{\tilde{s}_1^2 + \tilde{s}_2^2}\)<br><br>It fixes the scale problem by maximizing the ratio of <strong>between-class variance</strong> to <strong>within-class variance</strong>. The denominator \(\tilde{s}_c^2\) measures the scatter (variance) of projected points within class \(c\).<br><br>* if they ask to compare between features and choose the best one, we use this formula to compare (goal is maximization)
14ML0012	Basic	14ML-LDA	How do we get from \(J(\mathbf{w}) = \frac{(\tilde{\mu}_1 - \tilde{\mu}_2)^2}{\tilde{s}_1^2 + \tilde{s}_2^2}\) to the standard form \(J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}\)?	1. <strong>Between-class scatter (numerator):</strong> \((\mathbf{w}^T\mathbf{\mu}_1 - \mathbf{w}^T\mathbf{\mu}_2)^2 = \mathbf{w}^T(\mathbf{\mu}_1-\mathbf{\mu}_2)(\mathbf{\mu}_1-\mathbf{\mu}_2)^T\mathbf{w} = \mathbf{w}^T \mathbf{S}_B \mathbf{w}\), where \(\mathbf{S}_B\) is the between-class scatter matrix.<br>2. <strong>Within-class scatter (denominator):</strong> \(\tilde{s}_1^2 + \tilde{s}_2^2 = \sum_{n \in C_1}(\mathbf{w}^T\mathbf{x}_n - \tilde{\mu}_1)^2 + \sum_{n \in C_2}(\mathbf{w}^T\mathbf{x}_n - \tilde{\mu}_2)^2 = \mathbf{w}^T \mathbf{S}_W \mathbf{w}\), where \(\mathbf{S}_W\) is the total within-class scatter matrix.
14ML0013	Basic	14ML-LDA	What is the standard matrix form of Fisher's Linear Discriminant objective function?	The objective is to maximize the ratio of between-class scatter to within-class scatter in the projected space:<br><br>* <em>make the projections on w as <strong>far</strong> as possible and as <strong>compact</strong> as possible</em><br><br> \(J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}\)<br><br>where \(\mathbf{S}_B\) is the between-class scatter matrix and \(\mathbf{S}_W\) is the within-class scatter matrix.
14ML0014	Basic	14ML-LDA	To find the optimal projection vector \(\mathbf{w}\), what mathematical problem arises from maximizing \(J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}\)?	Taking the derivative and setting it to zero leads to the generalized eigenvalue problem:<br><br> \(\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w} \\ (\mathbf{S}_W^{-1} \mathbf{S}_B) \mathbf{w} = \lambda \mathbf{w}\)<br><br>* This means the optimal \(\mathbf{w}\) is an eigenvector of \(\mathbf{S}_W^{-1}\mathbf{S}_B\).
14ML0015	Basic	14ML-LDA	In the two-class case, what simpler form does the between-class scatter matrix \(\mathbf{S}_B\) take?	For two classes, \(\mathbf{S}_B\) simplifies to an outer product:<br><br> \(\mathbf{S}_B = (\mathbf{\mu}_1 - \mathbf{\mu}_2)(\mathbf{\mu}_1 - \mathbf{\mu}_2)^T\)<br><br>where \(\mathbf{\mu}_1\) and \(\mathbf{\mu}_2\) are the class mean vectors.
14ML0016	Basic	14ML-LDA	What is the closed-form solution for the optimal LDA projection direction \(\mathbf{w}\) in the two-class case?	For two classes, the solution to \(\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w}\) gives:<br><br> \(\mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)\)<br><br>The optimal direction is proportional to \(\mathbf{S}_W^{-1}\) times the difference between class means.
14ML0017	Basic	14ML-LDA	Why isn't the optimal LDA direction simply the line connecting the class means \((\mathbf{\mu}_1 - \mathbf{\mu}_2)\)?	The multiplication by \(\mathbf{S}_W^{-1}\) accounts for the shape and correlation of the data. If features are correlated or have different variances, \(\mathbf{S}_W^{-1}\) "whitens" the space, stretching directions of low variance and compressing directions of high variance to create spherical classes before finding the best separation.
14ML0018	Basic	14ML-LDA	Outline the 5 key steps to compute the LDA projection vector \(\mathbf{w}\) for a 2D, two-class problem.	1. <strong>Compute Class Means:</strong><br>   \(\mathbf{\mu}_1 = \frac{1}{N_1}\sum_{n \in C_1} \mathbf{x}_n\), \(\mathbf{\mu}_2 = \frac{1}{N_2}\sum_{n \in C_2} \mathbf{x}_n\).<br>2. <strong>Compute 0 Centered matrices:</strong><br>   \(\mathbf{\bar{X}}_k = \mathbf{X}_k - \mathbf{1}_{n_k} \mathbf{\mu}_k\)<br>3. <strong>Compute Within-Class Scatter Matrices(unnormalized sum of squares):</strong><br>   \(\mathbf{S}_1 = \sum_{n \in C_1} (\mathbf{x}_n - \mathbf{\mu}_1)(\mathbf{x}_n - \mathbf{\mu}_1)^T\), \(\mathbf{S}_2\) similarly.<br>4. <strong>Compute Total Within-Class Scatter:</strong><br>   \(\mathbf{S}_W = \mathbf{S}_1 + \mathbf{S}_2\).<br>5. <strong>Compute Between-Class Scatter:</strong><br>   \(\mathbf{S}_B = (\mathbf{\mu}_1 - \mathbf{\mu}_2)(\mathbf{\mu}_1 - \mathbf{\mu}_2)^T\).<br>   * we would only need \((\mathbf{\mu}_1 - \mathbf{\mu}_2)\)<br>6. <strong>Solve for \(\mathbf{w}\):</strong><br>   \(\mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)\).<br>   * In practice, compute \(\mathbf{w} = \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)\) and then normalize it (e.g., to unit length).
14ML0019	Basic	14ML-LDA	What is the input data format for binary LDA with \(n\) samples and \(m\) features?	We have two classes (\(C_1\) and \(C_2\)) with:<br><br>- \(\mathbf{X}_1\): \((n_1 \times m)\) matrix for class 1<br>- \(\mathbf{X}_2\): \((n_2 \times m)\) matrix for class 2<br>- Each row is a sample, each column is a feature<br>- Column means are computed separately for each class
14ML0020	Basic	14ML-LDA	How do you center the data for each class, preserving dimensions?	For each class \(k \in \{1, 2\}\):<br><br>1. Compute row vector of means (means of features for all the datas):<br>   \(\mathbf{\mu}_k = \frac{1}{n_k} \mathbf{1}_{n_k}^T \mathbf{X}_k\) (size: \((1 \times m)\))<br>2. Center the data(each data's feature values subtracted by relative mean for that feature):<br>   \(\mathbf{\bar{X}}_k = \mathbf{X}_k - \mathbf{1}_{n_k} \mathbf{\mu}_k\)<br>   where \(\mathbf{1}_{n_k}\) is \((n_k \times 1)\) column vector<br>   <strong>Dimensions Preserved:</strong> \(\mathbf{\bar{X}}_k\) remains \((n_k \times m)\)
14ML0021	Basic	14ML-LDA	How do you compute scatter matrices with correct dimensionality?	For each centered class matrix \(\mathbf{\bar{X}}_k\) (n data * m features):<br><br> \(\mathbf{S}_k = \mathbf{\bar{X}}_k^T \mathbf{\bar{X}}_k\)<br><br><strong>Dimensions:</strong> \(\mathbf{\bar{X}}_k^T\) is \((m \times n_k)\), \(\mathbf{\bar{X}}_k\) is \((n_k \times m)\), so \(\mathbf{S}_k\) is \((m \times m)\).<br><strong>Note:</strong> This is the <strong>scatter matrix</strong> (unnormalized sum of squares).<br><br>* Covariance would be \(\frac{1}{n_k-1}\mathbf{S}_k\).<br><br><strong>Critical:</strong> The order \(\mathbf{\bar{X}}_k^T \mathbf{\bar{X}}_k\) yields feature covariance. Reverse order gives sample similarity.
14ML0022	Basic	14ML-LDA	How do you compute \(\mathbf{S}_W\) and what's the dimensional requirement?	\(\mathbf{S}_W = \mathbf{S}_1 + \mathbf{S}_2\)<br><br><strong>Dimensions:</strong> \((m \times m)\) matrix.<br><strong>Invertibility Requirement:</strong> For \(\mathbf{S}_W^{-1}\) to exist, need \(n_1 + n_2 - 2 \geq m\) (more samples than features minus 2).<br><strong>If \(m > n_1 + n_2 - 2\):</strong> \(\mathbf{S}_W\) is singular. Use regularization: \(\mathbf{S}_W + \lambda\mathbf{I}_m\).
14ML0023	Basic	14ML-LDA	How do you compute the mean difference vector, watching dimensions?	From Step 1 means \(\mathbf{\mu}_1\) and \(\mathbf{\mu}_2\) (each \((1 \times m)\) row vectors):<br><br> \(\mathbf{d} = (\mathbf{\mu}_1 - \mathbf{\mu}_2)^T\)<br><br><strong>Dimensions:</strong> Transpose gives \((m \times 1)\) column vector.<br><strong>Alternative:</strong> If means computed as column vectors \((m \times 1)\), then \(\mathbf{d} = \mathbf{\mu}_1 - \mathbf{\mu}_2\) directly.<br><strong>Consistency is key:</strong> Ensure means and \(\mathbf{d}\) are column vectors for \(\mathbf{S}_W^{-1}\mathbf{d}\) multiplication.
14ML0024	Basic	14ML-LDA	What's the final computation with dimension verification?	Solve:<br><br> \(\mathbf{w} = \mathbf{S}_W^{-1} \mathbf{d}\)<br><br><strong>Dimension Check:</strong><br>- \(\mathbf{S}_W^{-1}\): \((m \times m)\)<br>- \(\mathbf{d}\): \((m \times 1)\)<br>- \(\mathbf{w}\): \((m \times 1)\) ✓<br>  <strong>Practical:</strong> Solve \(\mathbf{S}_W \mathbf{w} = \mathbf{d}\) (avoid explicit inverse).<br>  <strong>Normalize:</strong> \(\mathbf{w} \leftarrow \frac{\mathbf{w}}{\|\mathbf{w}\|}\) (direction matters, magnitude doesn't).
14ML0025	Basic	14ML-LDA	What dimension errors occur with \(\mathbf{X} - \mathbf{\mu}\) (without broadcasting)?	<strong>Error:</strong> \(\mathbf{X}\) is \((n \times m)\), \(\mathbf{\mu}\) is \((1 \times m)\) or \((m \times 1)\).<br>Subtraction fails: \((n \times m) - (1 \times m)\) is not defined in standard matrix algebra.<br><strong>Correct:</strong> Use broadcasting: \(\mathbf{X} - \mathbf{1}_n \mathbf{\mu}\)<br><br>- \(\mathbf{1}_n\): \((n \times 1)\) column vector<br>- \(\mathbf{\mu}\): \((1 \times m)\) row vector<br>- \(\mathbf{1}_n \mathbf{\mu}\): \((n \times m)\) replicated means<br>  <strong>Programming:</strong> In NumPy, \(\mathbf{X} - \mathbf{\mu}\) works with broadcasting if \(\mathbf{\mu}\) is \((1 \times m)\).
14ML0026	Basic	14ML-LDA	What are two critical pitfalls or considerations when applying LDA?	1. <strong>Singular \(\mathbf{S}_W\) Problem:</strong> If the number of samples \(N\) is less than the number of features \(d\), or if features are linearly dependent, \(\mathbf{S}_W\) is singular and cannot be inverted. <strong>Solution:</strong> Use PCA first for dimensionality reduction, or apply regularization (e.g., \(\mathbf{S}_W + \lambda\mathbf{I}\)).<br>2. <strong>Assumption of Common Covariance:</strong> LDA assumes all classes share the same covariance structure (i.e., \(\mathbf{S}_1 \approx \mathbf{S}_2\)). If this is severely violated (classes have very different shapes/spreads), the projection may be suboptimal. QDA (Quadratic DA) relaxes this assumption.<br>3. <strong>Normalization:</strong> The magnitude of \(\mathbf{w}\) from \(\mathbf{S}_W^{-1}(\mathbf{\mu}_1-\mathbf{\mu}_2)\) is arbitrary. The *direction* is what matters. For stability, often normalize \(\mathbf{w}\) to unit length.