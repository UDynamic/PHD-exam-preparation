{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084eb311",
   "metadata": {},
   "source": [
    "Okay, let's dive into Support Vector Machines (SVMs) in a simple and friendly way! ðŸ˜Š\n",
    "\n",
    "### What is a Support Vector Machine (SVM)?\n",
    "SVM is a **supervised machine learning algorithm** used for **classification** and **regression** tasks. But it's most famous for **classification**.\n",
    "\n",
    "Think of SVM as a **smart separator**. It tries to find the best line (or hyperplane in higher dimensions) that **clearly separates** different classes in your data.\n",
    "\n",
    "---\n",
    "\n",
    "### The Big Idea:\n",
    "Imagine you have two groups of points on a graph. You want to draw a line that separates them as clearly as possible. SVM finds the **widest possible margin** between the two groups.\n",
    "\n",
    "- The points that are **closest to the line** are called **support vectors**.\n",
    "- These are the **most important points** in defining the decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "### How SVM Works (Simplified):\n",
    "1. **Input Data**: You have labeled data (e.g., red dots and blue dots).\n",
    "2. **Find the Best Line**: SVM tries to find a line that **maximizes the margin** between the two classes.\n",
    "3. **Support Vectors**: The data points closest to the line are the **support vectors**.\n",
    "4. **Decision Boundary**: The line that separates the classes is the **decision boundary**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example in 2D:\n",
    "Letâ€™s say you have two classes: cats and dogs. Each has features like weight and height.\n",
    "\n",
    "SVM will try to find the **best line** that separates cats from dogs. The line that gives the **maximum distance** from the nearest cat and dog is the best one.\n",
    "\n",
    "---\n",
    "\n",
    "### Kernels in SVM:\n",
    "Sometimes, the data isn't linearly separable (you can't draw a straight line to separate the classes). Thatâ€™s where **kernels** come in!\n",
    "\n",
    "- **Kernel Trick**: It transforms the data into a higher dimension where it becomes linearly separable.\n",
    "- Common kernels:\n",
    "  - **Linear Kernel** (for simple data)\n",
    "  - **RBF (Radial Basis Function)** (for complex, non-linear data)\n",
    "  - **Polynomial Kernel**\n",
    "  -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e065842",
   "metadata": {},
   "source": [
    "Absolutely! Let's dive deeper into the **mathematical formulas** behind Support Vector Machines (SVM) and add **examples** to make it clearer. ðŸ˜Š\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® SVM Formulas Explained\n",
    "\n",
    "### 1. **Linear SVM (Hard Margin)**\n",
    "\n",
    "Letâ€™s assume we have a **linearly separable dataset**. The goal is to find a **hyperplane** that separates the two classes with the **maximum margin**.\n",
    "\n",
    "#### The Hyperplane Equation:\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$\n",
    "- $ w $: weight vector (normal to the hyperplane)\n",
    "- $ x $: input data point\n",
    "- $ b $: bias term\n",
    "\n",
    "#### The Margin:\n",
    "The **margin** is the distance between the hyperplane and the **closest data points** (support vectors). The margin is given by:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "#### Optimization Goal:\n",
    "We want to **maximize the margin**, which is equivalent to **minimizing $ \\|w\\| $**.\n",
    "\n",
    "So, the **optimization problem** is:\n",
    "$$\n",
    "\\text{Minimize: } \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "Subject to the constraints:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i\n",
    "$$\n",
    "Where:\n",
    "- $ y_i \\in \\{-1, 1\\} $: class labels\n",
    "- $ x_i $: data points\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Soft Margin SVM (for noisy data)**\n",
    "\n",
    "In real-world data, perfect separation is often impossible. So, we allow **some misclassifications** by introducing **slack variables** $ \\xi_i $.\n",
    "\n",
    "#### New Optimization Problem:\n",
    "$$\n",
    "\\text{Minimize: } \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "Subject to:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "- $ C $: regularization parameter (controls the trade-off between maximizing margin and minimizing errors)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Kernel Trick (for non-linear data)**\n",
    "\n",
    "When data is **not linearly separable**, we use **kernel functions** to map the data into a **higher-dimensional space** where it becomes linearly separable.\n",
    "\n",
    "#### Kernel Function:\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "$$\n",
    "Where $ \\phi $ is a mapping function to a higher-dimensional space.\n",
    "\n",
    "#### Common Kernels:\n",
    "- **Linear Kernel**: $ K(x_i, x_j) = x_i \\cdot x_j $\n",
    "- **RBF (Radial Basis Function)**:\n",
    "  $$\n",
    "  K(x_i, x_j) = \\exp\\left(-\\gamma \\|x_i - x_j\\|^2\\right)\n",
    "  $$\n",
    "- **Polynomial Kernel**:\n",
    "  $$\n",
    "  K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Example: Linear SVM with Math\n",
    "\n",
    "Letâ€™s take a simple 2D example:\n",
    "\n",
    "### Data:\n",
    "- Class A (label = +1): (1, 2), (2, 3)\n",
    "- Class B (label = -1): (3, 1), (4, 2)\n",
    "\n",
    "We want to find the best line that separates these two classes.\n",
    "\n",
    "### Step 1: Assume a hyperplane:\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = 0\n",
    "$$\n",
    "\n",
    "Letâ€™s say we find:\n",
    "- $ w = [1, 1] $\n",
    "- $ b = -4 $\n",
    "\n",
    "So the line is:\n",
    "$$\n",
    "x_1 + x_2 - 4 = 0\n",
    "$$\n",
    "\n",
    "### Step 2: Check the margin:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|} = \\frac{2}{\\sqrt{1^2 + 1^2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2}\n",
    "$$\n",
    "\n",
    "This is the **maximum margin** for this data.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Example: RBF Kernel in Python\n",
    "\n",
    "Letâ€™s use the **RBF kernel** to classify a non-linear dataset like the **moons dataset**.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate non-linear data\n",
    "X, y = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create SVM with RBF kernel\n",
    "model = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"SVM with RBF Kernel\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "Accuracy: 95.00%\n",
    "```\n",
    "\n",
    "Youâ€™ll see a **non-linear decision boundary** that separates the two moon-shaped clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ Summary of Key Formulas\n",
    "\n",
    "| Concept | Formula |\n",
    "|--------|---------|\n",
    "| Hyperplane | $ w \\cdot x + b = 0 $ |\n",
    "| Margin | $ \\frac{2}{\\|w\\|} $ |\n",
    "| Optimization (Hard Margin) | $ \\min \\frac{1}{2} \\|w\\|^2 $ with $ y_i (w \\cdot x_i + b) \\geq 1 $ |\n",
    "| Optimization (Soft Margin) | $ \\min \\frac{1}{2} \\|w\\|^2 + C \\sum \\xi_i $ |\n",
    "| RBF Kernel | $ K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $ |\n",
    "\n",
    "---\n",
    "\n",
    "If you want to see how **SVM works for regression** (called **SVR**), or how to **tune hyperparameters** like $ C $ and $ \\gamma $, just let me know! I'm here to help you learn step by step. ðŸ˜Šâœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e84a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fddfd74a",
   "metadata": {},
   "source": [
    "Absolutely! Let's go into the **academic version** of the **Support Vector Machine (SVM)** using the **Lagrange multipliers** method. This is the **mathematical foundation** of SVM and is essential for understanding how it works under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 1. **SVM Optimization Problem (Primal Form)**\n",
    "\n",
    "We want to find the **hyperplane** that **maximizes the margin** between two classes.\n",
    "\n",
    "Letâ€™s define:\n",
    "- $ x_i \\in \\mathbb{R}^n $: input data points\n",
    "- $ y_i \\in \\{-1, +1\\} $: class labels\n",
    "- $ w \\in \\mathbb{R}^n $: weight vector\n",
    "- $ b \\in \\mathbb{R} $: bias term\n",
    "\n",
    "The **hyperplane** is defined as:\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$\n",
    "\n",
    "The **margin** is:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "We want to **maximize the margin**, which is equivalent to **minimizing**:\n",
    "$$\n",
    "\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "Subject to the **constraints**:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 2. **Lagrangian Function**\n",
    "\n",
    "To solve this **constrained optimization problem**, we use **Lagrange multipliers**.\n",
    "\n",
    "We define the **Lagrangian** as:\n",
    "$$\n",
    "\\mathcal{L}(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha_i \\geq 0 $: Lagrange multipliers\n",
    "- The minus sign is because we are **minimizing** the objective function.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 3. **Dual Form of the Problem**\n",
    "\n",
    "We now **optimize the Lagrangian** with respect to $ w $ and $ b $, and then express the problem in terms of the **Lagrange multipliers** $ \\alpha_i $.\n",
    "\n",
    "### Step 1: Take partial derivatives\n",
    "\n",
    "Take the derivative of $ \\mathcal{L} $ with respect to $ w $ and $ b $, and set them to zero:\n",
    "\n",
    "#### Derivative w.r.t. $ w $:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0\n",
    "\\Rightarrow w = \\sum_{i=1}^{n} \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "#### Derivative w.r.t. $ b $:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "\\Rightarrow \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Plug back into the Lagrangian\n",
    "\n",
    "Now substitute $ w $ into the Lagrangian:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} \\left\\| \\sum_{i=1}^{n} \\alpha_i y_i x_i \\right\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "After simplifying, we get the **dual form** of the SVM optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 4. **Support Vectors and Decision Function**\n",
    "\n",
    "Only the **support vectors** (data points with $ \\alpha_i > 0 $) contribute to the solution.\n",
    "\n",
    "The **decision function** is:\n",
    "$$\n",
    "f(x) = \\text{sign} \\left( \\sum_{i=1}^{n} \\alpha_i y_i x_i \\cdot x + b \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x_i $ are the **support vectors**\n",
    "- $ \\alpha_i $ are the **Lagrange multipliers** for those points\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 5. **Kernel Trick (Non-Linear SVM)**\n",
    "\n",
    "For **non-linearly separable data**, we use the **kernel trick**.\n",
    "\n",
    "We replace the inner product $ x_i \\cdot x_j $ with a **kernel function** $ K(x_i, x_j) $, which implicitly maps the data to a higher-dimensional space.\n",
    "\n",
    "So the **dual problem becomes**:\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 6. **Example: Solving for Î± in a Small Dataset**\n",
    "\n",
    "Letâ€™s take a **small dataset** with 2 points in each class:\n",
    "\n",
    "### Class A (Label = +1):\n",
    "- $ x_1 = (1, 1) $\n",
    "- $ x_2 = (2, 2) $\n",
    "\n",
    "### Class B (Label = -1):\n",
    "- $ x_3 = (1, 3) $\n",
    "- $ x_4 = (2, 4) $\n",
    "\n",
    "We want to find the **Lagrange multipliers** $ \\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4 $.\n",
    "\n",
    "Letâ€™s assume:\n",
    "- $ \\alpha_1 = \\alpha_2 = \\alpha $, $ \\alpha_3 = \\alpha_4 = \\beta $\n",
    "\n",
    "From the constraint:\n",
    "$$\n",
    "\\sum \\alpha_i y_i = 0 \\Rightarrow \\alpha(1) + \\alpha(1) + \\beta(-1) + \\beta(-1) = 0\n",
    "\\Rightarrow 2\\alpha - 2\\beta = 0 \\Rightarrow \\alpha = \\beta\n",
    "$$\n",
    "\n",
    "Now plug into the dual objective function and solve for $ \\alpha $.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ Summary of Key Equations\n",
    "\n",
    "| Concept | Equation |\n",
    "|--------|----------|\n",
    "| Primal Objective | $ \\min \\frac{1}{2} \\|w\\|^2 $ |\n",
    "| Primal Constraints | $ y_i (w \\cdot x_i + b) \\geq 1 $ |\n",
    "| Lagrangian | $ \\mathcal{L} = \\frac{1}{2} \\|w\\|^2 - \\sum \\alpha_i [y_i (w \\cdot x_i + b) - 1] $ |\n",
    "| Dual Objective | $ \\max \\sum \\alpha_i - \\frac{1}{2} \\sum \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j $ |\n",
    "| Kernel Trick | $ x_i \\cdot x_j \\rightarrow K(x_i, x_j) $ |\n",
    "| Decision Function | $ f(x) = \\text{sign} \\left( \\sum \\alpha_i y_i x_i \\cdot x + b \\right) $ |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Final Notes\n",
    "\n",
    "- The **dual form** is used in practice because it allows for **kernelization**.\n",
    "- The **Lagrange multipliers** $ \\alpha_i $ are **non-zero only for support vectors**.\n",
    "- The **optimization is convex**, so there is a **unique global solution**.\n",
    "\n",
    "---\n",
    "\n",
    "If you want to see how this works in code (e.g., using **Sequential Minimal Optimization** or **CVXOPT**), or how to **solve it with a small dataset**, just let me know! I'm here to help you learn step by step. ðŸ˜Šâœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
