#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

24ML0001	Basic	24ML-Evaluation	What is a classifier in pattern recognition?	A function or algorithm that assigns an input feature vector \(\mathbf{x}\) to a discrete class label \(y\) from a set of classes \(\mathcal{C}_k\). It maps the feature space to a decision boundary.<br><br>\( y = f(\mathbf{x}), \quad y \in \{\mathcal{C}_1, \mathcal{C}_2, ..., \mathcal{C}_K\} \)
24ML0002	Basic	24ML-Evaluation	What are the standard class labels in binary classification?	Typically defined as Positive (P, 1) and Negative (N, 0). The goal is to distinguish between the presence (positive) and absence (negative) of a condition.<br><br>\( \text{Class Labels: } \{1, 0\} \text{ or } \{\text{Positive, Negative}\} \)
24ML0003	Basic	24ML-Evaluation	How is a 2x2 confusion matrix structured for binary classification?	Rows represent actual classes; columns represent predicted classes.<br><br><table><tr><th></th><th>Predicted Positive</th><th>Predicted Negative</th></tr><tr><td>Actual Positive</td><td>True Positive (TP)</td><td>False Negative (FN)</td></tr><tr><td>Actual Negative</td><td>False Positive (FP)</td><td>True Negative (TN)</td></tr></table><br>\( N = TP + TN + FP + FN \)
24ML0004	Basic	24ML-Evaluation	What is True Positive Rate (TPR) and how is it calculated?	The proportion of actual positives that are correctly identified by the classifier. Also called Sensitivity or Recall.<br><br>\( \text{TPR} = \frac{TP}{TP + FN} = \frac{TP}{P} \)
24ML0005	Basic	24ML-Evaluation	What is False Positive Rate (FPR) and how is it calculated?	The proportion of actual negatives that are incorrectly classified as positives. Also called Fall-out.<br><br>\( \text{FPR} = \frac{FP}{FP + TN} = \frac{FP}{N} \)
24ML0006	Basic	24ML-Evaluation	What is True Negative Rate (TNR) and how is it calculated?	The proportion of actual negatives that are correctly identified. Also called Specificity.<br><br>\( \text{TNR} = \frac{TN}{FP + TN} = \frac{TN}{N} \)
24ML0007	Basic	24ML-Evaluation	What is False Negative Rate (FNR) and how is it calculated?	The proportion of actual positives that are incorrectly classified as negatives. Also called Miss Rate.<br><br>\( \text{FNR} = \frac{FN}{TP + FN} = \frac{FN}{P} \)
24ML0008	Basic	24ML-Evaluation	What is Precision and how is it calculated?	The proportion of positive predictions that are actually correct.<br><br>\( \text{Precision} = \frac{TP}{TP + FP} \)
24ML0009	Basic	24ML-Evaluation	What are the complementary relationships between the four fundamental rates?	The four basic rates form complementary pairs:<br><br>\( \text{TPR} + \text{FNR} = 1 \quad \text{(Positives)} \)<br><br>\( \text{TNR} + \text{FPR} = 1 \quad \text{(Negatives)} \)
24ML0010	Basic	24ML-Evaluation	What is the difference between a hard classifier and a probabilistic/score-based classifier?	<ul><li><b>Hard classifier:</b> Directly outputs a discrete class label.</li><li><b>Score-based classifier:</b> Outputs a continuous value (probability or discriminant score) \(s(\mathbf{x})\) representing confidence. A threshold \(\tau\) is applied to convert the score to a label.</li></ul><br>\( \hat{y} = \begin{cases} 1 & \text{if } s(\mathbf{x}) \ge \tau \\ 0 & \text{if } s(\mathbf{x}) < \tau \end{cases} \)
24ML0011	Basic	24ML-Evaluation	What is a Receiver Operating Characteristic (ROC) graph?	A 2D graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold \(\tau\) is varied. It plots TPR (Sensitivity) on the y-axis against FPR (1-Specificity) on the x-axis.<br><br>\( \text{ROC Space: } (FPR, TPR) \in [0,1] \times [0,1] \)
24ML0012	Basic	24ML-Evaluation	What does a single point in ROC space represent?	A single point (FPR, TPR) corresponds to a specific confusion matrix generated by applying a single threshold \(\tau\) to the classifier's scores.
24ML0013	Basic	24ML-Evaluation	How is the ROC curve constructed from a score-based classifier?	<ol><li>Sort test instances by their score \(s(\mathbf{x})\) in descending order.</li><li>Sweep the threshold \(\tau\) from \(+\infty\) to \(-\infty\).</li><li>For each threshold, calculate the TPR and FPR.</li><li>Plot the resulting (FPR, TPR) pairs and connect them.</li></ol>
24ML0014	Basic	24ML-Evaluation	What are the coordinates of the extreme points on an ROC curve?	<ul><li><b>Threshold \(= +\infty\):</b> No positives predicted. (FPR = 0, TPR = 0). Bottom-left corner.</li><li><b>Threshold \(= -\infty\):</b> All instances predicted as positive. (FPR = 1, TPR = 1). Top-right corner.</li></ul>
24ML0015	Basic	24ML-Evaluation	What does the diagonal line \(y = x\) represent in ROC space?	Represents a random classifier. A classifier on this line has no discriminatory power (TPR = FPR). Any classifier below the diagonal performs worse than random guessing.
24ML0016	Basic	24ML-Evaluation	What point represents a perfect classifier in ROC space?	The top-left corner (FPR = 0, TPR = 1). This indicates all positives are correctly identified and no negatives are incorrectly flagged as positive.
24ML0017	Basic	24ML-Evaluation	What is the Area Under the ROC Curve (AUC)?	A single scalar value summarizing the entire ROC curve. It measures the overall performance of a classifier across all possible thresholds. Ranges from 0 to 1.<br><br>\( \text{AUC} \in [0, 1] \)
24ML0018	Basic	24ML-Evaluation	How is the AUC value interpreted?	<ul><li><b>AUC = 1.0:</b> Perfect separation (all positive scores > all negative scores).</li><li><b>AUC = 0.9 - 1.0:</b> Excellent classifier.</li><li><b>AUC = 0.8 - 0.9:</b> Good classifier.</li><li><b>AUC = 0.7 - 0.8:</b> Fair classifier.</li><li><b>AUC = 0.6 - 0.7:</b> Poor classifier.</li><li><b>AUC = 0.5:</b> Random classifier (no discrimination).</li><li><b>AUC < 0.5:</b> Worse than random (systematic error; invert decision rule).</li></ul>
24ML0019	Basic	24ML-Evaluation	What properties is AUC invariant to?	<ul><li><b>Monotonic transformations:</b> AUC is unchanged if the scores are transformed by any strictly increasing function (e.g., log, exponential, scaling).</li><li><b>Threshold invariance:</b> Summarizes performance independent of a specific threshold.</li></ul>
24ML0020	Basic	24ML-Evaluation	When should you use a Precision-Recall (PR) curve instead of an ROC curve?	ROC curves can be overly optimistic on highly imbalanced datasets because FPR (FP/N) becomes very small when N is large, even with many false positives. PR curves (Precision vs. Recall) focus on the positive class and are more informative for imbalanced data.
24ML0021	Basic	24ML-Evaluation	How does severe class imbalance affect the ROC curve?	The ROC curve remains visually optimistic. A large absolute number of False Positives results in a small FPR if the total Negatives (N) is huge. The curve can look excellent (high TPR, low FPR) while Precision is actually very low.
24ML0022	Basic	24ML-Evaluation	How is ROC analysis extended to multi-class classification?	<b>One-vs-All (OvA) strategy:</b><br><br><ol><li>Treat each class \(C_k\) as "positive" and all other classes as "negative".</li><li>Plot separate ROC curves for each class.</li><li>Calculate Macro-AUC (average of AUCs) or Micro-AUC (pool all decisions together).</li></ol>
24ML0023	Basic	24ML-Evaluation	How does overfitting affect ROC and AUC?	A classifier that memorizes the training data will achieve AUC = 1.0 on the training set but significantly lower AUC on the validation set. Always evaluate ROC/AUC on held-out test data.