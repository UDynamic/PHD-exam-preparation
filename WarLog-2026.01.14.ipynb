{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dad8e3",
   "metadata": {},
   "source": [
    "## Sensitibvity analysis method for backpropagation\n",
    "\n",
    "The sensitivity analysis in backpropagation involves calculating the sensitivity of the output with respect to the weights in the network. This is done by computing the partial derivatives of the output with respect to each weight. The general formulation for sensitivity analysis is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ E $ is the error (or loss) function.\n",
    "- $ w_{ij} $ is the weight connecting the $ i $-th neuron in the previous layer to the $ j $-th neuron in the current layer.\n",
    "- $ a_j $ is the activation of the $ j $-th neuron.\n",
    "- $ z_j $ is the weighted sum of inputs to the $ j $-th neuron.\n",
    "\n",
    "The first term, $ \\frac{\\partial E}{\\partial a_j} $, is the derivative of the error with respect to the activation of the neuron. The second term, $ \\frac{\\partial a_j}{\\partial z_j} $, is the derivative of the activation function with respect to the weighted sum. The third term, $ \\frac{\\partial z_j}{\\partial w_{ij}} $, is simply the input to the neuron $ a_i $.\n",
    "\n",
    "This formulation allows us to compute the sensitivity of the error with respect to each weight in the network, which is essential for updating the weights during the training process. Let me know if you have any specific questions about this formulation or would like to move on to the SSE method! ðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
