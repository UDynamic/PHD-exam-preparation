#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6
q6MjIb^kb**	Basic	ML-LinearRegression	What is the hypothesis function for simple linear regression? <br/>	"<span style=""color:gold""> \(h_{\theta}(x) = \theta_0 + \theta_1x\), where \(\theta_0\) is the intercept and \(\theta_1\) is the slope. This gives the prediction \(\hat{y}\)."
t%ObJ)|9.!**	Basic	ML-LinearRegression	What is the goal when training a linear regression model? <br/>	"<span style=""color:gold""> To find parameters \(\theta_0, \theta_1\) that minimize the cost function \(J(\theta_0, \theta_1)\)."
NwIyQjVnC7**	Basic	ML-LinearRegression	What is the Sum of Squared Errors (SSE) cost function? <br/>	"<span style=""color:gold""> \(J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2\). The \(\frac{1}{2}\) simplifies derivatives."
r$kXp8LmF2**	Basic	ML-LinearRegression	What is Mean Squared Error (MSE) and how does it differ from SSE? <br/>	"<span style=""color:gold""> \(J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2\). It averages error by dividing by \(m\), making it comparable across datasets."
s#dTz5QaW9**	Basic	ML-LinearRegression	What is Root Mean Squared Error (RMSE)? <br/>	"<span style=""color:gold""> \(J(\theta) = \sqrt{\frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2}\). It's in the same units as \(y\) for easier interpretation."
uPcB4fXyK1**	Basic	ML-LinearRegression	What is Mean Absolute Error (MAE) and its key difference from MSE? <br/>	"<span style=""color:gold""> \(J(\theta) = \frac{1}{m} \sum_{i=1}^m |h_\theta(x^{(i)}) - y^{(i)}|\). Unlike MSE, it's less sensitive to outliers but uses sign() in gradient."
vLmN3bZwJ0**	Basic	ML-LinearRegression	What are the 3 main steps of Gradient Descent? <br/>	"<span style=""color:gold""> 1. Initialize \(\theta\) randomly. 2. Update: \(\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)\). 3. Repeat until convergence."
wKjH2gYvI8**	Basic	ML-LinearRegression	What are the effects of too small or too large learning rate \(\alpha\)? <br/>	"<span style=""color:gold""> Too small: very slow convergence. Too large: may overshoot minimum, causing divergence or oscillation."
xZqA1sWdR7**	Basic	ML-LinearRegression	How do we know when Gradient Descent has converged? <br/>	"<span style=""color:gold""> When parameter changes are small: \(|\Delta\theta_j| \leq \epsilon\) for all \(j\), where \(\epsilon\) is a small tolerance."
yPvB0rXuO6**	Basic	ML-LinearRegression	Why must Gradient Descent update all parameters simultaneously? <br/>	"<span style=""color:gold""> To ensure each gradient \(\frac{\partial J}{\partial\theta_j}\) is computed using the same parameter values before any updates."
zOwN9qTtP5**	Basic	ML-LinearRegression	What is \(\frac{\partial J}{\partial\theta_0}\) for MSE cost? <br/>	"<span style=""color:gold""> \(\frac{\partial J}{\partial\theta_0} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})\)."
aQmM8pSsN4**	Basic	ML-LinearRegression	What is \(\frac{\partial J}{\partial\theta_j}\) for MSE cost? <br/>	"<span style=""color:gold""> \(\frac{\partial J}{\partial\theta_j} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}\)."
bRlL7oRrM3**	Basic	ML-LinearRegression	Why is MSE with linear regression guaranteed to find global minimum? <br/>	"<span style=""color:gold""> The cost function is convex (bowl-shaped), so Gradient Descent converges to global optimum, not local minima."
cSkK6nQqL2**	Basic	ML-LinearRegression	What's the difference between an epoch and an iteration? <br/>	"<span style=""color:gold""> **Iteration:** One parameter update. **Epoch:** One full pass through entire training dataset (contains multiple iterations in mini-batch GD)."
dTjJ5pPpK1**	Basic	ML-LinearRegression	How does Batch Gradient Descent compute gradients? <br/>	"<span style=""color:gold""> Uses entire training set: \(\theta_j := \theta_j - \alpha\cdot\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\)."
eUhI4oOoJ0**	Basic	ML-LinearRegression	What are advantages and disadvantages of BGD? <br/>	"<span style=""color:gold""> **Pros:** Accurate, guaranteed convergence for convex functions. **Cons:** Slow for large datasets, high memory usage, can get stuck in local minima for non-convex functions."
fViH3nNnI9**	Basic	ML-LinearRegression	How does SGD compute gradients differently from BGD? <br/>	"<span style=""color:gold""> Uses one random example: \(\theta_j := \theta_j - \alpha\cdot (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\)."
gWjG2mMmH8**	Basic	ML-LinearRegression	What are advantages and disadvantages of SGD? <br/>	"<span style=""color:gold""> **Pros:** Fast, low memory, can escape local minima. **Cons:** Noisy updates, may not converge exactly, needs data shuffling."
hXkF1lLlG7**	Basic	ML-LinearRegression	What is the compromise of Mini-Batch Gradient Descent? <br/>	"<span style=""color:gold""> Uses \(b\) examples per update: \(\theta_j := \theta_j - \alpha\cdot\frac{1}{b} \sum_{i=1}^b (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\). Balances speed and stability."
iYlE0kKkF6**	Basic	ML-LinearRegression	What is the closed-form solution for linear regression? <br/>	"<span style=""color:gold""> \(\theta = (X^T X)^{-1} X^T y\), where \(X\) is design matrix with rows as training examples."
jZmD9jJjE5**	Basic	ML-LinearRegression	What are advantages of the Normal Equation over Gradient Descent? <br/>	"<span style=""color:gold""> No iterations needed, no learning rate to tune, guaranteed exact solution (if \(X^TX\) invertible)."
kAnC8iIiD4**	Basic	ML-LinearRegression	What are limitations of the Normal Equation? <br/>	"<span style=""color:gold""> Computationally expensive for large features (\(O(n^3)\) for inversion), fails if \(X^TX\) non-invertible (redundant features or \(n > m\))."
lBoB7hHhC3**	Basic	ML-LinearRegression	When should you use Normal Equation versus Gradient Descent? <br/>	"<span style=""color:gold""> **Normal Equation:** Small number of features (\(n < 10,000\)). **Gradient Descent:** Large \(n\), very large \(m\), or non-linear models."
mCpA6gGgB2**	Basic	ML-LinearRegression	Why is MAE's gradient different from MSE? <br/>	"<span style=""color:gold""> MAE uses \(\text{sign}(h_\theta(x^{(i)}) - y^{(i)})\) instead of \((h_\theta(x^{(i)}) - y^{(i)})\), making it non-differentiable at zero but robust to outliers."
nDqZ5fFfA1**	Basic	ML-LinearRegression	When might you choose MAE over MSE? <br/>	"<span style=""color:gold""> Choose **MAE** when your data has many outliers (robust). Choose **MSE** for normally distributed errors (efficient, convex)."
oErY4eEeZ0**	Basic	ML-LinearRegression	What are typical mini-batch sizes and their effects? <br/>	"<span style=""color:gold""> 32, 64, 128. Larger batches: more stable but slower. Smaller batches: faster updates but noisier."
pFsX3dDdY9**	Basic	ML-LinearRegression	Why might \(X^TX\) be non-invertible? <br/>	"<span style=""color:gold""> 1. Redundant/linearly dependent features. 2. More features than examples (\(n > m\)). 3. Features with zero variance."
qGtW2cCcX8**	Basic	ML-LinearRegression	What's a good practice when implementing Gradient Descent? <br/>	"<span style=""color:gold""> Plot \(J(\theta)\) versus iterations to diagnose convergence issues and choose appropriate \(\alpha\)."
rHuV1bBbW7**	Basic	ML-LinearRegression	What is the core paradigm of supervised learning? <br/>	"<span style=""color:gold""> Learning a mapping from input variables \(x\) to an output variable \(y\), using a labeled dataset of example input-output pairs \(\{ (x^{(i)}, y^{(i)}) \}_{i=1}^m\)."
sIvU0aAaV6**	Basic	ML-LinearRegression	In linear regression, what is the form of the hypothesis function \(h_\theta(x)\) for a single feature? <br/>	"<span style=""color:gold""> \(h_\theta(x) = \theta_0 + \theta_1 x\), where \(\theta_0\) is the intercept/bias and \(\theta_1\) is the weight/slope. It predicts the output \(\hat{y}\)."
tJwT9zZzU5**	Basic	ML-LinearRegression	What is the purpose of a cost function \(J(\theta)\) in machine learning? <br/>	"<span style=""color:gold""> It quantifies the error between the model's predictions \(h_\theta(x)\) and the true targets \(y\). The learning goal is to find parameters \(\theta\) that minimize \(J(\theta)\)."
uKxS8yYyT4**	Basic	ML-LinearRegression	What is the formula for the Mean Squared Error (MSE) cost function for linear regression? <br/>	"<span style=""color:gold""> \(J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2\). The \(\frac{1}{2}\) is often included to cancel the factor of 2 from differentiation."
vLyR7xXxS3**	Basic	ML-LinearRegression	Describe the Gradient Descent algorithm in one sentence. <br/>	"<span style=""color:gold""> An iterative optimization algorithm that adjusts parameters \(\theta\) by taking steps proportional to the negative gradient of the cost function \(J(\theta)\)."
wMzQ6wWwR2**	Basic	ML-LinearRegression	Write the general parameter update rule for Gradient Descent. <br/>	"<span style=""color:gold""> \(\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j} J(\theta)\), repeated simultaneously for all \(j\). \(\alpha\) is the learning rate."
xNaP5vVvQ1**	Basic	ML-LinearRegression	What are the consequences of setting the learning rate \(\alpha\) too small or too large? <br/>	"<span style=""color:gold""> Too small: very slow convergence. Too large: can overshoot the minimum, causing divergence or oscillatory, slow convergence."
yObO4uUuP0**	Basic	ML-LinearRegression	What is the partial derivative \(\frac{\partial J}{\partial\theta_0}\) for the MSE cost with hypothesis \(h_\theta(x)=\theta_0+\theta_1x\)? <br/>	"<span style=""color:gold""> \(\frac{\partial J}{\partial\theta_0} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})\)."
zPcN3tTtO9**	Basic	ML-LinearRegression	What is the partial derivative \(\frac{\partial J}{\partial\theta_1}\) for the MSE cost with hypothesis \(h_\theta(x)=\theta_0+\theta_1x\)? <br/>	"<span style=""color:gold""> \(\frac{\partial J}{\partial\theta_1} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}\)."
aQdM2sSsN8**	Basic	ML-LinearRegression	Why is using (M)SE for linear regression advantageous? <br/>	"<span style=""color:gold""> The cost function \(J(\theta)\) is convex. Gradient Descent on a convex function is guaranteed to converge to the global minimum, not a local one."
bReL1rRrM7**	Basic	ML-LinearRegression	How does Batch Gradient Descent compute the gradient in each iteration? <br/>	"<span style=""color:gold""> It uses the **entire training set** to compute the gradient. The update for \(\theta_j\) sums over all \(m\) examples: \(\theta_j := \theta_j - \alpha\cdot\frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\)."
cSfK0qQqL6**	Basic	ML-LinearRegression	How does Stochastic Gradient Descent compute the gradient for an update? <br/>	"<span style=""color:gold""> It uses **one randomly chosen training example** \((x^{(i)}, y^{(i)})\) per update: \(\theta_j := \theta_j - \alpha\cdot (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\)."
dTgJ9pPpK5**	Basic	ML-LinearRegression	What is the compromise offered by Mini-batch Gradient Descent? <br/>	"<span style=""color:gold""> It uses a small random subset (mini-batch) of size \(b\) (e.g., 32) to compute the gradient: \(\theta_j := \theta_j - \alpha\cdot\frac{1}{b} \sum_{i=1}^{b} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\)."
eUhI8oOoJ4**	Basic	ML-LinearRegression	Distinguish between an *epoch* and an *iteration* in Gradient Descent. <br/>	"<span style=""color:gold"">**Iteration:** One update of the model parameters. **Epoch:** One full pass through the entire training dataset (may contain many iterations, e.g., in mini-batch GD)."
fViH7nNnI3**	Basic	ML-LinearRegression	What is the closed-form solution for linear regression parameters \(\theta\)? <br/>	"<span style=""color:gold""> \(\theta = (X^T X)^{-1} X^T y\), where \(X\) is the design matrix (with a column of 1s for the intercept) and \(y\) is the target vector."
gWjG6mMmH2**	Basic	ML-LinearRegression	What is the practical difference between Sum of Squared Errors (SSE) and Mean Squared Error (MSE)? <br/>	"<span style=""color:gold""> SSE: \(J = \frac{1}{2}\sum (h-y)^2\). MSE: \(J = \frac{1}{2m}\sum (h-y)^2\). MSE normalizes by dataset size \(m\), making cost comparable across different sized datasets. Their gradients differ by a factor of \(1/m\)."
hXkF5lLlG1**	Basic	ML-LinearRegression	What is the Mean Absolute Error (MAE) cost function and its key property? <br/>	"<span style=""color:gold""> \(J(\theta) = \frac{1}{m} \sum_{i=1}^m |h_\theta(x^{(i)}) - y^{(i)}|\). It is less sensitive to outliers than MSE, but its gradient involves the sign function, which is not differentiable at zero."
iYlE4kKkF0**	Basic	ML-LinearRegression	What is a critical implementation rule for the vanilla Gradient Descent update step? <br/>	"<span style=""color:gold""> All parameters \(\theta_j\) must be updated **simultaneously** using the *old* values of \(\theta\). Do not use a newly updated \(\theta_0\) to compute the update for \(\theta_1\)."
jZmD9jJjE9**	Basic	ML-LinearRegression	Compare the convergence behavior of BGD and SGD. <br/>	"<span style=""color:gold"">**BGD:** Converges smoothly to the minimum. **SGD:** Converges with noise; the path is erratic. It can escape shallow local minima but may oscillate near the global minimum."
kAnC8iIiD8**	Basic	ML-LinearRegression	When might you prefer Gradient Descent over the Normal Equation? <br/>	"<span style=""color:gold""> When \(n\) (number of features) is very large (>10,000), as inverting \(X^TX\) (\(O(n^3)\)) is computationally expensive. GD scales better."
lBoB7hHhC7**	Basic	ML-LinearRegression	What are two reasons the Normal Equation \(\theta = (X^TX)^{-1}X^Ty\) might fail? <br/>	"<span style=""color:gold""> 1. **Non-invertibility:** \(X^TX\) is singular if features are linearly dependent (redundant). 2. **Computational inefficiency:** If \(n\) is very large, the inversion is slow."
}