#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6
06ML0001	Basic	06ML-LinearRegression	What is the hypothesis function for simple linear regression? `<br/>`	 \(h_{\theta}(x) = \theta_0 + \theta_1x\), where \(\theta_0\) is the intercept and \(\theta_1\) is the slope. This gives the prediction \(\hat{y}\).
06ML0002	Basic	06ML-LinearRegression	What is the goal when training a linear regression model? `<br/>`	 To find parameters \(\theta_0, \theta_1\) that minimize the cost function \(J(\theta_0, \theta_1)\).
06ML0003	Basic	06ML-LinearRegression	What is the Sum of Squared Errors (SSE) cost function? `<br/>`	 \(J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2\). The \(\frac{1}{2}\) simplifies derivatives.
06ML0004	Basic	06ML-LinearRegression	What is Mean Squared Error (MSE) and how does it differ from SSE? `<br/>`	 \(J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2\). It averages error by dividing by \(m\), making it comparable across datasets.
06ML0005	Basic	06ML-LinearRegression	What is Root Mean Squared Error (RMSE)? `<br/>`	 \(J(\theta) = \sqrt{\frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2}\). It's in the same units as \(y\) for easier interpretation.
06ML0006	Basic	06ML-LinearRegression	What is Mean Absolute Error (MAE) and its key difference from MSE? `<br/>`	 \(J(\theta) = \frac{1}{m} \sum_{i=1}^m |h_\theta(x^{(i)}) - y^{(i)}|\). Unlike MSE, it's less sensitive to outliers but uses sign() in gradient.
06ML0007	Basic	06ML-LinearRegression	What are the 3 main steps of Gradient Descent? `<br/>`	 1. Initialize \(\theta\) randomly. 2. Update: \(\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)\). 3. Repeat until convergence.
06ML0008	Basic	06ML-LinearRegression	What are the effects of too small or too large learning rate \(\alpha\)? `<br/>`	 Too small: very slow convergence. Too large: may overshoot minimum, causing divergence or oscillation.
06ML0009	Basic	06ML-LinearRegression	How do we know when Gradient Descent has converged? `<br/>`	 When parameter changes are small: \(|\Delta\theta_j| \leq \epsilon\) for all \(j\), where \(\epsilon\) is a small tolerance.
06ML0010	Basic	06ML-LinearRegression	Why must Gradient Descent update all parameters simultaneously? `<br/>`	 To ensure each gradient \(\frac{\partial J}{\partial\theta_j}\) is computed using the same parameter values before any updates.
06ML0011	Basic	06ML-LinearRegression	What is \(\frac{\partial J}{\partial\theta_0}\) for MSE cost? `<br/>`	 \(\frac{\partial J}{\partial\theta_0} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})\).
06ML0012	Basic	06ML-LinearRegression	What is \(\frac{\partial J}{\partial\theta_j}\) for MSE cost? `<br/>`	 \(\frac{\partial J}{\partial\theta_j} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}\).
06ML0013	Basic	06ML-LinearRegression	Why is MSE with linear regression guaranteed to find global minimum? `<br/>`	 The cost function is convex (bowl-shaped), so Gradient Descent converges to global optimum, not local minima.
06ML0014	Basic	06ML-LinearRegression	What's the difference between an epoch and an iteration? `<br/>`	 **Iteration:** One parameter update. **Epoch:** One full pass through entire training dataset (contains multiple iterations in mini-batch GD).
06ML0015	Basic	06ML-LinearRegression	How does Batch Gradient Descent compute gradients? `<br/>`	 Uses entire training set: \(\theta_j := \theta_j - \alpha\cdot\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\).
06ML0016	Basic	06ML-LinearRegression	What are advantages and disadvantages of BGD? `<br/>`	 **Pros:** Accurate, guaranteed convergence for convex functions. **Cons:** Slow for large datasets, high memory usage, can get stuck in local minima for non-convex functions.
06ML0017	Basic	06ML-LinearRegression	How does SGD compute gradients differently from BGD? `<br/>`	 Uses one random example: \(\theta_j := \theta_j - \alpha\cdot (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\).
06ML0018	Basic	06ML-LinearRegression	What are advantages and disadvantages of SGD? `<br/>`	 **Pros:** Fast, low memory, can escape local minima. **Cons:** Noisy updates, may not converge exactly, needs data shuffling.
06ML0019	Basic	06ML-LinearRegression	What is the compromise of Mini-Batch Gradient Descent? `<br/>`	 Uses \(b\) examples per update: \(\theta_j := \theta_j - \alpha\cdot\frac{1}{b} \sum_{i=1}^b (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\). Balances speed and stability.
06ML0020	Basic	06ML-LinearRegression	What is the closed-form solution for linear regression? `<br/>`	 \(\theta = (X^T X)^{-1} X^T y\), where \(X\) is design matrix with rows as training examples.
06ML0021	Basic	06ML-LinearRegression	What are advantages of the Normal Equation over Gradient Descent? `<br/>`	 No iterations needed, no learning rate to tune, guaranteed exact solution (if \(X^TX\) invertible).
06ML0022	Basic	06ML-LinearRegression	What are limitations of the Normal Equation? `<br/>`	 Computationally expensive for large features (\(O(n^3)\) for inversion), fails if \(X^TX\) non-invertible (redundant features or \(n > m\)).
06ML0023	Basic	06ML-LinearRegression	When should you use Normal Equation versus Gradient Descent? `<br/>`	 **Normal Equation:** Small number of features (\(n < 10,000\)). **Gradient Descent:** Large \(n\), very large \(m\), or non-linear models.
06ML0024	Basic	06ML-LinearRegression	Why is MAE's gradient different from MSE? `<br/>`	 MAE uses \(\text{sign}(h_\theta(x^{(i)}) - y^{(i)})\) instead of \((h_\theta(x^{(i)}) - y^{(i)})\), making it non-differentiable at zero but robust to outliers.
06ML0025	Basic	06ML-LinearRegression	When might you choose MAE over MSE? `<br/>`	 Choose **MAE** when your data has many outliers (robust). Choose **MSE** for normally distributed errors (efficient, convex).
06ML0026	Basic	06ML-LinearRegression	What are typical mini-batch sizes and their effects? `<br/>`	 32, 64, 128. Larger batches: more stable but slower. Smaller batches: faster updates but noisier.
06ML0027	Basic	06ML-LinearRegression	Why might \(X^TX\) be non-invertible? `<br/>`	 1. Redundant/linearly dependent features. 2. More features than examples (\(n > m\)). 3. Features with zero variance.
06ML0028	Basic	06ML-LinearRegression	What's a good practice when implementing Gradient Descent? `<br/>`	 Plot \(J(\theta)\) versus iterations to diagnose convergence issues and choose appropriate \(\alpha\).
06ML0029	Basic	06ML-LinearRegression	What is the core paradigm of supervised learning? `<br/>`	 Learning a mapping from input variables \(x\) to an output variable \(y\), using a labeled dataset of example input-output pairs \(\{ (x^{(i)}, y^{(i)}) \}_{i=1}^m\).
06ML0030	Basic	06ML-LinearRegression	In linear regression, what is the form of the hypothesis function \(h_\theta(x)\) for a single feature? `<br/>`	 \(h_\theta(x) = \theta_0 + \theta_1 x\), where \(\theta_0\) is the intercept/bias and \(\theta_1\) is the weight/slope. It predicts the output \(\hat{y}\).
06ML0031	Basic	06ML-LinearRegression	What is the purpose of a cost function \(J(\theta)\) in machine learning? `<br/>`	 It quantifies the error between the model's predictions \(h_\theta(x)\) and the true targets \(y\). The learning goal is to find parameters \(\theta\) that minimize \(J(\theta)\).
06ML0032	Basic	06ML-LinearRegression	What is the formula for the Mean Squared Error (MSE) cost function for linear regression? `<br/>`	 \(J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2\). The \(\frac{1}{2}\) is often included to cancel the factor of 2 from differentiation.
06ML0033	Basic	06ML-LinearRegression	Describe the Gradient Descent algorithm in one sentence. `<br/>`	 An iterative optimization algorithm that adjusts parameters \(\theta\) by taking steps proportional to the negative gradient of the cost function \(J(\theta)\).
06ML0034	Basic	06ML-LinearRegression	Write the general parameter update rule for Gradient Descent. `<br/>`	 \(\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j} J(\theta)\), repeated simultaneously for all \(j\). \(\alpha\) is the learning rate.
06ML0035	Basic	06ML-LinearRegression	What are the consequences of setting the learning rate \(\alpha\) too small or too large? `<br/>`	 Too small: very slow convergence. Too large: can overshoot the minimum, causing divergence or oscillatory, slow convergence.
06ML0036	Basic	06ML-LinearRegression	What is the partial derivative \(\frac{\partial J}{\partial\theta_0}\) for the MSE cost with hypothesis \(h_\theta(x)=\theta_0+\theta_1x\)? `<br/>`	 \(\frac{\partial J}{\partial\theta_0} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})\).
06ML0037	Basic	06ML-LinearRegression	What is the partial derivative \(\frac{\partial J}{\partial\theta_1}\) for the MSE cost with hypothesis \(h_\theta(x)=\theta_0+\theta_1x\)? `<br/>`	 \(\frac{\partial J}{\partial\theta_1} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}\).
06ML0038	Basic	06ML-LinearRegression	Why is using (M)SE for linear regression advantageous? `<br/>`	 The cost function \(J(\theta)\) is convex. Gradient Descent on a convex function is guaranteed to converge to the global minimum, not a local one.
06ML0039	Basic	06ML-LinearRegression	What is the closed-form solution for linear regression parameters \(\theta\)? `<br/>`	 \(\theta = (X^T X)^{-1} X^T y\), where \(X\) is the design matrix (with a column of 1s for the intercept) and \(y\) is the target vector.
06ML0040	Basic	06ML-LinearRegression	What is the practical difference between Sum of Squared Errors (SSE) and Mean Squared Error (MSE)? `<br/>`	 SSE: \(J = \frac{1}{2}\sum (h-y)^2\). MSE: \(J = \frac{1}{2m}\sum (h-y)^2\). MSE normalizes by dataset size \(m\), making cost comparable across different sized datasets. Their gradients differ by a factor of \(1/m\).
06ML0041	Basic	06ML-LinearRegression	What are two reasons the Normal Equation \(\theta = (X^TX)^{-1}X^Ty\) might fail? `<br/>`	 1. **Non-invertibility:** \(X^TX\) is singular if features are linearly dependent (redundant). 2. **Computational inefficiency:** If \(n\) is very large, the inversion is slow.