{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b481ea1",
   "metadata": {},
   "source": [
    "Absolutely! Letâ€™s now **formally prove** the inequality:\n",
    "\n",
    "$$\n",
    "\\text{Error rate of 1-NN} \\leq p + (1 - p) = 2 \\times \\min(p, 1 - p)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Proof:**\n",
    "\n",
    "We are working in a **binary classification** setting with two classes: $ c_1 $ and $ c_2 $.\n",
    "\n",
    "Let:\n",
    "\n",
    "- $ p = P(c_1 | x) $: the probability that a test instance $ x $ belongs to class $ c_1 $\n",
    "- $ 1 - p = P(c_2 | x) $: the probability that $ x $ belongs to class $ c_2 $\n",
    "- The **Bayes error** is $ \\min(p, 1 - p) $\n",
    "\n",
    "We want to show that the **error rate of the 1-NN classifier** is bounded by:\n",
    "\n",
    "$$\n",
    "\\text{Error rate of 1-NN} \\leq 2 \\times \\min(p, 1 - p)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Step 1: Error Occurs When Nearest Neighbor is Wrong**\n",
    "\n",
    "The 1-NN classifier assigns the class of the **nearest neighbor** to the test instance.\n",
    "\n",
    "So, the 1-NN classifier **makes an error** if the nearest neighbor is from the **wrong class**.\n",
    "\n",
    "Letâ€™s define the **error probability** as:\n",
    "\n",
    "$$\n",
    "\\text{Error}(x) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if the nearest neighbor is from the wrong class} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Step 2: Worst-Case Scenario for 1-NN**\n",
    "\n",
    "In the **worst case**, the 1-NN classifier can make an error in **two ways**:\n",
    "\n",
    "1. **True class is $ c_1 $**, but the nearest neighbor is from $ c_2 $\n",
    "2. **True class is $ c_2 $**, but the nearest neighbor is from $ c_1 $\n",
    "\n",
    "Assuming the data is **infinite and well-sampled**, the probability of these events is:\n",
    "\n",
    "- $ P(\\text{error when true class is } c_1) = 1 - p $\n",
    "- $ P(\\text{error when true class is } c_2) = p $\n",
    "\n",
    "So the **maximum possible error rate** is:\n",
    "\n",
    "$$\n",
    "\\text{Error rate of 1-NN} \\leq (1 - p) + p = 2 \\times \\min(p, 1 - p)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Step 3: Final Inequality**\n",
    "\n",
    "Since $ \\min(p, 1 - p) $ is the **Bayes error**, we now have:\n",
    "\n",
    "$$\n",
    "\\text{Error rate of 1-NN} \\leq 2 \\times \\min(p, 1 - p)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Conclusion:**\n",
    "\n",
    "We have now **formally proven** that:\n",
    "\n",
    "$$\n",
    "\\text{Error rate of 1-NN} \\leq p + (1 - p) = 2 \\times \\min(p, 1 - p)\n",
    "$$\n",
    "\n",
    "This shows that the 1-NN classifier is **never more than twice as bad** as the optimal Bayes classifier.\n",
    "\n",
    "Would you like to see a **code example** or **plot** to visualize this? ðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
