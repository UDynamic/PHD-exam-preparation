#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

17ML0001	Basic	17ML-KNN&KDE	What defines a parametric method in machine learning?	It assumes the data follows a specific probability distribution with a fixed number of parameters (e.g., mean μ and covariance Σ for a Gaussian). Learning involves estimating these parameters from training data.<br><br>\( p(x | \theta) \)<br><br>where θ is a finite-dimensional parameter vector.
17ML0002	Basic	17ML-KNN&KDE	What defines a non-parametric method?	It makes no strong assumptions about the form of the underlying distribution. The model complexity grows with the amount of data. The "parameters" are essentially the training data itself or a structure derived from it.
17ML0003	Basic	17ML-KNN&KDE	Where is computational cost concentrated in parametric vs. non-parametric methods?	- **Parametric:** High cost during **training** (parameter estimation like MLE/MAP). Low cost during testing (evaluate simple function with learned parameters).<br>- **Non-Parametric:** Low/negligible cost during **training** (just store data). High cost during **testing** (search through/compare to stored data).<br><br>**Pitfall:** Believing non-parametric methods are always computationally cheaper. Their cost is deferred to test/prediction time.
17ML0004	Basic	17ML-KNN&KDE	When do we use Maximum Likelihood Estimation (MLE)?	When the *form* of the distribution (e.g., Gaussian) is known, but its *parameters* (μ, Σ) are unknown. MLE finds the parameter values that maximize the probability of observing the training data.<br><br>\( \theta_{MLE} = \arg\max_{\theta} p(D|\theta) \)
17ML0005	Basic	17ML-KNN&KDE	What is another name for non-parametric methods like K-NN and Parzen Window?	**Instance-based learning** or **memory-based learning**. They "learn" by memorizing instances (training data) and making predictions based on similarity/distance to these stored instances.
17ML0006	Basic	17ML-KNN&KDE	When should non-parametric methods be preferred?	1. When modeling the data with a known parametric form is difficult or unrealistic.<br>2. When the underlying data distribution is unknown or highly complex (multi-modal, non-standard).<br>3. When you have a large amount of data to mitigate their high test-time cost and data hunger.
17ML0007	Basic	17ML-KNN&KDE	What is the trade-off behind the statement: Non-parametric methods need "a large amount of data to mitigate their high test-time cost and data hunger"?	This describes two related challenges of non-parametric methods:<br><br>1. **Data Hunger:** To form accurate estimates (e.g., a smooth density in KDE or a reliable local vote in K-NN), they require **many data points** to densely populate the feature space, especially as dimensionality increases.<br>2. **High Test-Time Cost:** Prediction involves searching/processing many stored data points. More training data makes this **slower**.<br>   **Paradoxically,** you need **more data** to achieve good accuracy (mitigating the "hunger"), which in turn makes predictions **even slower** (exacerbating the "cost"). The trade-off is that the benefit of improved accuracy from more data often outweighs the computational penalty.<br><br>\( \text{Accuracy} \uparrow \text{ with } n \uparrow \quad \text{but} \quad \text{Speed} \downarrow \text{ with } n \uparrow \)<br><br>**Special Consideration:** This is why approximate nearest neighbor search (e.g., using KD-trees, locality-sensitive hashing) is crucial for scaling non-parametric methods to large datasets.
17ML0008	Basic	17ML-KNN&KDE	In the density estimation framework, how does K-NN estimate p(x)?	Fix the number of neighbors k, let the volume v around x expand until it contains those k points.<br><br>\( p(x) \approx \frac{k}{n v} \)<br><br>where v is the volume of the hypersphere containing the k nearest neighbors. Note: k is fixed, v varies.
17ML0009	Basic	17ML-KNN&KDE	In density estimation, what does the notation \(p(x)\) represent, and what is an example for K-NN?	\(p(x)\) represents the **probability density function (PDF)** at point \(x\). It tells us how dense the data is at that specific location in the feature space. A higher \(p(x)\) means points are more concentrated around \(x\).<br><br>**Example in K-NN:** In the formula \(p(x) \approx \frac{k}{n v}\):<br><br>- Imagine 1000 data points (\(n=1000\)) in 2D space.<br>- For a query point \(x\), we find its \(k=10\) nearest neighbors.<br>- We calculate the volume \(v\) of the smallest circle containing those 10 points (e.g., radius \(r=2\), so \(v=\pi r^2 \approx 12.57\)).<br>- The density estimate at \(x\) is:<br><br>  \( p(x) \approx \frac{10}{1000 \times 12.57} \approx 0.000796 \)<br><br>  This is a **low density** value, suggesting \(x\) is in a relatively sparse region. If the 10 neighbors were packed in a circle with \(v=0.5\), then \(p(x) \approx 0.02\), indicating a high-density region.<br><br>**Key:** This \(p(x)\) is **not a probability** (it can be >1) but a **density**. The integral of \(p(x)\) over all space equals 1.
17ML0010	Basic	17ML-KNN&KDE	In the density estimation framework, how does the Parzen Window estimate p(x)?	Fix the volume v (via bandwidth h), count how many data points k fall inside this volume centered at x.<br><br>\( p(x) \approx \frac{k}{n v} \quad \text{with} \quad v = h^d \)<br><br>where d is dimensionality. Here, v is fixed, k varies.
17ML0011	Basic	17ML-KNN&KDE	What does it mean for K-NN and Parzen Window to be "lazy learners"?	They perform **no explicit generalization** during training. They simply store the training data. All computation (distance calculations, kernel sums) is deferred until a prediction (test query) is made.
17ML0012	Basic	17ML-KNN&KDE	Write the complete mathematical decision rule for K-NN classification.	For a query point x, find its k nearest neighbors. Let \(k_j\) be the number of neighbors belonging to class \(C_j\). Assign x to the class with the highest count:<br><br>\( \text{Assign to class } j^* \text{ where } j^* = \arg\max_{j=1,\dots,c} k_j \)
17ML0013	Basic	17ML-KNN&KDE	Can K-NN produce non-linear decision boundaries?	Yes. Because the decision is based on local neighborhood votes, the boundary can be arbitrarily complex and non-linear, adapting to the data. The boundary is piecewise linear for specific metrics but can approximate any shape.
17ML0014	Basic	17ML-KNN&KDE	Under what conditions is K-NN highly sensitive to noise/outliers?	- **Very small k (k=1):** A single noisy neighbor dictates the class.<br>- **Small datasets:** Noise isn't "averaged out."<br>- **Irrelevant features / High dimensionality:** Distance becomes less meaningful, amplifying noise influence.
17ML0015	Basic	17ML-KNN&KDE	Why does "distance become less meaningful" in high dimensions or with irrelevant features, and how does this amplify noise in K-NN?	**1. The Geometry Problem:** In high dimensions, almost all points are **roughly the same distance** from each other. The volume grows exponentially, so data becomes sparse. The concept of "nearest" becomes unreliable.<br><br>**2. Irrelevant Features:** Adding features that don't correlate with the class label is like adding **pure noise dimensions**. These dimensions increase the distance between points *randomly*, drowning out the signal from relevant features.
17ML0016	Basic	17ML-KNN&KDE	Why is k typically chosen as an odd number in K-NN classification?	To **avoid ties** in the majority vote for binary classification problems. For multi-class, odd k doesn't guarantee no tie, but reduces the probability.
17ML0017	Basic	17ML-KNN&KDE	What is the formula for weighted Euclidean distance, and why use it?	\( d_w(x, x') = \sqrt{\sum_{i=1}^{d} w_i (x_i - x'_i)^2} \)<br><br>Weights \(w_i\) can be used to emphasize or de-emphasize certain features, potentially improving performance if some features are more relevant than others.
17ML0018	Basic	17ML-KNN&KDE	Conceptually, why does a large k lead to a simpler model in K-NN?	A large k means the prediction for x is based on a **larger region** of the input space. This **averages over more data points**, smoothing out fine details and noise, resulting in a less complex, lower-variance decision boundary.
17ML0019	Basic	17ML-KNN&KDE	How does increasing the training set size N generally affect K-NN accuracy?	It typically **increases accuracy** (up to a point). More data provides a denser sampling of the feature space, so the k nearest neighbors for a query point are more likely to be truly representative of the local region. This helps the estimate converge.
17ML0020	Basic	17ML-KNN&KDE	What is the standard K-NN regression prediction formula?	\( \hat{y} = \frac{1}{k} \sum_{j=1}^{k} y^{(j)} \)<br><br>where \(y^{(j)}\) are the target values of the k nearest neighbors to the query point x.
17ML0021	Basic	17ML-KNN&KDE	What is the general rule for deciding if an algorithm is suitable for a task?	Check if the algorithm's **inductive bias** matches the problem's structure. K-NN's bias is **local smoothness**—nearby points have similar outputs. This is reasonable for many regression problems, making it appropriate, though sensitive to irrelevant features and curse of dimensionality.
17ML0022	Basic	17ML-KNN&KDE	Write the general Parzen window (kernel density) estimator formula.	\( \hat{p}_n(x) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{h^d} K\left( \frac{x - x^{(i)}}{h} \right) \)<br><br>where \(K(\cdot)\) is the kernel function (e.g., Gaussian), \(h\) is the bandwidth, and \(d\) is dimensionality.
17ML0023	Basic	17ML-KNN&KDE	In simple terms, what does the Parzen window formula do, and can you show a small example?	**Simple explanation:** The formula **sums up little "bumps" (kernels)** placed on top of each data point. The height of these bumps at your query point `x` tells you how dense the data is there.<br><br>**Breaking it down:**<br><br>- \(\frac{1}{n}\): Averages over all `n` data points.<br>- \(\frac{1}{h^d}\): Adjusts for the window size (`h`) and dimensions (`d`) to keep the total area = 1.<br>- \(K\left( \frac{x - x^{(i)}}{h} \right)\): The kernel function. It gives a **weight** based on how close `x` is to data point `x^{(i)}`. Close points get high weight.<br><br>**Small Example (1D):**<br>Data points: `[2, 4, 7]` (n=3). Use Gaussian kernel with h=2.<br>Estimate density at x=3:<br><br>1. Distance from x=3 to each point:<br>   - To 2: |3-2|=1 → (1/h) = 1/2 = 0.5<br>   - To 4: |3-4|=1 → 0.5<br>   - To 7: |3-7|=4 → 4/2 = 2.0<br>2. Plug into Gaussian kernel \(K(u) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2}\):<br>   - K(0.5) ≈ 0.352<br>   - K(0.5) ≈ 0.352<br>   - K(2.0) ≈ 0.054<br>3. Apply formula: \(\hat{p}(3) = \frac{1}{3} \times \frac{1}{2^1} \times (0.352 + 0.352 + 0.054)\)<br>   \(\hat{p}(3) = \frac{1}{3} \times 0.5 \times 0.758 = 0.126\)<br><br>This means the estimated density at x=3 is 0.126. Near the cluster of points {2,4}, density is higher; far from point 7, density contribution is small.
17ML0024	Basic	17ML-KNN&KDE	Describe the step-by-step algorithm for classification using Parzen windows.	1. For each class \(C_j\), compute its density estimate \(\hat{p}(x|C_j)\) using only data from \(C_j\).<br>2. Estimate class priors \(\hat{P}(C_j)\) (e.g., \(n_j/n\)).<br>3. For a test point x, compute the posterior for each class: \(\hat{P}(C_j|x) \propto \hat{p}(x|C_j) \hat{P}(C_j)\).<br>4. Assign x to the class with the highest \(\hat{P}(C_j|x)\).
17ML0025	Basic	17ML-KNN&KDE	For D={2,3,4,8,10,11,12}, h=3, rectangular kernel, estimate p(x=1)	Rectangular kernel: \(K(u)=1\) if \(|u| \le 0.5\), else 0. \(u = (x-x^{(i)})/h = (1-x^{(i)})/3\).<br>We check each point: For x=1, only points where \(|1-x^{(i)}| \le 1.5\) contribute (i.e., x=2,3). So k=2.<br><br>\( \hat{p}(1) = \frac{1}{n h} \cdot k = \frac{1}{7 \cdot 3} \cdot 2 = \frac{2}{21} \)
17ML0026	Basic	17ML-KNN&KDE	What is the more general name for the Parzen window method?	**Kernel Density Estimation (KDE).** "Kernel" refers to the window function \(K(\cdot)\) that weights nearby points.
17ML0027	Basic	17ML-KNN&KDE	When classifying with density estimates (Parzen/K-NN), do you just pick the class with highest density \(\hat{p}(x|C_k)\) at point x?	No. You must multiply each class density by its **prior probability** \(\hat{P}(C_k)\) (estimated from class frequencies). Pick the class with the highest product \(\hat{p}(x|C_k) \cdot \hat{P}(C_k)\), which is proportional to the posterior probability.<br><br>**K-NN's majority vote** implements this rule implicitly because common classes (high prior) are more likely to appear among the k neighbors.<br>**Parzen Window** implements it explicitly by multiplying the density estimate for each class by its estimated prior.<br><br>**Pitfall:** Choosing based solely on \(\hat{p}(x|C_k)\) is wrong. A rare class might have high density at a point, but the class could still be unlikely overall.
17ML0028	Basic	17ML-KNN&KDE	What is the mathematical property of a "soft" kernel window in KDE, and what does the argument \(u\) represent?	A soft window uses a kernel function \(K(u)\) that is **continuous** and gives non-zero weight to all points, with the weight **smoothly decaying** as distance increases. It has no hard boundary.<br><br>**Definition of \(u\):** \(u = \frac{x - x^{(i)}}{h}\), where:<br><br>- \(x\) is the query point where we estimate density<br>- \(x^{(i)}\) is the \(i\)-th training data point<br>- \(h\) is the bandwidth parameter<br>  Thus, \(u\) is the **normalized distance** from the query point to a training point.<br><br>**Gaussian (soft) example:**<br><br>\( K_{\text{Gauss}}(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}u^2} \)<br><br>All points contribute, but far points (large \(|u|\)) have exponentially small weight.<br><br>**Rectangular (hard) example:**<br><br>\( K_{\text{Rect}}(u) = \begin{cases} 1 & \text{if } |u| \leq 0.5 \\ 0 & \text{otherwise} \end{cases} \)<br><br>Points outside the cutoff (\(|u| > 0.5\)) get exactly zero weight.
17ML0029	Basic	17ML-KNN&KDE	How does the bandwidth h (σ) affect a Gaussian KDE estimate visually and conceptually?	- **Small h:** Kernels are narrow spikes on each data point. Density estimate is **noisy, multi-modal, follows data closely** (low bias, high variance). Risk of overfitting.<br>- **Large h:** Kernels are wide, overlapping blobs. Density estimate is **over-smoothed, unimodal, loses detail** (high bias, low variance). Risk of underfitting.
17ML0030	Basic	17ML-KNN&KDE	What is the general d-dimensional Gaussian kernel formulation for KDE, including the bandwidth parameter \(\sigma\)?	The Gaussian kernel with bandwidth \(\sigma\) (often denoted \(h\)) is:<br><br>\( K_{\sigma}(u) = \frac{1}{(\sqrt{2\pi}\sigma)^d} \exp\left(-\frac{\|u\|^2}{2\sigma^2}\right) \)<br><br>where:<br><br>- \(u = \frac{x - x^{(i)}}{\sigma}\) is the normalized distance vector<br>- \(d\) is the dimensionality<br>- \(\|u\|\) is the Euclidean norm<br>- \(\sigma = h\) controls the "width" or spread of the kernel<br><br>The full Parzen estimate becomes:<br><br>\( \hat{p}(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{\sigma^d} K\left(\frac{x - x^{(i)}}{\sigma}\right) \)<br><br>**Note:** The \(\frac{1}{(\sqrt{2\pi}\sigma)^d}\) term ensures the kernel integrates to 1 over \(\mathbb{R}^d\).
17ML0031	Basic	17ML-KNN&KDE	How does the bandwidth parameter \(\sigma\) affect the Gaussian KDE estimate both visually and statistically?	**Small \(\sigma\) (narrow kernel):**<br><br>- Visually: Sharp, spiky peaks at each data point<br>- Statistically: Low bias (fits training data closely), high variance<br>- Risk: Overfitting, noisy estimate, poor generalization<br>- Density estimate: \(\hat{p}(x)\) has many modes<br><br>**Large \(\sigma\) (wide kernel):**<br><br>- Visually: Over-smoothed, flat surface<br>- Statistically: High bias (misses details), low variance<br>- Risk: Underfitting, loses local structure<br>- Density estimate: \(\hat{p}(x)\) approaches a single broad bump<br><br>**Optimal \(\sigma\):** Balances bias-variance trade-off, minimizing test error (found via cross-validation).<br><br>**Rule of thumb:** \(\hat{\sigma} \approx 1.06 \cdot \hat{s} \cdot n^{-1/5}\) for 1D data (Silverman's rule), where \(\hat{s}\) is sample standard deviation.
17ML0032	Basic	17ML-KNN&KDE	How does window size h affect training error and generalization?	- **Very small h:** Near-zero training error (density peaks at each data point). Poor generalization (high test error due to variance).<br>- **Very large h:** High training error (oversmoothed). Poor generalization (high test error due to bias).<br>- **Optimal h:** Balances bias and variance, minimizing test error (found via cross-validation).
17ML0033	Basic	17ML-KNN&KDE	What is a key theoretical advantage of the Parzen window method?	Given a sufficient amount of data (\(n \rightarrow \infty\)) and an appropriately shrinking bandwidth (\(h_n \rightarrow 0\), \(n h_n^d \rightarrow \infty\)), the estimate \(\hat{p}_n(x)\) **converges to the true density** \(p(x)\) for almost any distribution. It's a universal approximator.
17ML0034	Basic	17ML-KNN&KDE	Why does KDE suffer exponentially from dimensionality?	In high dimensions (\(d\) large), the volume \(v = h^d\) becomes astronomically large or small. To maintain a meaningful density estimate (non-zero, finite), you need **exponentially more data points** (\(n\)) to fill the space and get points inside the kernel's effective volume.
17ML0035	Basic	17ML-KNN&KDE	Summarize the key operational difference between K-NN and Parzen Window (KDE).	- **K-NN:** Fix **k** (number of neighbors), let **volume v** adapt. Density estimate is **inversely proportional to v**.<br>- **Parzen/KDE:** Fix **volume v** (via h), let **count k** adapt. Density estimate is **proportional to k**.
17ML0036	Basic	17ML-KNN&KDE	**PITFALL:** Correct this statement: "Parametrics need training data for learning parameters to decide for the new data (for test almost all or some training data is used)."	**Correction:** Parametric methods use training data **only during training** to estimate parameters. For testing, they use **only the learned parameters**, discarding the training data. It is non-parametric (instance-based) methods that use **all or some training data directly during testing**.
17ML0037	Basic	17ML-KNN&KDE	Is this statement correct: "K-NN is sensitive to noise in low dimensions or features"?	**No, this is incorrect.** K-NN is actually **most sensitive to noise in HIGH dimensions** (many features), not low dimensions. This is due to the **curse of dimensionality**:<br><br>1. **In low dimensions:** Data is relatively dense, distances are meaningful, and the "nearest neighbors" are truly similar. Noise has limited impact.<br>2. **In high dimensions:**<br><br>   - All points become almost equally distant<br>   - The distance metric becomes dominated by noise in irrelevant features<br>   - The concept of "nearest neighbor" loses meaning<br>   - A few noisy features can completely corrupt the distance calculation.
