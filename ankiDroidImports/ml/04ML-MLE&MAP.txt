#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

3p2gU!8F>G	Basic	ML-MLE&MAP	What is the fundamental goal of parameter estimation in statistical modeling?	To infer the unknown parameters \(\theta\) of a probability distribution, given observed data \(x\). We want to find the most plausible \(\theta\) that explains our data.
1*KQmC9?&f	Basic	ML-MLE&MAP	What is the likelihood function \(p(x | \theta )\)?	It is a function of the parameters \(\theta\). It measures the probability (or probability density) of observing the given data \(x\) under a specific setting of the parameters. It is <i>not</i> a probability distribution over \(\theta\).<br><br>\(p(x | \theta )\)
j$%p6A>E6y	Basic	ML-MLE&MAP	What is the prior distribution \(p(\theta)\)?	It represents our belief about the parameters \(\theta\) <i>before</i> seeing any data. It incorporates domain knowledge or assumptions.<br><br>\(p(\theta)\)
s+t:@Jq}3k	Basic	ML-MLE&MAP	State Bayes' Theorem as it applies to parameter estimation.	It updates our belief about parameters \(\theta\) after observing data \(x\). The prior \(p(\theta)\) is combined with the likelihood \(p(x | \theta)\) to form the posterior.<br><br>\(p(\theta | x) = \frac{p(x | \theta ) \cdot p(\theta)}{p(x)}\)
9:q=PW9T0)	Basic	ML-MLE&MAP	What is the posterior distribution \(p(\theta | x)\)?	It represents our updated belief about the parameters \(\theta\) <i>after</i> observing the data \(x\). It is the main output of Bayesian inference, combining prior knowledge and empirical evidence.<br><br>\(p(\theta | x)\)
2F6oQi$[!U	Basic	ML-MLE&MAP	What is the evidence or marginal likelihood \(p(x)\)?	The total probability of the data under all possible parameter settings, weighted by the prior. It serves as a normalizing constant for the posterior.<br><br>\(p(x) = \int p(x | \theta) p(\theta) d\theta\)
vm!@1O)FYP	Basic	ML-MLE&MAP	What is the Maximum A Posteriori (MAP) estimate \(\hat{\theta}_{MAP}\)?	It is the mode (peak) of the posterior distribution. It is the single most probable parameter value given the data and the prior.<br><br>\(\hat{\theta}_{MAP} = \arg \max_{\theta} p(\theta | x)\)
T@F4<U(u(q	Basic	ML-MLE&MAP	How is the MAP estimate computed in practice, ignoring constants?	Since the evidence \(p(x)\) is independent of \(\theta\), we maximize the unnormalized posterior: the likelihood times the prior.<br><br>\(\hat{\theta}_{MAP} = \arg \max_{\theta} p(\theta | x) = \arg \max_{\theta} [ p(x | \theta ) \cdot p(\theta) ]\)
V#x8bC;H3D	Basic	ML-MLE&MAP	What is the Maximum Likelihood Estimate (MLE), and how does it relate to MAP?	MLE is \(\hat{\theta}_{MLE} = \arg \max_{\theta} p(x | \theta)\). It is a special case of MAP where the prior \(p(\theta)\) is a uniform (flat) distribution. MAP introduces a regularization effect via the prior.
dX$r`/B][J	Basic	ML-MLE&MAP	Why do we typically maximize the log-posterior instead of the posterior directly?	The logarithm is a monotonically increasing function, so the argmax is unchanged. It converts products into sums, which is numerically more stable, especially with many data points.<br><br>\(\hat{\theta}_{MAP} = \arg \max_{\theta} [ \log p(x | \theta) + \log p(\theta) ]\)
TdR1@\cE=P	Basic	ML-MLE&MAP	What is a key pitfall in interpreting a single MAP estimate versus the full posterior?	The MAP estimate is just one point. It discards all other information about the shape of the posterior (e.g., uncertainty, skewness, multimodality). The full posterior distribution provides a complete picture of parameter uncertainty.
N;c*9g,^5q	Basic	ML-MLE&MAP	When can the MAP estimate be problematic?	When the prior \(p(\theta)\) is chosen poorly (too informative or misspecified) or when the data is scarce, the MAP estimate can be overly influenced by the prior. It's crucial to perform sensitivity analysis.