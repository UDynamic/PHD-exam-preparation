{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c475676e",
   "metadata": {},
   "source": [
    "Goal of ML: \n",
    "\n",
    "$Error_{train} \\approx Error_{test} \\approx = 0 $\n",
    "\n",
    "Bias: the error of model detecting true patterns in the data. \n",
    "\n",
    "Variance: amount of change in the model's estimate after chainging the training dataset.\n",
    "\n",
    "---\n",
    "\n",
    "Overfitting(High Variance):\n",
    "\n",
    "model is too sensitive to the training data (=> low variance :the model is stable and not overly influenced by the training data)\n",
    "\n",
    "$Error_{train}: \\text{low} \\quad \\text{but} \\quad Error_{test}: \\text{high} $\n",
    "$Error_{train} \\ll Error_{test} $\n",
    "\n",
    "* Overly complex model.(=> complex hypothesis space.)\n",
    "* Too many features.\n",
    "* Too few training data. (=> use simple models for few data)\n",
    "* Too much noize in the data\n",
    "* \n",
    "---\n",
    "\n",
    "Underfitting (High Bias):\n",
    "Model is unable to detect true patterns in the data.(=> Low Bias: the model is complex enough to capture the patterns.)\n",
    "\n",
    "* $Error_{train} \\approx Error_{test} \\gg 0$\n",
    "* when both $Error_{train}$ and $Error_{test}$ is high.\n",
    "* Simpl model.(=> simple hypothesis space.)\n",
    "---\n",
    "Model complexity and Bias variance trade off:\n",
    "\n",
    "* when model is too simple. (Low Complexity → High Bias, Low Variance)\n",
    "* High Complexity → Low Bias, High Variance\n",
    "---\n",
    "Generalization Error: \n",
    "error a model makes on new, unseen data.\n",
    "\n",
    "$Generalization Error = (Bias)^2 + Variance + Irreducible Error$\n",
    "\n",
    "reducible errors: Bias and Variance. we can reduce them by improving the model.\n",
    "Irreducible Error: is due to noise in the data. we can't eliminate it.\n",
    "\n",
    "How to Reduce Generalization Error:\n",
    "1. Increase model complexity (if underfitting).\n",
    "2. Simplify model (if overfitting).\n",
    "3. Use more training data (reduces variance).\n",
    "4. Use regularization (e.g., L1/L2) to penalize overly complex models.\n",
    "5. Cross-validation to evaluate generalization performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Bias-Variance with Infinite Data\n",
    "**Front:** What happens to bias and variance as training data size approaches infinity? <br/>\n",
    "**Back:** **Variance approaches zero, bias remains unchanged.** With infinite data, the model sees all possible variations, making predictions stable (zero variance). However, bias—the model's fundamental inability to capture the true relationship—is determined by model capacity, not data quantity.\n",
    "\n",
    "## Learning Curve Convergence\n",
    "**Front:** What do learning curves show as training size increases toward infinity? <br/>\n",
    "**Back:** The gap between training and test error (variance) narrows to zero, and both converge to the same value—the bias of the model. If both errors remain high at convergence, the model has high bias (underfitting).\n",
    "\n",
    "## Model Selection with Infinite Data\n",
    "**Front:** What model should you choose if you have access to infinite data? <br/>\n",
    "**Back:** **The most complex model possible** (lowest bias). With infinite data, variance is zero, so the bias-variance tradeoff disappears. Complexity is free—choose the model class that can best approximate the true function.\n",
    "\n",
    "## Irreducible Error Persistence\n",
    "**Front:** What happens to irreducible error as training data approaches infinity? <br/>\n",
    "**Back:** **Irreducible error remains constant.** This is the inherent noise in the data generation process (e.g., measurement error, true stochasticity). No amount of data can eliminate it, setting a lower bound on possible error.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa4322c",
   "metadata": {},
   "source": [
    "## Goal of Machine Learning\n",
    "**Front:** What is the ideal outcome for a well-performing machine learning model? <br/>\n",
    "**Back:** $Error_{train} \\approx Error_{test} \\approx 0$. The model performs equally well on the data it was trained on and on new, unseen data.\n",
    "\n",
    "## Bias (Concept)\n",
    "**Front:** In the context of model error, what is *bias*? <br/>\n",
    "**Back:** The error arising from the model's inability to represent the true underlying pattern in the data. A high-bias model is too simplistic.\n",
    "\n",
    "## Variance (Concept)\n",
    "**Front:** In the context of model error, what is *variance*? <br/>\n",
    "**Back:** The error arising from the model's sensitivity to fluctuations in the specific training dataset. A high-variance model is overly complex and fits the noise.\n",
    "\n",
    "## Overfitting\n",
    "**Front:** What is overfitting, and what are the typical error characteristics? <br/>\n",
    "**Back:** The model learns the training data (including noise) too well. $Error_{train}$ is very low, but $Error_{test}$ is significantly higher ($Error_{train} \\ll Error_{test}$). It corresponds to **high variance**.\n",
    "\n",
    "## Common Causes of Overfitting\n",
    "**Front:** List three common causes of overfitting (high variance). <br/>\n",
    "**Back:** 1. Model is too complex (e.g., high-degree polynomial). 2. Too many features relative to data points. 3. Training data contains significant noise.\n",
    "\n",
    "## Underfitting\n",
    "**Front:** What is underfitting, and what are the typical error characteristics? <br/>\n",
    "**Back:** The model fails to learn the underlying pattern in the data. Both $Error_{train}$ and $Error_{test}$ are high ($Error_{train} \\approx Error_{test} \\gg 0$). It corresponds to **high bias**.\n",
    "\n",
    "## Common Causes of Underfitting\n",
    "**Front:** List two common causes of underfitting (high bias). <br/>\n",
    "**Back:** 1. Model is too simple (e.g., linear model for a complex problem). 2. Too few features to capture the relevant patterns.\n",
    "\n",
    "## Bias-Variance Trade-off (Complexity)\n",
    "**Front:** How does model complexity relate to bias and variance? <br/>\n",
    "**Back:** **Low Complexity:** High Bias, Low Variance (underfitting). **High Complexity:** Low Bias, High Variance (overfitting). The goal is to find the optimal complexity that balances both.\n",
    "\n",
    "## Generalization Error Decomposition\n",
    "**Front:** What is the canonical decomposition of the expected generalization error? <br/>\n",
    "**Back:** $E[(y - \\hat{f}(x))^2] = \\text{Bias}[\\hat{f}(x)]^2 + \\text{Var}[\\hat{f}(x)] + \\sigma^2$. Where $\\sigma^2$ is the irreducible error.\n",
    "\n",
    "## Reducible vs. Irreducible Error\n",
    "**Front:** Distinguish between reducible and irreducible error in the bias-variance decomposition. <br/>\n",
    "**Back:** **Reducible:** Bias and Variance. We can reduce these by improving the model. **Irreducible:** $\\sigma^2$, the inherent noise in the data. It sets a lower bound on total error and cannot be eliminated.\n",
    "\n",
    "## Strategy: Reduce High Bias\n",
    "**Front:** What actions can you take if your model is underfitting (high bias)? <br/>\n",
    "**Back:** Increase model complexity: Use a more powerful model (e.g., higher degree polynomial, neural network), add more relevant features, or reduce regularization strength.\n",
    "\n",
    "## Strategy: Reduce High Variance\n",
    "**Front:** What actions can you take if your model is overfitting (high variance)? <br/>\n",
    "**Back:** Simplify the model: Use fewer features, get more training data, increase regularization strength (L1/L2), or use a simpler model class.\n",
    "\n",
    "## Infinite Data Limit (Bias & Variance)\n",
    "**Front:** As the training dataset size $m \\to \\infty$, what happens to bias and variance? <br/>\n",
    "**Back:** **Variance $\\to 0$.** **Bias remains constant.** With infinite data, the model estimate stabilizes, but its fundamental representational capacity (bias) is unchanged.\n",
    "\n",
    "## Learning Curve Convergence\n",
    "**Front:** On a learning curve (error vs. training size), what do the converging training and test errors represent as $m \\to \\infty$? <br/>\n",
    "**Back:** They converge to the same value: the **bias** of the model plus the **irreducible error** $\\sigma^2$. The gap between them (due to variance) vanishes.\n",
    "\n",
    "## Model Selection with Infinite Data\n",
    "**Front:** If you had infinite data, what type of model would you choose and why? <br/>\n",
    "**Back:** The most complex, lowest-bias model possible (e.g., a very large neural network). With infinite data, variance is eliminated, so you only need to minimize bias.\n",
    "\n",
    "## Pitfall: Misdiagnosing High Error\n",
    "**Front:** If both training and test error are high, is the problem always high bias? What's a key consideration? <br/>\n",
    "**Back:** Not always. While high bias is a likely cause, it could also be due to **very high variance coupled with poorly representative data**. Check if the training error itself is high (true bias) or if there's a massive gap to test error (variance).\n",
    "\n",
    "## Pitfall: The \"No Free Lunch\" of Complexity\n",
    "**Front:** Why isn't \"always use the most complex model\" a good strategy in practice? <br/>\n",
    "**Back:** Because in the real world with **finite data**, increasing complexity reduces bias but increases variance. The optimal model is the one that best balances this trade-off for your specific dataset size and problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
