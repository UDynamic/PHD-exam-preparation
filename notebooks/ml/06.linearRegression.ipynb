{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e30e58",
   "metadata": {},
   "source": [
    "Hypothesis: \n",
    "\n",
    "$ h_{\\theta}(x) = \\theta_0 + \\theta_1x $\n",
    "\n",
    "$ h_{\\theta}(x) = \\hat{y} $\n",
    "\n",
    "---\n",
    "Cost function + GD for parameters:\n",
    "\n",
    "SSE :Sum squared error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{2} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2 \\Rightarrow \\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "MSE :Mean Sum squared error ($ \\frac{1}{2m} $ instead of $ \\frac{1}{2} $)\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2 \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "RMSE: root of mean squared error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\sqrt{MSE} = \\sqrt{\\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2} \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{2 \\sqrt{ \\frac{1}{2m} \\sum (h - y)^2 }} \\cdot \\frac{1}{m} \\sum (h - y) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{2 \\sqrt{ \\frac{1}{2m} \\sum (h - y)^2 }} \\cdot \\frac{1}{m} \\sum (h - y) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "MAE: mean average error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i}) \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\text{sign}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\text{sign}(h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "goal: $ \\min{J(\\theta_0, \\theta_1)} $\n",
    "\n",
    "---\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "1. starts with random variables for $\\theta_0, \\theta_1$\n",
    "2. Reapeat untill convergence:\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1) $$\n",
    "\n",
    "$ \\alpha $: learning rate\n",
    "learning rate too small: slow convergence\n",
    "learning rate too big: divergence or slow convergence\n",
    "\n",
    "convergence : $\\Delta{\\theta_1} \\leq \\epsilon$\n",
    "\n",
    "* at each update, both $\\theta_0, \\theta_1$ must be updated simultaniously.\n",
    "$$ \\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1) \\to \\Delta{\\theta_0} = - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "$$ \\theta_1 = \\theta_1 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1) \\to \\Delta{\\theta_1} = - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "---\n",
    "* in linear regression, cost function is a convex function, so in condition of convergence, it's converged to the global optimum.\n",
    "\n",
    "* Batch GD: in each epoch, all the data is used for Updates.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
