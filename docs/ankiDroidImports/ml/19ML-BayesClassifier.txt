#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

19ML0001	Basic	19ML-BayesClassifier	What is the fundamental formula of Bayes' theorem for a class \(C_k\) given data point \(\mathbf{x}\)?	The posterior probability \(P(C_k | \mathbf{x})\) is proportional to the likelihood times the prior.<br><br>\( P(C_k | \mathbf{x}) = \frac{p(\mathbf{x} | C_k) P(C_k)}{p(\mathbf{x})} \)<br><br>where:<br>\(p(\mathbf{x} | C_k)\) is the likelihood.<br>\(P(C_k)\) is the prior.<br>\(p(\mathbf{x})\) is the evidence (or marginal likelihood).
19ML0002	Basic	19ML-BayesClassifier	What is \(P(\mathbf{x})\), the evidence, in terms of the class priors and likelihoods?	The marginal likelihood \(P(\mathbf{x})\) is obtained by summing (or integrating) the joint distribution over all possible classes. It is the sum of the numerators of Bayes' theorem for each class, not the sum of posteriors.<br><br>\( P(\mathbf{x}) = \sum_{k=1}^{K} P(\mathbf{x} \cap C_k) = \sum_{k=1}^{K} p(\mathbf{x} | C_k) P(C_k) \)<br><br>This ensures the posterior probabilities \(P(C_k | \mathbf{x})\) sum to 1.
19ML0003	Basic	19ML-BayesClassifier	Why is the evidence \(p(\mathbf{x})\) often omitted when comparing posteriors for different classes?	Because \(p(\mathbf{x}) = \sum_k p(\mathbf{x} | C_k) P(C_k)\) is constant for all classes given \(\mathbf{x}\). For comparison/decision making, we only need the numerator: \(p(\mathbf{x} | C_k) P(C_k)\). The evidence acts as a normalization constant ensuring posteriors sum to 1.
19ML0004	Basic	19ML-BayesClassifier	Is \(P(\mathbf{x})\) the sum of the posteriors \(P(C_k | \mathbf{x})\) for all classes?	No, this is incorrect and a common pitfall. The sum of posteriors is always 1 for any \(\mathbf{x}\):<br><br>\( \sum_{k=1}^{K} P(C_k | \mathbf{x}) = 1 \)<br><br>The evidence \(P(\mathbf{x})\) is a probability (density) for \(\mathbf{x}\) itself. It is a normalization constant that *makes* the posteriors sum to 1, not the result of that sum.
19ML0005	Basic	19ML-BayesClassifier	What is the fundamental logical error in stating "\(P(\mathbf{x}) = \sum_k P(C_k | \mathbf{x})\)"?	It reverses the dependency. In probability theory, \(P(\mathbf{x})\) is calculated **first** from the generative model (priors and likelihoods). Then, posteriors \(P(C_k | \mathbf{x})\) are derived **from** \(P(\mathbf{x})\) via Bayes' theorem. The sum of posteriors is always 1 by definition, but this is a **consequence** of normalization using \(P(\mathbf{x})\), not a way to compute \(P(\mathbf{x})\).
19ML0006	Basic	19ML-BayesClassifier	What is the optimal (Bayes) classification rule to minimize probability of error?	Assign \(\mathbf{x}\) to the class \(C_k\) with the highest posterior probability.<br><br>\( \text{Assign to } C_k \text{ if } P(C_k | \mathbf{x}) > P(C_j | \mathbf{x}) \text{ for all } j \neq k \)<br><br>This is equivalent to maximizing the numerator \(p(\mathbf{x} | C_k) P(C_k)\).
19ML0007	Basic	19ML-BayesClassifier	Does the function \(p(\mathbf{x}|C_k)\) give the probability of \(\mathbf{x}\) belonging to class \(C_k\)?	**No—critical distinction.** \(p(\mathbf{x}|C_k)\) is the **likelihood**, not the posterior probability of class membership.<br><br>- **Likelihood \(p(\mathbf{x}|C_k)\):** "Given that the class is \(C_k\), what is the probability (density) of observing this specific \(\mathbf{x}\)?"<br>- **Posterior \(P(C_k|\mathbf{x})\):** "Given that we observed this specific \(\mathbf{x}\), what is the probability it belongs to class \(C_k\)?"<br><br>**Likelihood is not a probability over classes.** It does not sum to 1 over \(k\); it integrates to 1 over \(\mathbf{x}\) for each fixed \(k\).
19ML0008	Basic	19ML-BayesClassifier	Why is it incorrect to say "\(p(\mathbf{x}|C_k)\) gives the probability that \(\mathbf{x}\) belongs to class \(C_k\)"?	This statement **reverses the conditioning**. The correct statement is:<br><br>**Correct:** \(p(\mathbf{x}|C_k)\) is the probability of observing \(\mathbf{x}\) **given that** the class is \(C_k\).<br><br>**Incorrect:** \(p(\mathbf{x}|C_k)\) is the probability that the class is \(C_k\) **given that** we observed \(\mathbf{x}\).<br><br>The latter is \(P(C_k|\mathbf{x})\), which requires **Bayes' theorem** to compute from the likelihood, prior, and evidence:<br><br>\( P(C_k|\mathbf{x}) = \frac{p(\mathbf{x}|C_k)P(C_k)}{p(\mathbf{x})} \)<br><br>**Analogy:** If \(C_k\) = "rain" and \(\mathbf{x}\) = "wet grass", then:<br><br>- \(p(\text{wet grass}|\text{rain})\) is high (likelihood)<br>- \(P(\text{rain}|\text{wet grass})\) is also high but not the same quantity—it depends on the prior probability of rain and other causes of wet grass
19ML0009	Basic	19ML-BayesClassifier	In binary classification, if we say "\(p\) is the probability of \(\mathbf{x}\) belonging to \(C_1\)", which quantity are we actually referring to?	**We are (usually) referring to the posterior \(P(C_1|\mathbf{x})\), not the likelihood.**<br><br>This is evident from common statements:<br><br>- "If \(p > 0.5\), classify as \(C_1\)" → This compares posterior probability to 0.5 threshold<br>- "\(1-p\) is the probability of belonging to \(C_2\)" → This uses the fact that posteriors sum to 1: \(P(C_2|\mathbf{x}) = 1 - P(C_1|\mathbf{x})\)<br><br>**Important:** This only works for **posteriors**. Likelihoods do **not** satisfy \(p(\mathbf{x}|C_2) = 1 - p(\mathbf{x}|C_1)\). Likelihoods are independent probability densities, each integrating to 1 over \(\mathbf{x}\).<br><br>**Rule of thumb:** If you're making a decision about class membership, you're working with **posteriors** (or their proportional equivalents \(p(\mathbf{x}|C_k)P(C_k)\)). If you're generating data from a known class, you're working with **likelihoods**.
19ML0010	Basic	19ML-BayesClassifier	In binary classification, what is the error probability if \(p = P(C_1 | \mathbf{x})\)?	If we predict the class with the higher posterior, the probability of error for that specific \(\mathbf{x}\) is the posterior of the *other* class.<br><br>\( P(\text{error} | \mathbf{x}) = \min(p, 1-p) \)<br><br>If \(p > 1-p\), we predict \(C_1\) and the error is \(1-p\).
19ML0011	Basic	19ML-BayesClassifier	For \(K>2\) classes, what is the error probability at a point \(\mathbf{x}\) for the Bayes classifier?	The error is one minus the posterior of the winning class.<br><br>\( P(\text{error} | \mathbf{x}) = 1 - \max_k P(C_k | \mathbf{x}) = \sum_{j \neq k^*} P(C_j | \mathbf{x}) \)<br><br>where \(k^*\) is the class with the highest posterior.
19ML0012	Basic	19ML-BayesClassifier	How is the optimal decision boundary defined for two classes using Bayes?	The boundary is the set of points \(\mathbf{x}\) where the posteriors are equal.<br><br>\( P(C_1 | \mathbf{x}) = P(C_2 | \mathbf{x}) \)<br><br>Using Bayes theorem, this simplifies to:<br><br>\( p(\mathbf{x} | C_1) P(C_1) = p(\mathbf{x} | C_2) P(C_2) \)
19ML0013	Basic	19ML-BayesClassifier	For two classes, how do the likelihood ratio and prior ratio relate at the decision boundary?	At the Bayes decision boundary:<br><br>\( \frac{p(\mathbf{x} | C_1)}{p(\mathbf{x} | C_2)} = \frac{P(C_2)}{P(C_1)} \)<br><br>* The class boundary shifts if priors are unequal; higher prior pulls the boundary towards the other class.<br>* Stronger prior belief in \(C_1\) means you need less evidence (a lower likelihood ratio) from the data \(\mathbf{x}\) to classify as \(C_1\)
19ML0014	Basic	19ML-BayesClassifier	How do unequal class priors \(P(C_1)\) and \(P(C_2)\) affect the optimal Bayes decision boundary?	The boundary shifts. A higher prior for a class expands its decision region. The boundary moves *away* from the more probable class and *towards* the less probable class.<br>**Intuition:** Stronger prior belief in \(C_1\) means you need less evidence (a lower likelihood ratio) from the data \(\mathbf{x}\) to classify as \(C_1\). The threshold on the likelihood ratio is lowered, favoring \(C_1\) over more of the input space.
19ML0015	Basic	19ML-BayesClassifier	Clarify: "Higher prior pulls the boundary towards the other class." Which way does it move?	This phrasing can be ambiguous. Precise statement: **A higher prior for class \(C_k\) causes the decision boundary to shift *away from* the densest regions of \(p(\mathbf{x}|C_k)\) and *into* the region more typical of the other class.** For example, if \(P(C_1)\) increases, the boundary moves toward the mean of \(C_2\), effectively giving \(C_1\) a larger share of the feature space.
19ML0016	Basic	19ML-BayesClassifier	If class priors \(P(C_k)\) are unknown/assumed equal, what simplified rule is used?	Maximize the likelihood. Assign \(\mathbf{x}\) to class \(C_k\) with the highest \(p(\mathbf{x} | C_k)\).<br><br>\( \text{Assign to } C_k \text{ if } p(\mathbf{x} | C_k) > p(\mathbf{x} | C_j) \text{ for all } j \neq k \)<br><br>This is a special case of the Bayes rule with uniform priors.
19ML0017	Basic	19ML-BayesClassifier	Is it true that \(p(\mathbf{x} | C_1) = 1 - p(\mathbf{x} | C_2)\) for binary classification?	No. This is a common pitfall. Likelihoods \(p(\mathbf{x} | C_k)\) are probability *densities* (or masses) for \(\mathbf{x}\) under class \(C_k\). They are not complementary probabilities. Each is defined by its own class-conditional distribution and they do not sum to 1 over classes.
19ML0018	Basic	19ML-BayesClassifier	What probabilities *do* sum to 1 for a binary classification problem at a point \(\mathbf{x}\)?	The posterior probabilities sum to 1.<br><br>\( P(C_1 | \mathbf{x}) + P(C_2 | \mathbf{x}) = 1 \)<br><br>If \(p\) denotes \(P(C_1 | \mathbf{x})\), then \(P(C_2 | \mathbf{x}) = 1 - p\). This is true for posteriors, not likelihoods.
19ML0019	Basic	19ML-BayesClassifier	What is a loss matrix (or risk matrix) \(L\) and why is it used?	A matrix \(L_{kj}\) specifying the penalty (loss) incurred for assigning a pattern to class \(C_j\) when its true class is \(C_k\). It generalizes the 0-1 loss (where error counts equally) to applications where some mistakes are more costly than others (e.g., medical diagnosis).
19ML0020	Basic	19ML-BayesClassifier	Derive the Bayes decision boundary for two classes with Gaussian class-conditional densities:<br><br>\(p(\mathbf{x}|C_1) \sim \mathcal{N}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1)\)<br><br>\(p(\mathbf{x}|C_2) \sim \mathcal{N}(\boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2)\)<br><br>using the equality \(p(\mathbf{x}|C_1)P(C_1) = p(\mathbf{x}|C_2)P(C_2)\).	**Step 1:** Write the Gaussian PDFs explicitly.<br><br>\( \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}_1|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu}_1)^T \boldsymbol{\Sigma}_1^{-1}(\mathbf{x} - \boldsymbol{\mu}_1)\right) P(C_1) \)<br><br>\( = \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}_2|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu}_2)^T \boldsymbol{\Sigma}_2^{-1}(\mathbf{x} - \boldsymbol{\mu}_2)\right) P(C_2) \)<br><br>**Step 2:** Take the natural logarithm of both sides.<br><br>\( \ln P(C_1) - \frac{1}{2}\ln|\boldsymbol{\Sigma}_1| - \frac{1}{2}(\mathbf{x} - \boldsymbol{\mu}_1)^T \boldsymbol{\Sigma}_1^{-1}(\mathbf{x} - \boldsymbol{\mu}_1) \)<br><br>\( = \ln P(C_2) - \frac{1}{2}\ln|\boldsymbol{\Sigma}_2| - \frac{1}{2}(\mathbf{x} - \boldsymbol{\mu}_2)^T \boldsymbol{\Sigma}_2^{-1}(\mathbf{x} - \boldsymbol{\mu}_2) \)<br><br>The \((2\pi)^{d/2}\) terms cancel.<br><br>**Step 3:** Rearrange to get the general quadratic discriminant function.<br><br>\( (\mathbf{x} - \boldsymbol{\mu}_1)^T \boldsymbol{\Sigma}_1^{-1}(\mathbf{x} - \boldsymbol{\mu}_1) - (\mathbf{x} - \boldsymbol{\mu}_2)^T \boldsymbol{\Sigma}_2^{-1}(\mathbf{x} - \boldsymbol{\mu}_2) \)<br><br>\( = \ln\frac{|\boldsymbol{\Sigma}_2|}{|\boldsymbol{\Sigma}_1|} + 2\ln\frac{P(C_1)}{P(C_2)} \)<br><br>This is a **quadratic decision boundary** in \(\mathbf{x}\).
19ML0021	Basic	19ML-BayesClassifier	Simplify the Gaussian decision boundary when \(\boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 = \boldsymbol{\Sigma}\) (equal covariance matrices).	**Step 1:** Starting from the log form:<br><br>\( -\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu}_1)^T \boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu}_1) + \ln P(C_1) \)<br><br>\( = -\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu}_2)^T \boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu}_2) + \ln P(C_2) \)<br><br>**Step 2:** Cancel \(-\frac{1}{2}\) and expand the quadratic forms:<br><br>\( \mathbf{x}^T\boldsymbol{\Sigma}^{-1}\mathbf{x} - 2\boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}\mathbf{x} + \boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 - 2\ln P(C_1) \)<br><br>\( = \mathbf{x}^T\boldsymbol{\Sigma}^{-1}\mathbf{x} - 2\boldsymbol{\mu}_2^T\boldsymbol{\Sigma}^{-1}\mathbf{x} + \boldsymbol{\mu}_2^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2 - 2\ln P(C_2) \)<br><br>**Step 3:** Cancel \(\mathbf{x}^T\boldsymbol{\Sigma}^{-1}\mathbf{x}\) and rearrange:<br><br>\( 2(\boldsymbol{\mu}_2 - \boldsymbol{\mu}_1)^T\boldsymbol{\Sigma}^{-1}\mathbf{x} = \boldsymbol{\mu}_2^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2 - \boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 + 2\ln\frac{P(C_1)}{P(C_2)} \)<br><br>**Step 4:** This is a **linear decision boundary**:<br><br>\( (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T\boldsymbol{\Sigma}^{-1}\mathbf{x} = \frac{1}{2}(\boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2) + \ln\frac{P(C_2)}{P(C_1)} \)
19ML0022	Basic	19ML-BayesClassifier	Further simplify the decision boundary when \(\boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 = \boldsymbol{\Sigma}\) and \(P(C_1) = P(C_2)\).	With equal priors, \(\ln\frac{P(C_2)}{P(C_1)} = \ln(1) = 0\).<br><br>The boundary simplifies to:<br><br>\( (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T\boldsymbol{\Sigma}^{-1}\mathbf{x} = \frac{1}{2}(\boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2) \)<br><br>This can be rewritten as:<br><br>\( (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T\boldsymbol{\Sigma}^{-1}\left(\mathbf{x} - \frac{\boldsymbol{\mu}_1 + \boldsymbol{\mu}_2}{2}\right) = 0 \)<br><br>**Interpretation:** The decision boundary is a **hyperplane** passing through the midpoint \(\frac{\boldsymbol{\mu}_1 + \boldsymbol{\mu}_2}{2}\) and orthogonal to \(\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)\).<br><br>In the case of isotropic covariance \(\boldsymbol{\Sigma} = \sigma^2\mathbf{I}\), this further reduces to:<br><br>\( (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T\mathbf{x} = \frac{1}{2}(||\boldsymbol{\mu}_1||^2 - ||\boldsymbol{\mu}_2||^2) \)<br><br>which is a hyperplane perpendicular to the line joining the means.
19ML0023	Basic	19ML-BayesClassifier	Under what conditions is the Bayes optimal decision boundary **linear** for binary classification?	The Bayes decision boundary is linear if and only if the class-conditional densities belong to **any exponential family with equal dispersion parameters**. The most common cases:<br><br>**1. Gaussian with equal covariance matrices:**<br><br>\( p(\mathbf{x}|C_1) \sim \mathcal{N}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma}), \quad p(\mathbf{x}|C_2) \sim \mathcal{N}(\boldsymbol{\mu}_2, \boldsymbol{\Sigma}) \)<br><br>The log ratio \(\ln\frac{p(\mathbf{x}|C_1)}{p(\mathbf{x}|C_2)}\) is linear in \(\mathbf{x}\).<br><br>**2. Other exponential family distributions with equal scale:**<br><br>- Bernoulli (Naive Bayes) with equal variance<br>- Poisson with equal rate parameters<br>- Multinomial with equal dispersion<br><br>**Key insight:** The boundary is linear when the **only difference between classes is the mean parameter**, not the covariance/scale/dispersion.
19ML0024	Basic	19ML-BayesClassifier	Show mathematically why equal covariance matrices lead to a linear decision boundary.	**Step 1:** The Bayes decision rule compares \(p(\mathbf{x}|C_1)P(C_1)\) and \(p(\mathbf{x}|C_2)P(C_2)\). Taking logs:<br><br>\( \ln p(\mathbf{x}|C_1) - \ln p(\mathbf{x}|C_2) > \ln\frac{P(C_2)}{P(C_1)} \)<br><br>**Step 2:** For Gaussians with \(\boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 = \boldsymbol{\Sigma}\):<br><br>\( -\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu}_1)^T\boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu}_1) + \frac{1}{2}(\mathbf{x} - \boldsymbol{\mu}_2)^T\boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu}_2) > \ln\frac{P(C_2)}{P(C_1)} \)<br><br>**Step 3:** Expand and cancel \(\mathbf{x}^T\boldsymbol{\Sigma}^{-1}\mathbf{x}\):<br><br>\( (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T\boldsymbol{\Sigma}^{-1}\mathbf{x} - \frac{1}{2}(\boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2) > \ln\frac{P(C_2)}{P(C_1)} \)<br><br>**Step 4:** Rearrange to form:<br><br>\( (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T\boldsymbol{\Sigma}^{-1}\mathbf{x} > \frac{1}{2}(\boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2) + \ln\frac{P(C_2)}{P(C_1)} \)<br><br>**Result:** The left side is **linear in x**, right side is constant → **linear decision boundary**.
19ML0025	Basic	19ML-BayesClassifier	Given binary classes with equal covariance matrix, class means \(\boldsymbol{\mu}_1 = [0,0]^T\), \(\boldsymbol{\mu}_2 = [3,3]^T\), and precision matrix \(\boldsymbol{\Sigma}^{-1} = \begin{bmatrix} 0.95 & -0.15 \\ -0.15 & 0.55 \end{bmatrix}\), classify the point \(\mathbf{x} = [1, 2.2]^T\) assuming equal priors \(P(C_1) = P(C_2) = 0.5\).	**Step 1: Recall the decision rule for equal covariance and equal priors.**<br><br>With equal priors and equal covariance, the Bayes optimal decision is to assign \(\mathbf{x}\) to the class with the **smaller Mahalanobis distance**:<br><br>\( \text{Assign to } C_1 \text{ if } D_M(\mathbf{x}, \boldsymbol{\mu}_1) < D_M(\mathbf{x}, \boldsymbol{\mu}_2) \)<br><br>where \(D_M(\mathbf{x}, \boldsymbol{\mu}_k) = \sqrt{(\mathbf{x} - \boldsymbol{\mu}_k)^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}_k)}\).<br><br>**Step 2: Compute Mahalanobis distance to \(C_1\).**<br><br>\(\mathbf{x} - \boldsymbol{\mu}_1 = \begin{bmatrix} 1 - 0 \\ 2.2 - 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 2.2 \end{bmatrix}\)<br><br>First compute \((\mathbf{x} - \boldsymbol{\mu}_1)^T \boldsymbol{\Sigma}^{-1}\):<br><br>\( \begin{bmatrix} 1 & 2.2 \end{bmatrix} \begin{bmatrix} 0.95 & -0.15 \\ -0.15 & 0.55 \end{bmatrix} = \begin{bmatrix} (1)(0.95) + (2.2)(-0.15) & (1)(-0.15) + (2.2)(0.55) \end{bmatrix} \)<br><br>\( = \begin{bmatrix} 0.95 - 0.33 & -0.15 + 1.21 \end{bmatrix} = \begin{bmatrix} 0.62 & 1.06 \end{bmatrix} \)<br><br>Now compute \((\mathbf{x} - \boldsymbol{\mu}_1)^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}_1)\):<br><br>\( \begin{bmatrix} 0.62 & 1.06 \end{bmatrix} \begin{bmatrix} 1 \\ 2.2 \end{bmatrix} = (0.62)(1) + (1.06)(2.2) = 0.62 + 2.332 = 2.952 \)<br><br>Thus, \(D_M^2(\mathbf{x}, \boldsymbol{\mu}_1) = 2.952\), so \(D_M(\mathbf{x}, \boldsymbol{\mu}_1) = \sqrt{2.952} \approx 1.718\).<br><br>**Step 3: Compute Mahalanobis distance to \(C_2\).**<br><br>\(\mathbf{x} - \boldsymbol{\mu}_2 = \begin{bmatrix} 1 - 3 \\ 2.2 - 3 \end{bmatrix} = \begin{bmatrix} -2 \\ -0.8 \end{bmatrix}\)<br><br>First compute \((\mathbf{x} - \boldsymbol{\mu}_2)^T \boldsymbol{\Sigma}^{-1}\):<br><br>\( \begin{bmatrix} -2 & -0.8 \end{bmatrix} \begin{bmatrix} 0.95 & -0.15 \\ -0.15 & 0.55 \end{bmatrix} = \begin{bmatrix} (-2)(0.95) + (-0.8)(-0.15) & (-2)(-0.15) + (-0.8)(0.55) \end{bmatrix} \)<br><br>\( = \begin{bmatrix} -1.9 + 0.12 & 0.3 - 0.44 \end{bmatrix} = \begin{bmatrix} -1.78 & -0.14 \end{bmatrix} \)<br><br>Now compute \((\mathbf{x} - \boldsymbol{\mu}_2)^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}_2)\):<br><br>\( \begin{bmatrix} -1.78 & -0.14 \end{bmatrix} \begin{bmatrix} -2 \\ -0.8 \end{bmatrix} = (-1.78)(-2) + (-0.14)(-0.8) = 3.56 + 0.112 = 3.672 \)<br><br>Thus, \(D_M^2(\mathbf{x}, \boldsymbol{\mu}_2) = 3.672\), so \(D_M(\mathbf{x}, \boldsymbol{\mu}_2) = \sqrt{3.672} \approx 1.916\).<br><br>**Step 4: Compare distances and assign label.**<br><br>\( D_M^2(\mathbf{x}, \boldsymbol{\mu}_1) = 2.95 \)<br><br>\( D_M^2(\mathbf{x}, \boldsymbol{\mu}_2) = 3.67 \)<br><br>Since \(2.95 < 3.67\), the Mahalanobis distance to \(C_1\) is **smaller** than to \(C_2\).<br><br>**Therefore, assign \(\mathbf{x} = [1, 2.2]^T\) to class \(C_1\).**
19ML0026	Basic	19ML-BayesClassifier	Does a linear decision boundary guarantee that we are achieving the **minimum possible error** (Bayes error)?	**Yes — if the true class-conditional distributions actually satisfy the linearity condition.**<br><br>If \(p(\mathbf{x}|C_1)\) and \(p(\mathbf{x}|C_2)\) are both Gaussian with **equal covariance matrices**, then:<br><br>1. The Bayes optimal decision boundary is **linear**<br>2. Any classifier that implements this linear boundary (with the correct parameters) achieves the **Bayes error rate**<br>3. No other classifier, linear or nonlinear, can achieve lower error<br><br>**Important nuance:** If you **assume** equal covariance but the true distributions have unequal covariances, then:<br><br>- The optimal boundary is **quadratic**<br>- Forcing a linear boundary increases error above Bayes error<br>- Linear discriminant analysis (LDA) is suboptimal in this case<br><br>**Bottom line:** Linear boundary + correct parameters + true equal covariance = minimum achievable error.
19ML0027	Basic	19ML-BayesClassifier	Is a linear classifier always Bayes optimal if the decision boundary looks linear on training data?	**No — this is a dangerous pitfall.**<br><br>**Reasons:**<br><br>1. **Misspecification:** The true distributions may have unequal covariances, but sampling variability makes the empirical boundary appear linear<br>2. **Suboptimal threshold:** Even with correct covariances, using incorrect priors or loss ratios shifts the boundary away from the optimal location<br>3. **Non-Gaussian data:** Linear boundary can be optimal for some non-Gaussian distributions, but this must be verified — never assume<br><br>**Visual pitfall:** A scatter plot showing separable data with a linear gap does **not** prove the Bayes boundary is linear. The optimal boundary might curve between overlapping regions, but the overlap might not be visible in a finite sample.<br><br>**Always check:** Test whether allowing quadratic or nonlinear features significantly improves held-out accuracy. If yes, the equal covariance assumption likely fails.
19ML0028	Basic	19ML-BayesClassifier	What is the general sufficient condition for the Bayes decision boundary to be linear?	**Condition:** The log-likelihood ratio \(\ln\frac{p(\mathbf{x}|C_1)}{p(\mathbf{x}|C_2)}\) must be a **linear function** of \(\mathbf{x}\).<br><br>This occurs when both class-conditional densities belong to the **same exponential family** with identical dispersion/nuisance parameters:<br><br>\( p(\mathbf{x}|C_k) = h(\mathbf{x})g(\theta_k)\exp\left(\eta(\theta_k)^T \mathbf{x}\right) \)<br><br>where only the natural parameter \(\eta(\theta_k)\) differs between classes, while \(h(\mathbf{x})\) and the base measure are identical.<br><br>**Examples:**<br><br>- Gaussian with equal covariance ✓<br>- Bernoulli with equal variance ✓<br>- Poisson with equal exposure ✓<br>- Multinomial with equal total counts ✓<br><br>**Counter-examples (nonlinear Bayes boundaries):**<br><br>- Gaussian with unequal covariance matrices (quadratic)<br>- Mixture distributions<br>- Uniform distributions with different supports<br>- Most non-parametric densities
19ML0029	Basic	19ML-BayesClassifier	What is a Loss (or Risk) Matrix L, and how is it indexed for a K-class problem?	A K × K matrix L where element L_{kj} quantifies the penalty for predicting class C_j when the true class is C_k.<br>**Indices:** The first index k is the **true class**, the second index j is the **predicted class/action**.<br><br>\( \mathbf{L} = \begin{pmatrix} L_{11} & L_{12} & \cdots & L_{1K} \\ L_{21} & L_{22} & \cdots & L_{2K} \\ \vdots & \vdots & \ddots & \vdots \\ L_{K1} & L_{K2} & \cdots & L_{KK} \end{pmatrix} \)<br><br>It generalizes the 0-1 loss to handle asymmetric costs (e.g., false negatives cost more than false positives in disease screening).
19ML0030	Basic	19ML-BayesClassifier	How do you interpret the diagonal and off-diagonal entries of L? What is the 0-1 loss matrix?	* **Diagonal (L_{kk}):** Loss for a correct prediction. Often 0 (no penalty).<br>* **Off-diagonal (L_{kj}, k ≠ j):** Loss for a specific error (predicting j when truth is k).<br>* **0-1 Loss Matrix:** L_{kj} = 0 if k = j, and 1 if k ≠ j. All errors are equally costly.<br><br>\( \mathbf{L}_{0-1} = \begin{pmatrix} 0 & 1 & \cdots & 1 \\ 1 & 0 & \cdots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & \cdots & 0 \end{pmatrix} \)
19ML0031	Basic	19ML-BayesClassifier	Given the posterior vector p(x) and loss matrix L, how is the conditional risk vector R(x) computed?	The conditional risk R(C_j | x) for taking action C_j is the expected loss over the true class posteriors. For all actions, it's a matrix-vector product:<br><br>\( \mathbf{R}(\mathbf{x}) = \mathbf{L}^T \mathbf{p}(\mathbf{x}) \)<br><br>where:<br><br>\( \mathbf{p}(\mathbf{x}) = \begin{bmatrix} P(C_1|\mathbf{x}) \\ P(C_2|\mathbf{x}) \\ \vdots \\ P(C_K|\mathbf{x}) \end{bmatrix}, \quad \mathbf{R}(\mathbf{x}) = \begin{bmatrix} R(C_1|\mathbf{x}) \\ R(C_2|\mathbf{x}) \\ \vdots \\ R(C_K|\mathbf{x}) \end{bmatrix} \)<br><br>Thus, \( R(C_j|x) = ∑_{k=1}^{K} L_{kj} P(C_k|x) \) is the dot product of the j-th column of L with p(x).
19ML0032	Basic	19ML-BayesClassifier	Given the posterior vector p(x) and loss matrix L, how is the conditional risk for action C_j computed using vector notation?	The conditional risk \( R(C_j | \mathbf{x}) \) is the dot product between the **j-th column of the loss matrix L** and the posterior probability vector p(x).<br><br>\( R(C_j | \mathbf{x}) = \sum_{k=1}^{K} L_{kj} P(C_k | \mathbf{x}) \)<br><br>In vector form, if we let \( \mathbf{l}_j \) represent the j-th column of L, then:<br><br>\( R(C_j | \mathbf{x}) = \mathbf{l}_j^T \mathbf{p}(\mathbf{x}) \)<br><br>where \( \mathbf{l}_j = [L_{1j}, L_{2j}, ..., L_{Kj}]^T \) and \( \mathbf{p}(\mathbf{x}) = [P(C_1|\mathbf{x}), P(C_2|\mathbf{x}), ..., P(C_K|\mathbf{x})]^T \).
19ML0033	Basic	19ML-BayesClassifier	What is the physical interpretation of the operation \( \mathbf{l}_j^T \mathbf{p}(\mathbf{x}) \) for computing conditional risk?	It is the **expected value of the loss** incurred when taking action j, where the expectation is taken over the uncertainty in the true class. The vector \( \mathbf{l}_j \) contains all possible losses for action j (one for each possible true class), and \( \mathbf{p}(\mathbf{x}) \) contains the probabilities of those true classes given the observation x. Their dot product gives the weighted average loss.
19ML0034	Basic	19ML-BayesClassifier	What is the optimal Bayes decision rule using the conditional risk vector R(x)?	Choose the action (class) corresponding to the **minimum element** in the conditional risk vector:<br><br>\( \text{Predict } C_{j^*} \text{ where } j^* = \arg\min_{j \in \{1,\dots,K\}} R(C_j | \mathbf{x}) \)<br><br>In vector terms: j^* = argmin R(x). This minimizes the total expected loss (Bayes Risk).
19ML0035	Basic	19ML-BayesClassifier	For a binary classification problem with classes \(C_1\) and \(C_2\), what are the conditional risks \(R(C_1|\mathbf{x})\) and \(R(C_2|\mathbf{x})\) under an arbitrary loss matrix \(\mathbf{L}\)?	The loss matrix for binary classification is:<br><br>\( \mathbf{L} = \begin{pmatrix} L_{11} & L_{12} \\ L_{21} & L_{22} \end{pmatrix} \)<br><br>where \(L_{kj}\) is the loss for predicting class \(C_j\) when true class is \(C_k\).<br><br>The conditional risks are:<br><br>\( R(C_1|\mathbf{x}) = L_{11}P(C_1|\mathbf{x}) + L_{21}P(C_2|\mathbf{x}) \)<br><br>\( R(C_2|\mathbf{x}) = L_{12}P(C_1|\mathbf{x}) + L_{22}P(C_2|\mathbf{x}) \)
19ML0036	Basic	19ML-BayesClassifier	Derive the decision rule for binary classification under a general loss matrix in terms of the likelihood ratio \(\frac{p(\mathbf{x}|C_1)}{p(\mathbf{x}|C_2)}\).	**Step 1:** Choose \(C_1\) if \(R(C_1|\mathbf{x}) < R(C_2|\mathbf{x})\):<br><br>\( L_{11}P(C_1|\mathbf{x}) + L_{21}P(C_2|\mathbf{x}) < L_{12}P(C_1|\mathbf{x}) + L_{22}P(C_2|\mathbf{x}) \)<br><br>**Step 2:** Rearrange terms:<br><br>\( (L_{11} - L_{12})P(C_1|\mathbf{x}) < (L_{22} - L_{21})P(C_2|\mathbf{x}) \)<br><br>**Step 3:** Substitute Bayes' theorem \(P(C_k|\mathbf{x}) = \frac{p(\mathbf{x}|C_k)P(C_k)}{p(\mathbf{x})}\):<br><br>\( (L_{11} - L_{12})\frac{p(\mathbf{x}|C_1)P(C_1)}{p(\mathbf{x})} < (L_{22} - L_{21})\frac{p(\mathbf{x}|C_2)P(C_2)}{p(\mathbf{x})} \)<br><br>**Step 4:** Cancel \(p(\mathbf{x}) > 0\) and rearrange to obtain the likelihood ratio threshold:<br><br>\( \frac{p(\mathbf{x}|C_1)}{p(\mathbf{x}|C_2)} > \frac{(L_{22} - L_{21})P(C_2)}{(L_{11} - L_{12})P(C_1)} \)
19ML0037	Basic	19ML-BayesClassifier	What is the most common algebraic mistake when deriving the binary Bayes decision rule with a loss matrix?	**Forgetting to flip the inequality when dividing by negative numbers.**<br><br>\( (L_{11} - L_{12}) < 0 \quad \text{and} \quad (L_{22} - L_{21}) < 0 \)<br><br>**Wrong:** "Divide both sides by \((L_{11} - L_{12})\) and \((L_{22} - L_{21})\)"<br><br>This preserves the original inequality direction and yields an incorrect threshold.<br><br>**Correct:** Multiply by -1 first to make coefficients positive, *then* divide.<br><br>**Quick sanity check:**<br><br>- If your derived threshold has \((L_{22} - L_{21})\) in numerator and \((L_{11} - L_{12})\) in denominator, the inequality **must** be `>`<br>- If you see `<`, your sign is flipped
19ML0038	Basic	19ML-BayesClassifier	What is the complete Bayes optimal decision rule for binary classification under an arbitrary loss matrix?	**Predict class \(C_1\) if:**<br><br>\( \frac{p(\mathbf{x}|C_1)}{p(\mathbf{x}|C_2)} > \frac{(L_{22} - L_{21})P(C_2)}{(L_{11} - L_{12})P(C_1)} \)<br><br>**Predict class \(C_2\) if:**<br><br>\( \frac{p(\mathbf{x}|C_1)}{p(\mathbf{x}|C_2)} < \frac{(L_{22} - L_{21})P(C_2)}{(L_{11} - L_{12})P(C_1)} \)<br><br>**If equality holds,** either decision yields the same expected loss.<br><br>**Assumptions:** This derivation assumes \(L_{11} < L_{12}\) and \(L_{22} < L_{21}\) (correct decisions have lower loss than errors), so \((L_{11} - L_{12}) < 0\) and \((L_{22} - L_{21}) < 0\), making both sides positive.
19ML0039	Basic	19ML-BayesClassifier	Show that the general binary decision rule reduces to the standard Bayes classifier under 0-1 loss.	For 0-1 loss matrix:<br><br>\( \mathbf{L}_{0-1} = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \)<br><br>where \(L_{11} = L_{22} = 0\) and \(L_{12} = L_{21} = 1\).<br><br>Substitute into the threshold:<br><br>\( \frac{(L_{22} - L_{21})P(C_2)}{(L_{11} - L_{12})P(C_1)} = \frac{(0 - 1)P(C_2)}{(0 - 1)P(C_1)} = \frac{(-1)P(C_2)}{(-1)P(C_1)} = \frac{P(C_2)}{P(C_1)} \)<br><br>Thus the decision rule becomes:<br><br>\( \text{Predict } C_1 \text{ if } \frac{p(\mathbf{x}|C_1)}{p(\mathbf{x}|C_2)} > \frac{P(C_2)}{P(C_1)} \)<br><br>Multiplying both sides by \(P(C_1)p(\mathbf{x}|C_2)\) yields \(p(\mathbf{x}|C_1)P(C_1) > p(\mathbf{x}|C_2)P(C_2)\), which is equivalent to \(P(C_1|\mathbf{x}) > P(C_2|\mathbf{x})\) — the standard Bayes classifier for minimum error.
19ML0040	Basic	19ML-BayesClassifier	What critical assumptions about loss values ensure the threshold \(\frac{(L_{22} - L_{21})P(C_2)}{(L_{11} - L_{12})P(C_1)}\) is positive and the inequality direction is correct?	**Assumption 1:** Correct decisions cost less than errors.<br><br>\( L_{11} < L_{12} \quad \text{and} \quad L_{22} < L_{21} \)<br><br>This ensures \((L_{11} - L_{12}) < 0\) and \((L_{22} - L_{21}) < 0\), so their ratio is **positive**.<br><br>**Assumption 2:** Loss differences are nonzero.<br>If \(L_{11} = L_{12}\) or \(L_{22} = L_{21}\), there is no penalty for misclassifying that class, and the decision rule degenerates (always predict one class).<br><br>**Pitfall:** If you accidentally reverse the indices when defining \(L_{kj}\), the inequality direction flips. Always verify: \(L_{kj}\) = loss when **truth = k, prediction = j**.
19ML0041	Basic	19ML-BayesClassifier	How can we interpret the threshold \(\frac{(L_{22} - L_{21})P(C_2)}{(L_{11} - L_{12})P(C_1)}\) intuitively?	The threshold acts as **cost-weighted effective priors**:<br><br>\( \text{Threshold} = \frac{P(C_2)}{P(C_1)} \times \frac{(L_{22} - L_{21})}{(L_{11} - L_{12})} \)<br><br>- \(\frac{P(C_2)}{P(C_1)}\) is the prior odds for \(C_2\) vs. \(C_1\)<br>- \(\frac{(L_{22} - L_{21})}{(L_{11} - L_{12})}\) is the **loss ratio** — the relative cost of misclassifying \(C_2\) vs. misclassifying \(C_1\)<br><br>**Intuition:**<br><br>- If misclassifying \(C_2\) is expensive (\(L_{22} - L_{21}\) is large negative magnitude), the threshold **decreases**, making it easier to classify as \(C_2\) (conservative toward the expensive-to-miss class)<br>- If misclassifying \(C_1\) is expensive (\(L_{11} - L_{12}\) is large negative magnitude), the threshold **increases**, making it easier to classify as \(C_1\)<br><br>This generalizes the prior odds ratio by incorporating asymmetric misclassification costs.
19ML0042	Basic	19ML-BayesClassifier	What does the 0-1 loss matrix look like, and what does the resulting rule become?	\(L_{kj} = 0\) if \(k = j\), and \(1\) if \(k \neq j\).<br><br>\( \mathbf{L}_{0-1} = \begin{pmatrix} 0 & 1 & \cdots & 1 \\ 1 & 0 & \cdots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & \cdots & 0 \end{pmatrix} \)<br><br>The conditional risk becomes:<br><br>\( R(C_j | \mathbf{x}) = \sum_{k \neq j} P(C_k | \mathbf{x}) = 1 - P(C_j | \mathbf{x}) \)<br><br>Minimizing this is equivalent to maximizing the posterior \(P(C_j | \mathbf{x})\), recovering the standard Bayes classifier for minimum error.
19ML0043	Basic	19ML-BayesClassifier	Clarify the statement: "Total error for the Bayes classifier for binary class is \(\min\{P(C_1), P(C_2)\} \times (1 - \min\{P(C_1), P(C_2)\})\)".	This is incorrect and a major pitfall. This expression actually calculates something else entirely - it's the variance of a Bernoulli random variable with probability \(\min\{P(C_1), P(C_2)\}\).<br><br>The correct Bayes error is:<br><br>1. **Pointwise**: \(P(\text{error} | \mathbf{x}) = \min[P(C_1|\mathbf{x}), P(C_2|\mathbf{x})]\)<br>2. **Overall**: \(P_{\text{error}} = \mathbb{E}_{\mathbf{x}}[\min[P(C_1|\mathbf{x}), P(C_2|\mathbf{x})]]\)<br><br>The error depends on the overlap of distributions \(p(\mathbf{x}|C_k)\), not just priors.
19ML0044	Basic	19ML-BayesClassifier	What is the Bayes Error Rate (or Bayes Risk for 0-1 loss)?	The minimum possible error rate achievable by any classifier for a given classification problem. It is the expected error of the optimal (Bayes) classifier, which makes decisions by selecting the class with maximum posterior probability \(P(C_k | \mathbf{x})\).<br><br>\( P_{\text{error}} = \mathbb{E}_{\mathbf{x}}[\min\{P(\text{error} | \mathbf{x})\}] \)<br><br>where \(P(\text{error} | \mathbf{x}) = 1 - \max_k P(C_k | \mathbf{x})\) is the minimum conditional error at point \(\mathbf{x}\).
19ML0045	Basic	19ML-BayesClassifier	Does mathematical expectation \(\mathbb{E}_{\mathbf{x}}[f(\mathbf{x})]\) mean multiplying the function \(f(\mathbf{x})\) by something to get the Bayes error?	No. Expectation is a **weighted average** over all possible values of \(\mathbf{x}\), not a simple multiplication.<br><br>**Incorrect interpretation:**<br><br>\( P_{\text{error}} \neq \min[P(C_1|\mathbf{x}), P(C_2|\mathbf{x})] \times \text{(some constant)} \)<br><br>**Correct interpretation:**<br><br>\( P_{\text{error}} = \mathbb{E}_{\mathbf{x}}[\min[P(C_1|\mathbf{x}), P(C_2|\mathbf{x})]] = \int \min[P(C_1|\mathbf{x}), P(C_2|\mathbf{x})] \, p(\mathbf{x}) \, d\mathbf{x} \)<br><br>The expectation:<br><br>- **Integrates** (continuous) or **sums** (discrete) over the entire feature space<br>- **Weights** each point's conditional error by the probability density \(p(\mathbf{x})\) of observing that \(\mathbf{x}\)<br>- Produces a **single scalar** summarizing the average error across the whole input distribution<br><br>You cannot obtain the total Bayes error by simply multiplying a pointwise error—you must integrate over all \(\mathbf{x}\).
19ML0046	Basic	19ML-BayesClassifier	What is the integral formula for the Bayes error rate?	The Bayes error is the expectation of the conditional error over the entire feature space:<br><br>\( P_{\text{error}} = \int_{\mathcal{X}} \left[1 - \max_k P(C_k | \mathbf{x})\right] p(\mathbf{x}) d\mathbf{x} \)<br><br>Alternatively, it can be expressed as:<br><br>\( P_{\text{error}} = 1 - \int_{\mathcal{X}} \max_k [p(\mathbf{x}|C_k)P(C_k)] d\mathbf{x} \)<br><br>since \(p(\mathbf{x}) = \sum_j p(\mathbf{x}|C_j)P(C_j)\).
19ML0047	Basic	19ML-BayesClassifier	For a two-class problem, what is the explicit formula for Bayes error?	For classes \(C_1\) and \(C_2\):<br><br>\( P_{\text{error}} = \int_{\mathcal{X}} \min\left[P(C_1|\mathbf{x}), P(C_2|\mathbf{x})\right] p(\mathbf{x}) d\mathbf{x} \)<br><br>Equivalently, using the decision regions \(\mathcal{R}_1\) and \(\mathcal{R}_2\):<br><br>\( P_{\text{error}} = P(C_1) \int_{\mathcal{R}_2} p(\mathbf{x}|C_1) d\mathbf{x} + P(C_2) \int_{\mathcal{R}_1} p(\mathbf{x}|C_2) d\mathbf{x} \)<br><br>where \(\mathcal{R}_1\) is where \(P(C_1|\mathbf{x}) > P(C_2|\mathbf{x})\), and \(\mathcal{R}_2\) is the complement.
19ML0048	Basic	19ML-BayesClassifier	Compute the Bayes error for a binary classification problem with:<br><br>- Class \(C_1\): \(p(x|C_1) = \mathcal{N}(-1, 1)\)<br>- Class \(C_2\): \(p(x|C_2) = \mathcal{N}(1, 1)\)<br>- Equal priors: \(P(C_1) = P(C_2) = 0.5\)	**Step 1: Find decision boundary.** With equal priors, the boundary is where \(p(x|C_1) = p(x|C_2)\).<br><br>\( \frac{1}{\sqrt{2\pi}} e^{-\frac{(x+1)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x-1)^2}{2}} \)<br><br>Simplifying: \((x+1)^2 = (x-1)^2 \Rightarrow x = 0\)<br><br>**Step 2: Apply Bayes error integral formula.**<br><br>\( P_{\text{error}} = \int_{-\infty}^{\infty} \min[P(C_1|x), P(C_2|x)] \, p(x) \, dx \)<br><br>With equal priors, \(P(C_1|x) > P(C_2|x)\) when \(x < 0\), so:<br><br>- For \(x < 0\): error = \(P(C_2|x)\)<br>- For \(x > 0\): error = \(P(C_1|x)\)<br><br>**Step 3: Compute using decision regions formulation.**<br><br>\( P_{\text{error}} = P(C_1) \int_{0}^{\infty} p(x|C_1) dx + P(C_2) \int_{-\infty}^{0} p(x|C_2) dx \)<br><br>\( P_{\text{error}} = 0.5 \int_{0}^{\infty} \mathcal{N}(-1,1) dx + 0.5 \int_{-\infty}^{0} \mathcal{N}(1,1) dx \)<br><br>**Step 4: Evaluate.**<br>\(\int_{0}^{\infty} \mathcal{N}(-1,1) dx = P(Z > 1)\) where \(Z \sim \mathcal{N}(0,1) = 0.1587\)<br>\(\int_{-\infty}^{0} \mathcal{N}(1,1) dx = P(Z < -1) = 0.1587\)<br><br>\( P_{\text{error}} = 0.5(0.1587) + 0.5(0.1587) = 0.1587 \)<br><br>**Result:** Bayes error = **15.87%**
19ML0049	Basic	19ML-BayesClassifier	Compute the Bayes error when classes are perfectly separable with no overlap:<br><br>- Class \(C_1\): \(p(x|C_1) = \mathcal{N}(-2, 0.1)\) (very narrow)<br>- Class \(C_2\): \(p(x|C_2) = \mathcal{N}(2, 0.1)\) (very narrow)<br>- Equal priors: \(P(C_1) = P(C_2) = 0.5\)	**Step 1: Decision boundary.** With nearly no overlap, the boundary is approximately at \(x = 0\).<br><br>**Step 2: Apply Bayes error integral.**<br><br>\( P_{\text{error}} = P(C_1) \int_{\mathcal{R}_2} p(x|C_1) dx + P(C_2) \int_{\mathcal{R}_1} p(x|C_2) dx \)<br><br>**Step 3: Evaluate the integrals.**<br>\(\int_{0}^{\infty} \mathcal{N}(-2,0.1) dx \approx 0\) (all \(C_1\) mass is far left of 0)<br>\(\int_{-\infty}^{0} \mathcal{N}(2,0.1) dx \approx 0\) (all \(C_2\) mass is far right of 0)<br><br>**Step 4: Compute.**<br><br>\( P_{\text{error}} = 0.5(0) + 0.5(0) = 0 \)<br><br>**Result:** Bayes error = **0%** (perfect classification possible)
19ML0050	Basic	19ML-BayesClassifier	Compute the Bayes error when classes are identical (complete overlap):<br><br>- Class \(C_1\): \(p(x|C_1) = \mathcal{N}(0, 1)\)<br>- Class \(C_2\): \(p(x|C_2) = \mathcal{N}(0, 1)\)<br>- Equal priors: \(P(C_1) = P(C_2) = 0.5\)	**Step 1: Posteriors equal priors everywhere.**<br>Since \(p(x|C_1) = p(x|C_2)\), we have:<br><br>\( P(C_1|x) = P(C_2|x) = 0.5 \quad \forall x \)<br><br>**Step 2: Bayes decision rule.** Any decision yields error 0.5 at every point.<br><br>**Step 3: Apply Bayes error integral.**<br><br>\( P_{\text{error}} = \int_{-\infty}^{\infty} \min[0.5, 0.5] \, p(x) \, dx = \int_{-\infty}^{\infty} 0.5 \, p(x) \, dx = 0.5 \times 1 = 0.5 \)<br><br>**Result:** Bayes error = **50%** (random guessing performance)
19ML0051	Basic	19ML-BayesClassifier	Compute Bayes error for:<br><br>- Class \(C_1\): \(p(x|C_1) = \mathcal{N}(-1, 1)\)<br>- Class \(C_2\): \(p(x|C_2) = \mathcal{N}(1, 1)\)<br>- Unequal priors: \(P(C_1) = 0.9\), \(P(C_2) = 0.1\)	**Step 1: Find decision boundary.** Boundary satisfies \(p(x|C_1)P(C_1) = p(x|C_2)P(C_2)\).<br><br>\( \frac{0.9}{\sqrt{2\pi}} e^{-\frac{(x+1)^2}{2}} = \frac{0.1}{\sqrt{2\pi}} e^{-\frac{(x-1)^2}{2}} \)<br><br>Taking logs and simplifying:<br><br>\( -\frac{(x+1)^2}{2} + \ln(0.9) = -\frac{(x-1)^2}{2} + \ln(0.1) \)<br><br>\( -\frac{(x+1)^2}{2} + \frac{(x-1)^2}{2} = \ln(0.1) - \ln(0.9) = \ln(1/9) \approx -2.197 \)<br><br>Solving: \(-2x = -2.197 \Rightarrow x \approx 1.0985\)<br><br>**Step 2: Apply Bayes error integral (decision regions formulation).**<br><br>\( P_{\text{error}} = P(C_1) \int_{1.0985}^{\infty} p(x|C_1) dx + P(C_2) \int_{-\infty}^{1.0985} p(x|C_2) dx \)<br><br>**Step 3: Evaluate.**<br>\(\int_{1.0985}^{\infty} \mathcal{N}(-1,1) dx = P(Z > 2.0985) \approx 0.018\) where \(Z \sim \mathcal{N}(0,1)\)<br>\(\int_{-\infty}^{1.0985} \mathcal{N}(1,1) dx = P(Z < 0.0985) \approx 0.539\)<br><br>**Step 4: Compute.**<br><br>\( P_{\text{error}} = 0.9(0.018) + 0.1(0.539) = 0.0162 + 0.0539 = 0.0701 \)<br><br>**Result:** Bayes error = **7.01%** (lower than equal priors case due to strong prior)
19ML0052	Basic	19ML-BayesClassifier	For an M-class classification problem with equal priors, what is the absolute maximum possible Bayes error rate \(P_e\)?	\( 0 \leq P_e \leq 1 - \frac{1}{M} \)<br><br>**Upper bound:** \(P_e \leq 1 - \frac{1}{M}\)<br><br>**Lower bound:** \(P_e \geq 0\)<br><br>**Intuition:** Even in the worst case—complete overlap of all class-conditional distributions—the Bayes classifier can always achieve accuracy at least \(1/M\) by random guessing. Therefore, the error cannot exceed \(1 - 1/M\).
19ML0053	Basic	19ML-BayesClassifier	What factors determine the Bayes error rate of a classification problem?	1. **Class Overlap:** The overlap between class-conditional distributions \(p(\mathbf{x}|C_k)\)<br>2. **Class Priors:** \(P(C_k)\)<br>3. **Dimensionality & Separability:** How well separated the classes are in feature space<br><br>The Bayes error is fundamentally determined by the inherent ambiguity in the data - how much the classes overlap in the feature space.
19ML0054	Basic	19ML-BayesClassifier	What is the difference between Bayes error and the error of a trained classifier?	- **Bayes Error:** Theoretical lower bound on error rate, determined by data distribution. Unachievable if estimated distributions are imperfect.<br>- **Empirical Error:** Error rate of a specific classifier on test data. Always \(\geq\) Bayes error (asymptotically).<br>- **Excess Risk:** Difference between classifier's error and Bayes error, due to model mismatch, finite data, or suboptimal training.
19ML0055	Basic	19ML-BayesClassifier	How can we estimate the Bayes error rate in practical problems?	1. **Monte Carlo Simulation:** Generate data from known distributions and compute error of Bayes classifier<br>2. **Plug-in Estimator:** Estimate \(p(\mathbf{x}|C_k)\) and \(P(C_k)\) from data, then compute integral numerically<br>3. **Lower Bounds:** Use information-theoretic bounds (Fano, Hellinger)<br>4. **1-NN Error Bound:** For large samples, error of 1-NN classifier \(\leq 2 \times \text{Bayes error}\)
19ML0056	Basic	19ML-BayesClassifier	What is the theoretical bound relating the error rate of the 1-Nearest Neighbor (1-NN) classifier to the Bayes error rate?	For large training samples (as the number of training examples \(N \to \infty\)), the asymptotic error rate \(P_{\text{1-NN}}\) of the 1-NN classifier is bounded by:<br><br>\( P_{\text{Bayes}} \leq P_{\text{1-NN}} \leq 2 P_{\text{Bayes}} (1 - P_{\text{Bayes}}) \leq 2 P_{\text{Bayes}} \)<br><br>More simply: **The 1-NN error rate is at most twice the Bayes error rate.**<br><br>\( P_{\text{1-NN}} \leq 2 \times P_{\text{Bayes}} \)<br><br>This shows that even this simple, non-parametric classifier achieves an error rate within a factor of 2 of the optimal Bayes classifier in the large-sample limit.
19ML0057	Basic	19ML-BayesClassifier	Does the bound \(P_{\text{1-NN}} \leq 2P_{\text{Bayes}}\) mean that 1-NN is always at most twice as bad as Bayes?	No. This bound is **asymptotic** (requires \(N \to \infty\)) and holds **in expectation** over random training sets. Common misinterpretations:<br><br>1. **Finite sample:** With small training sets, 1-NN error can be much larger than \(2 \times\) Bayes error<br>2. **Deterministic bound:** It's not a guaranteed upper bound for any specific training set—it's an asymptotic expected bound<br>3. **Curse of dimensionality:** In high dimensions, the "nearest neighbor" may not be close at all, violating the asymptotic assumption<br><br>The bound is theoretically important but practically optimistic for high-dimensional problems.
19ML0058	Basic	19ML-BayesClassifier	Why is \(P_{\text{error}} \neq \min\{P(C_1), P(C_2)\}\) in general?	This formula would only be correct if the classes were **completely separated** (no overlap) and we always guessed the minority class incorrectly. In reality, Bayes error depends on distribution overlap. For example, with equal priors \(P(C_1)=P(C_2)=0.5\), Bayes error can range from 0% (perfect separation) to 50% (complete overlap), not fixed at 25% as \(\min(0.5,0.5)\times(1-\min(0.5,0.5))=0.25\) would suggest.
19ML0059	Basic	19ML-BayesClassifier	For a binary classification problem, what is the difference between computing Bayes error using the integral formula versus simply counting misclassified test points?	This is the distinction between **theoretical Bayes error** and **empirical error rate**.<br><br>**Integral Formula (Theoretical Bayes Error):**<br><br>\( P_{\text{Bayes}} = \int \min[P(C_1|\mathbf{x}), P(C_2|\mathbf{x})] \, p(\mathbf{x}) \, d\mathbf{x} \)<br><br>- Requires knowledge of true distributions \(p(\mathbf{x}|C_k)\) and \(P(C_k)\)<br>- Is a **population quantity** — the true minimum achievable error<br>- Cannot be computed exactly from finite data<br><br>**Counting Misclassifications (Empirical Error):**<br><br>\( \hat{P}_{\text{error}} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(y_i \neq \hat{y}_i) \)<br><br>- Computed from finite test set of \(N\) examples<br>- Is an **estimate** of the classifier's error rate<br>- Depends on the specific classifier, not necessarily Bayes optimal<br><br>**Key Difference:** The integral gives the **true Bayes error** (theoretical lower bound). Counting gives an **empirical estimate** of error for a specific classifier on finite data.
19ML0060	Basic	19ML-BayesClassifier	Under what conditions does counting misclassified test points approximate the Bayes error integral?	Counting approximates the Bayes error **only if**:<br><br>1. **Bayes optimal classifier**: You are using the true Bayes decision rule (maximum posterior)<br>2. **Infinite test data**: As \(N_{\text{test}} \to \infty\), the empirical error converges to the true error of your classifier by the law of large numbers<br>3. **Correct model specification**: Your estimated posteriors match the true \(P(C_k|\mathbf{x})\)<br><br>Even then, the empirical error converges to:<br><br>\( \hat{P}_{\text{error}} \xrightarrow[N \to \infty]{} \int \mathbb{I}(\text{Bayes decision at \(\mathbf{x}\) is wrong}) \, p(\mathbf{x}) \, d\mathbf{x} \)<br><br>This is **exactly** the Bayes error integral — but only if your decision rule is truly optimal and your test set is infinite.
19ML0061	Basic	19ML-BayesClassifier	Why can't we just report test set accuracy and call it the Bayes error?	This is a common and serious pitfall. Reasons:<br><br>1. **Classifier suboptimality**: Your classifier may not be Bayes optimal, so its error is an **upper bound**, not the true Bayes error<br>2. **Finite sample variance**: Test set error is a random variable with variance \(\frac{P_{\text{error}}(1-P_{\text{error}})}{N}\)<br>3. **Overfitting**: Training on test data contaminates the estimate<br>4. **Distribution mismatch**: Test data may not truly represent \(p(\mathbf{x})\)<br><br>**Correct view:** Test error estimates **your classifier's performance**, not the fundamental Bayes error of the problem. The Bayes error is the **floor**; your classifier's error is somewhere **above** it (or equal in ideal cases).