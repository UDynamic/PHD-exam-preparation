{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084eb311",
   "metadata": {},
   "source": [
    "Okay, let's dive into Support Vector Machines (SVMs) in a simple and friendly way! ðŸ˜Š\n",
    "\n",
    "### What is a Support Vector Machine (SVM)?\n",
    "SVM is a **supervised machine learning algorithm** used for **classification** and **regression** tasks. But it's most famous for **classification**.\n",
    "\n",
    "Think of SVM as a **smart separator**. It tries to find the best line (or hyperplane in higher dimensions) that **clearly separates** different classes in your data.\n",
    "\n",
    "---\n",
    "\n",
    "### The Big Idea:\n",
    "Imagine you have two groups of points on a graph. You want to draw a line that separates them as clearly as possible. SVM finds the **widest possible margin** between the two groups.\n",
    "\n",
    "- The points that are **closest to the line** are called **support vectors**.\n",
    "- These are the **most important points** in defining the decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "### How SVM Works (Simplified):\n",
    "1. **Input Data**: You have labeled data (e.g., red dots and blue dots).\n",
    "2. **Find the Best Line**: SVM tries to find a line that **maximizes the margin** between the two classes.\n",
    "3. **Support Vectors**: The data points closest to the line are the **support vectors**.\n",
    "4. **Decision Boundary**: The line that separates the classes is the **decision boundary**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example in 2D:\n",
    "Letâ€™s say you have two classes: cats and dogs. Each has features like weight and height.\n",
    "\n",
    "SVM will try to find the **best line** that separates cats from dogs. The line that gives the **maximum distance** from the nearest cat and dog is the best one.\n",
    "\n",
    "---\n",
    "\n",
    "### Kernels in SVM:\n",
    "Sometimes, the data isn't linearly separable (you can't draw a straight line to separate the classes). Thatâ€™s where **kernels** come in!\n",
    "\n",
    "- **Kernel Trick**: It transforms the data into a higher dimension where it becomes linearly separable.\n",
    "- Common kernels:\n",
    "  - **Linear Kernel** (for simple data)\n",
    "  - **RBF (Radial Basis Function)** (for complex, non-linear data)\n",
    "  - **Polynomial Kernel**\n",
    "  -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e065842",
   "metadata": {},
   "source": [
    "Absolutely! Let's dive deeper into the **mathematical formulas** behind Support Vector Machines (SVM) and add **examples** to make it clearer. ðŸ˜Š\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® SVM Formulas Explained\n",
    "\n",
    "### 1. **Linear SVM (Hard Margin)**\n",
    "\n",
    "Letâ€™s assume we have a **linearly separable dataset**. The goal is to find a **hyperplane** that separates the two classes with the **maximum margin**.\n",
    "\n",
    "#### The Hyperplane Equation:\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$\n",
    "- $ w $: weight vector (normal to the hyperplane)\n",
    "- $ x $: input data point\n",
    "- $ b $: bias term\n",
    "\n",
    "#### The Margin:\n",
    "The **margin** is the distance between the hyperplane and the **closest data points** (support vectors). The margin is given by:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "#### Optimization Goal:\n",
    "We want to **maximize the margin**, which is equivalent to **minimizing $ \\|w\\| $**.\n",
    "\n",
    "So, the **optimization problem** is:\n",
    "$$\n",
    "\\text{Minimize: } \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "Subject to the constraints:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i\n",
    "$$\n",
    "Where:\n",
    "- $ y_i \\in \\{-1, 1\\} $: class labels\n",
    "- $ x_i $: data points\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Soft Margin SVM (for noisy data)**\n",
    "\n",
    "In real-world data, perfect separation is often impossible. So, we allow **some misclassifications** by introducing **slack variables** $ \\xi_i $.\n",
    "\n",
    "#### New Optimization Problem:\n",
    "$$\n",
    "\\text{Minimize: } \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "Subject to:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "- $ C $: regularization parameter (controls the trade-off between maximizing margin and minimizing errors)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Kernel Trick (for non-linear data)**\n",
    "\n",
    "When data is **not linearly separable**, we use **kernel functions** to map the data into a **higher-dimensional space** where it becomes linearly separable.\n",
    "\n",
    "#### Kernel Function:\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "$$\n",
    "Where $ \\phi $ is a mapping function to a higher-dimensional space.\n",
    "\n",
    "#### Common Kernels:\n",
    "- **Linear Kernel**: $ K(x_i, x_j) = x_i \\cdot x_j $\n",
    "- **RBF (Radial Basis Function)**:\n",
    "  $$\n",
    "  K(x_i, x_j) = \\exp\\left(-\\gamma \\|x_i - x_j\\|^2\\right)\n",
    "  $$\n",
    "- **Polynomial Kernel**:\n",
    "  $$\n",
    "  K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Example: Linear SVM with Math\n",
    "\n",
    "Letâ€™s take a simple 2D example:\n",
    "\n",
    "### Data:\n",
    "- Class A (label = +1): (1, 2), (2, 3)\n",
    "- Class B (label = -1): (3, 1), (4, 2)\n",
    "\n",
    "We want to find the best line that separates these two classes.\n",
    "\n",
    "### Step 1: Assume a hyperplane:\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = 0\n",
    "$$\n",
    "\n",
    "Letâ€™s say we find:\n",
    "- $ w = [1, 1] $\n",
    "- $ b = -4 $\n",
    "\n",
    "So the line is:\n",
    "$$\n",
    "x_1 + x_2 - 4 = 0\n",
    "$$\n",
    "\n",
    "### Step 2: Check the margin:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|} = \\frac{2}{\\sqrt{1^2 + 1^2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2}\n",
    "$$\n",
    "\n",
    "This is the **maximum margin** for this data.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Example: RBF Kernel in Python\n",
    "\n",
    "Letâ€™s use the **RBF kernel** to classify a non-linear dataset like the **moons dataset**.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate non-linear data\n",
    "X, y = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create SVM with RBF kernel\n",
    "model = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"SVM with RBF Kernel\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "Accuracy: 95.00%\n",
    "```\n",
    "\n",
    "Youâ€™ll see a **non-linear decision boundary** that separates the two moon-shaped clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ Summary of Key Formulas\n",
    "\n",
    "| Concept | Formula |\n",
    "|--------|---------|\n",
    "| Hyperplane | $ w \\cdot x + b = 0 $ |\n",
    "| Margin | $ \\frac{2}{\\|w\\|} $ |\n",
    "| Optimization (Hard Margin) | $ \\min \\frac{1}{2} \\|w\\|^2 $ with $ y_i (w \\cdot x_i + b) \\geq 1 $ |\n",
    "| Optimization (Soft Margin) | $ \\min \\frac{1}{2} \\|w\\|^2 + C \\sum \\xi_i $ |\n",
    "| RBF Kernel | $ K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $ |\n",
    "\n",
    "---\n",
    "\n",
    "If you want to see how **SVM works for regression** (called **SVR**), or how to **tune hyperparameters** like $ C $ and $ \\gamma $, just let me know! I'm here to help you learn step by step. ðŸ˜Šâœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
