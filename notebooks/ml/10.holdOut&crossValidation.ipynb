{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9e2b9f",
   "metadata": {},
   "source": [
    "# Holdout \n",
    "The holdout method splits a dataset into separate subsets to train and evaluate a machine learning model, ensuring unbiased performance estimation.\n",
    "\n",
    "## purpose \n",
    "unbiased estimate of its real-world performance (bias and variance)\n",
    "Fixing them requires model changes (regularization, complexity)\n",
    "\n",
    "## Simple Holdout (Train-Test Split)\n",
    "**Front:** What is simple holdout and its main limitation?\n",
    "**Back:** A single split (typically 70-80% train, 20-30% test). \n",
    "Limitation: High variance in performance estimates due to sensitivity to that one random split.\n",
    "\n",
    "## Train-Validation-Test Split\n",
    "**Front:** Why use a three-way split instead of two-way?\n",
    "**Back:** Three splits allow separate phases: Training (fit model), Validation (tune hyperparameters), and Testing (final unbiased evaluation). Prevents data leakage from tuning into final metrics.\n",
    "\n",
    "## Random Holdout\n",
    "**Front:** What defines random holdout and when is it appropriate?\n",
    "**Back:** Data points are randomly assigned to splits. Appropriate when data is IID (Independent and Identically Distributed) and balanced.\n",
    "\n",
    "## Stratified Holdout\n",
    "**Front:** What is stratified holdout and when is it crucial?\n",
    "**Back:** Splits preserve the original class/group proportions in each subset. Crucial for classification with imbalanced datasets to ensure all splits represent all classes.\n",
    "\n",
    "## What Holdout Reveals About a Model\n",
    "**Front:** How can you diagnose underfitting vs. overfitting using holdout?\n",
    "**Back:** High train AND test error → Underfitting (High Bias). Low train error but high test error → Overfitting (High Variance).\n",
    "\n",
    "## Key Trade-off of Holdout\n",
    "**Front:** What is the main statistical trade-off of using holdout?\n",
    "**Back:** Reduces BIAS in performance estimation (gives honest, pessimistic estimate) but increases VARIANCE (single split gives unstable estimate). For lower variance, use cross-validation.\n",
    "\n",
    "## When to Use Holdout\n",
    "**Front:** When is holdout preferable over cross-validation?\n",
    "**Back:** With very large datasets (>10k samples), for quick prototyping, or with limited computational resources.\n",
    "\n",
    "## Core Limitation\n",
    "**Front:** What is the fundamental limitation of any holdout method?\n",
    "**Back:** It wastes data. Part of the dataset is withheld from training the model, which can be problematic with small datasets.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b30e5",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
