#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

04ML0001	Basic	04ML-MLE&MAP	What is the fundamental goal of parameter estimation in statistical modeling?	To infer the unknown parameters \(\theta\) of a probability distribution, given observed data \(x\). We want to find the most plausible \(\theta\) that explains our data.
04ML0002	Basic	04ML-MLE&MAP	What is the likelihood function \(p(x | \theta )\)?	It is a function of the parameters \(\theta\). It measures the probability (or probability density) of observing the given data \(x\) under a specific setting of the parameters. It is *not* a probability distribution over \(\theta\).<br><br>\( p(x | \theta ) \)
04ML0003	Basic	04ML-MLE&MAP	What is the prior distribution \(p(\theta)\)?	It represents our belief about the parameters \(\theta\) *before* seeing any data. It incorporates domain knowledge or assumptions.<br><br>\( p(\theta) \)
04ML0004	Basic	04ML-MLE&MAP	State Bayes' Theorem as it applies to parameter estimation.	It updates our belief about parameters \(\theta\) after observing data \(x\). The prior \(p(\theta)\) is combined with the likelihood \(p(x | \theta)\) to form the posterior.<br><br>\( p(\theta | x) = \frac{p(x | \theta ) \cdot p(\theta)}{p(x)} \)
04ML0005	Basic	04ML-MLE&MAP	What is the posterior distribution \(p(\theta | x)\)?	It represents our updated belief about the parameters \(\theta\) *after* observing the data \(x\). It is the main output of Bayesian inference, combining prior knowledge and empirical evidence.<br><br>\( p(\theta | x) \)
04ML0006	Basic	04ML-MLE&MAP	What is the evidence or marginal likelihood \(p(x)\)?	The total probability of the data under all possible parameter settings, weighted by the prior. It serves as a normalizing constant for the posterior.<br><br>\( p(x) = \int p(x | \theta) p(\theta) d\theta \)
04ML0007	Basic	04ML-MLE&MAP	What is the Maximum A Posteriori (MAP) estimate \(\hat{\theta}_{MAP}\)?	It is the mode (peak) of the posterior distribution. It is the single most probable parameter value given the data and the prior.<br><br>\( \hat{\theta}_{MAP} = \arg \max_{\theta} p(\theta | x) \)
04ML0008	Basic	04ML-MLE&MAP	How is the MAP estimate computed in practice, ignoring constants?	Since the evidence \(p(x)\) is independent of \(\theta\), we maximize the unnormalized posterior: the likelihood times the prior.<br><br>\( \hat{\theta}_{MAP} = \arg \max_{\theta} p(\theta | x) = \arg \max_{\theta} [ p(x | \theta ) \cdot p(\theta) ] \)
04ML0009	Basic	04ML-MLE&MAP	What is the Maximum Likelihood Estimate (MLE), and how does it relate to MAP?	MLE is \(\hat{\theta}_{MLE} = \arg \max_{\theta} p(x | \theta)\). It is a special case of MAP where the prior \(p(\theta)\) is a uniform (flat) distribution. MAP introduces a regularization effect via the prior.
04ML0010	Basic	04ML-MLE&MAP	Why do we typically maximize the log-posterior instead of the posterior directly?	The logarithm is a monotonically increasing function, so the argmax is unchanged. It converts products into sums, which is numerically more stable, especially with many data points.<br><br>\( \hat{\theta}_{MAP} = \arg \max_{\theta} [ \log p(x | \theta) + \log p(\theta) ] \)
04ML0011	Basic	04ML-MLE&MAP	What is a key pitfall in interpreting a single MAP estimate versus the full posterior?	The MAP estimate is just one point. It discards all other information about the shape of the posterior (e.g., uncertainty, skewness, multimodality). The full posterior distribution provides a complete picture of parameter uncertainty.
04ML0012	Basic	04ML-MLE&MAP	When can the MAP estimate be problematic?	When the prior \(p(\theta)\) is chosen poorly (too informative or misspecified) or when the data is scarce, the MAP estimate can be overly influenced by the prior. It's crucial to perform sensitivity analysis.
04ML0013	Basic	04ML-MLE&MAP	how to remember bayes theorem's formulas?	Ps like \(\theta\):<br>Posterior: \(p(\theta|x)\)<br>Prior: \(p(\theta)\)<br>Likelihood: \(p(x|\theta)\)
04ML0014	Basic	04ML-MLE&MAP	What happens to MAP estimation as the dataset size \(n \to \infty\)?	MAP converges to MLE. With more data, the likelihood dominates the prior, making the prior's influence negligible, provided the prior gives non-zero probability to the true parameter.