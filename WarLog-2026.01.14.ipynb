{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dad8e3",
   "metadata": {},
   "source": [
    "## Sensitibvity analysis method for backpropagation\n",
    "\n",
    "The sensitivity analysis in backpropagation involves calculating the sensitivity of the output with respect to the weights in the network. This is done by computing the partial derivatives of the output with respect to each weight. The general formulation for sensitivity analysis is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ E $ is the error (or loss) function.\n",
    "- $ w_{ij} $ is the weight connecting the $ i $-th neuron in the previous layer to the $ j $-th neuron in the current layer.\n",
    "- $ a_j $ is the activation of the $ j $-th neuron.\n",
    "- $ z_j $ is the weighted sum of inputs to the $ j $-th neuron.\n",
    "\n",
    "The first term, $ \\frac{\\partial E}{\\partial a_j} $, is the derivative of the error with respect to the activation of the neuron. The second term, $ \\frac{\\partial a_j}{\\partial z_j} $, is the derivative of the activation function with respect to the weighted sum. The third term, $ \\frac{\\partial z_j}{\\partial w_{ij}} $, is simply the input to the neuron $ a_i $.\n",
    "\n",
    "This formulation allows us to compute the sensitivity of the error with respect to each weight in the network, which is essential for updating the weights during the training process. Let me know if you have any specific questions about this formulation or would like to move on to the SSE method! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81844e",
   "metadata": {},
   "source": [
    "I'm glad you asked for an explanation in English! Let's go through a **simple example of the Backpropagation algorithm** in a **3-layer neural network** (Input â†’ Hidden â†’ Output).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Network Structure:\n",
    "\n",
    "- **Input Layer:** 2 neurons  \n",
    "- **Hidden Layer:** 2 neurons  \n",
    "- **Output Layer:** 1 neuron  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 1: Forward Propagation\n",
    "\n",
    "Letâ€™s define the inputs and weights:\n",
    "\n",
    "- **Inputs:**  \n",
    "  $ x_1 = 0.05 $, $ x_2 = 0.10 $\n",
    "\n",
    "- **Weights (Hidden Layer):**  \n",
    "  $ w_1 = 0.15 $, $ w_2 = 0.20 $ (for hidden neuron 1)  \n",
    "  $ w_3 = 0.25 $, $ w_4 = 0.30 $ (for hidden neuron 2)\n",
    "\n",
    "- **Weights (Output Layer):**  \n",
    "  $ w_5 = 0.40 $, $ w_6 = 0.45 $\n",
    "\n",
    "- **Bias (optional):** For simplicity, we'll ignore bias in this example.\n",
    "\n",
    "- **Activation Function:** Sigmoid  \n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Hidden Layer Output:\n",
    "\n",
    "$$\n",
    "h_1 = \\sigma(w_1 x_1 + w_2 x_2) = \\sigma(0.15 \\cdot 0.05 + 0.20 \\cdot 0.10) = \\sigma(0.0375) \\approx 0.509\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\sigma(w_3 x_1 + w_4 x_2) = \\sigma(0.25 \\cdot 0.05 + 0.30 \\cdot 0.10) = \\sigma(0.0425) \\approx 0.511\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Output Layer:\n",
    "\n",
    "$$\n",
    "o = \\sigma(w_5 h_1 + w_6 h_2) = \\sigma(0.40 \\cdot 0.509 + 0.45 \\cdot 0.511) = \\sigma(0.43355) \\approx 0.606\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 2: Calculate the Error\n",
    "\n",
    "Letâ€™s say the **target output** is $ y = 0.01 $\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2}(y - o)^2 = \\frac{1}{2}(0.01 - 0.606)^2 \\approx 0.186\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 3: Backpropagation (Compute Gradients)\n",
    "\n",
    "We want to compute how much each weight contributed to the error.\n",
    "\n",
    "#### ðŸ”¸ Output Layer Gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial o} = -(y - o) = -(0.01 - 0.606) = 0.596\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial o}{\\partial z_o} = \\sigma'(z_o) = o(1 - o) = 0.606 \\cdot (1 - 0.606) \\approx 0.239\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial z_o} = \\frac{\\partial E}{\\partial o} \\cdot \\frac{\\partial o}{\\partial z_o} = 0.596 \\cdot 0.239 \\approx 0.142\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_5} = \\frac{\\partial E}{\\partial z_o} \\cdot h_1 = 0.142 \\cdot 0.509 \\approx 0.072\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_6} = \\frac{\\partial E}{\\partial z_o} \\cdot h_2 = 0.142 \\cdot 0.511 \\approx 0.0726\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¸ Hidden Layer Gradients:\n",
    "\n",
    "We now propagate the error back to the hidden layer.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial h_1} = \\frac{\\partial E}{\\partial z_o} \\cdot w_5 = 0.142 \\cdot 0.40 = 0.0568\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial z_h1} = \\sigma'(z_h1) = h_1(1 - h_1) = 0.509 \\cdot (1 - 0.509) \\approx 0.250\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial z_h1} = \\frac{\\partial E}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_h1} = 0.0568 \\cdot 0.250 \\approx 0.0142\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_1} = \\frac{\\partial E}{\\partial z_h1} \\cdot x_1 = 0.0142 \\cdot 0.05 = 0.00071\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_2} = \\frac{\\partial E}{\\partial z_h1} \\cdot x_2 = 0.0142 \\cdot 0.10 = 0.00142\n",
    "$$\n",
    "\n",
    "Repeat similar steps for $ w_3 $ and $ w_4 $.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 4: Update Weights\n",
    "\n",
    "Using **Gradient Descent**:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial E}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where $ \\eta $ is the learning rate (e.g., $ \\eta = 0.5 $).\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary\n",
    "\n",
    "- Forward pass computes outputs using weights and activation functions.\n",
    "- Backpropagation computes gradients of the error with respect to each weight.\n",
    "- Weights are updated using gradient descent to reduce the error.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a **Python code** example to implement this? I can write it for you! ðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
