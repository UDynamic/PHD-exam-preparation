{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e30e58",
   "metadata": {},
   "source": [
    "Hypothesis: \n",
    "\n",
    "$ h_{\\theta}(x) = \\theta_0 + \\theta_1x $\n",
    "\n",
    "$ h_{\\theta}(x) = \\hat{y} $\n",
    "\n",
    "---\n",
    "Cost function + GD for parameters:\n",
    "\n",
    "SSE :Sum squared error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{2} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2 \\Rightarrow \\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "MSE :Mean Sum squared error ($ \\frac{1}{2m} $ instead of $ \\frac{1}{2} $)\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2 \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "RMSE: root of mean squared error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\sqrt{MSE} = \\sqrt{\\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2} \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{2 \\sqrt{ \\frac{1}{2m} \\sum (h - y)^2 }} \\cdot \\frac{1}{m} \\sum (h - y) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{2 \\sqrt{ \\frac{1}{2m} \\sum (h - y)^2 }} \\cdot \\frac{1}{m} \\sum (h - y) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "MAE: mean average error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i}) \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\text{sign}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\text{sign}(h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "goal: $ \\min{J(\\theta_0, \\theta_1)} $\n",
    "\n",
    "---\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "1. starts with random variables for $\\theta_0, \\theta_1$\n",
    "2. Reapeat untill convergence:\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1) $$\n",
    "\n",
    "$ \\alpha $: learning rate\n",
    "learning rate too small: slow convergence\n",
    "learning rate too big: divergence or slow convergence\n",
    "\n",
    "convergence : $\\Delta{\\theta_1} \\leq \\epsilon$\n",
    "\n",
    "* at each update, both $\\theta_0, \\theta_1$ must be updated simultaniously.\n",
    "$$ \\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1) \\to \\Delta{\\theta_0} = - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "$$ \\theta_1 = \\theta_1 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1) \\to \\Delta{\\theta_1} = - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "---\n",
    "\n",
    "* in linear regression,with degree 2 polinomial(MSE or SSE) cost function is a convex function, so in condition of convergence, it's converged to the global optimum.\n",
    "\n",
    "---\n",
    "- epoch: one full pass over all the training data.\n",
    "- iteration: when you update the model parameters once (The number of iterations per epoch depends on the batch size).\n",
    "\n",
    "---\n",
    "Batch Gradient Descent (BGD):\n",
    "uses the entire training dataset to compute the gradient and update the parameters in each iteration.\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\n",
    "$$\n",
    "\n",
    "+ High accuracy\n",
    "+ Guaranteed convergence to the optimal solution (if the cost function is convex)\n",
    "+ Slow (because it processes the entire dataset in each iteration)\n",
    "- High memory usage\n",
    "- Can get stuck in local minima (if the cost function is not convex).\n",
    "\n",
    "Best for Small datasets where accuracy is critical.\n",
    "\n",
    "---\n",
    "\n",
    "Stochastic Gradient Descent (SGD):\n",
    "In Stochastic Gradient Descent, we use only one training example at a time to compute the gradient and update the parameters.\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\n",
    "$$\n",
    "\n",
    "+ Fast (only one sample is processed per iteration)\n",
    "+ Low memory usage\n",
    "+ Can escape local minima due to high variance\n",
    "- High variance (since it uses only one sample)\n",
    "- Noisy updates (may not converge to the exact minimum)\n",
    "- Need shuffling for each epoch\n",
    "\n",
    "Best for: Large datasets where speed is important.\n",
    "\n",
    "---\n",
    "\n",
    "Mini-Batch Gradient Descent\n",
    "is a compromise between BGD and SGD. We use a small batch of samples (e.g., 32, 64, 128) to compute the gradient and update the parameters.\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{b} \\sum_{i=1}^{b} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\n",
    "$$\n",
    "- $ b $: batch size\n",
    "\n",
    "+ Balances speed and accuracy\n",
    "+ Faster than BGD, more accurate than SGD\n",
    "+ Efficient for large datasets\n",
    "- Requires tuning the batch size\n",
    "\n",
    "Most practical applications, especially in **Deep Learning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdcaab",
   "metadata": {},
   "source": [
    "Normal Equation Method : \n",
    "\n",
    "$$ X \\theta = Y \\rightarrow \\theta = (X^T X)^{-1} X^T Y $$\n",
    "$(X^T X)$: making sure it's a square matrix\n",
    "for X: each row is a training example with features\n",
    "\n",
    "+ No need for iteration.\n",
    "+ No hyperparameter Tuning.\n",
    "+ Guaranteed convergence to the global minimum (if the problem is convex). GD could get stuck in local minima\n",
    "- Computationally expensive for large $ A > 10000 $ (number of Attributes). Unlike GD(+ GD can be adapted for faster convergence).\n",
    "- Doesn't work well if $ X^T X $ is non-invertible (due to (1)redundant features or (2)more features than examples). unlike GD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d19ad",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "**Front:** What is the core paradigm of supervised learning? <br/>\n",
    "**Back:** Learning a mapping from input variables $x$ to an output variable $y$, using a labeled dataset of example input-output pairs $\\{ (x^{(i)}, y^{(i)}) \\}_{i=1}^m$.\n",
    "\n",
    "## Hypothesis Function (Linear Model)\n",
    "**Front:** In linear regression, what is the form of the hypothesis function $h_\\theta(x)$ for a single feature? <br/>\n",
    "**Back:** $h_\\theta(x) = \\theta_0 + \\theta_1 x$, where $\\theta_0$ is the intercept/bias and $\\theta_1$ is the weight/slope. It predicts the output $\\hat{y}$.\n",
    "\n",
    "## Cost Function - Intuition\n",
    "**Front:** What is the purpose of a cost function $J(\\theta)$ in machine learning? <br/>\n",
    "**Back:** It quantifies the error between the model's predictions $h_\\theta(x)$ and the true targets $y$. The learning goal is to find parameters $\\theta$ that minimize $J(\\theta)$.\n",
    "\n",
    "## Mean Squared Error (MSE) Cost\n",
    "**Front:** What is the formula for the Mean Squared Error (MSE) cost function for linear regression? <br/>\n",
    "**Back:** $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$. The $\\frac{1}{2}$ is often included to cancel the factor of 2 from differentiation.\n",
    "\n",
    "## Gradient Descent - Core Idea\n",
    "**Front:** Describe the Gradient Descent algorithm in one sentence. <br/>\n",
    "**Back:** An iterative optimization algorithm that adjusts parameters $\\theta$ by taking steps proportional to the negative gradient of the cost function $J(\\theta)$.\n",
    "\n",
    "## Gradient Descent Update Rule\n",
    "**Front:** Write the general parameter update rule for Gradient Descent. <br/>\n",
    "**Back:** $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$, repeated simultaneously for all $j$. $\\alpha$ is the learning rate.\n",
    "\n",
    "## Learning Rate ($\\alpha$)\n",
    "**Front:** What are the consequences of setting the learning rate $\\alpha$ too small or too large? <br/>\n",
    "**Back:** Too small: very slow convergence. Too large: can overshoot the minimum, causing divergence or oscillatory, slow convergence.\n",
    "\n",
    "## Partial Derivative for MSE (Intercept)\n",
    "**Front:** What is the partial derivative $\\frac{\\partial J}{\\partial \\theta_0}$ for the MSE cost with hypothesis $h_\\theta(x)=\\theta_0+\\theta_1x$? <br/>\n",
    "**Back:** $\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})$.\n",
    "\n",
    "## Partial Derivative for MSE (Slope)\n",
    "**Front:** What is the partial derivative $\\frac{\\partial J}{\\partial \\theta_1}$ for the MSE cost with hypothesis $h_\\theta(x)=\\theta_0+\\theta_1x$? <br/>\n",
    "**Back:** $\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}$.\n",
    "\n",
    "## Convexity in Linear Regression\n",
    "**Front:** Why is using (M)SE for linear regression advantageous? <br/>\n",
    "**Back:** The cost function $J(\\theta)$ is convex. Gradient Descent on a convex function is guaranteed to converge to the global minimum, not a local one.\n",
    "\n",
    "## Batch Gradient Descent (BGD)\n",
    "**Front:** How does Batch Gradient Descent compute the gradient in each iteration? <br/>\n",
    "**Back:** It uses the **entire training set** to compute the gradient. The update for $\\theta_j$ sums over all $m$ examples: $\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$.\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "**Front:** How does Stochastic Gradient Descent compute the gradient for an update? <br/>\n",
    "**Back:** It uses **one randomly chosen training example** $(x^{(i)}, y^{(i)})$ per update: $\\theta_j := \\theta_j - \\alpha \\cdot (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$.\n",
    "\n",
    "## Mini-batch Gradient Descent\n",
    "**Front:** What is the compromise offered by Mini-batch Gradient Descent? <br/>\n",
    "**Back:** It uses a small random subset (mini-batch) of size $b$ (e.g., 32) to compute the gradient: $\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{b} \\sum_{i=1}^{b} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$.\n",
    "\n",
    "## Epoch vs. Iteration\n",
    "**Front:** Distinguish between an *epoch* and an *iteration* in Gradient Descent. <br/>\n",
    "**Back:** **Iteration:** One update of the model parameters. **Epoch:** One full pass through the entire training dataset (may contain many iterations, e.g., in mini-batch GD).\n",
    "\n",
    "## Normal Equation Method\n",
    "**Front:** What is the closed-form solution for linear regression parameters $\\theta$? <br/>\n",
    "**Back:** $\\theta = (X^T X)^{-1} X^T y$, where $X$ is the design matrix (with a column of 1s for the intercept) and $y$ is the target vector.\n",
    "\n",
    "## Cost Functions: SSE vs. MSE\n",
    "**Front:** What is the practical difference between Sum of Squared Errors (SSE) and Mean Squared Error (MSE)? <br/>\n",
    "**Back:** SSE: $J = \\frac{1}{2}\\sum (h-y)^2$. MSE: $J = \\frac{1}{2m}\\sum (h-y)^2$. MSE normalizes by dataset size $m$, making cost comparable across different sized datasets. Their gradients differ by a factor of $1/m$.\n",
    "\n",
    "## Cost Functions: MAE\n",
    "**Front:** What is the Mean Absolute Error (MAE) cost function and its key property? <br/>\n",
    "**Back:** $J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m |h_\\theta(x^{(i)}) - y^{(i)}|$. It is less sensitive to outliers than MSE, but its gradient involves the sign function, which is not differentiable at zero.\n",
    "\n",
    "## Gradient Descent Pitfall - Simultaneous Update\n",
    "**Front:** What is a critical implementation rule for the vanilla Gradient Descent update step? <br/>\n",
    "**Back:** All parameters $\\theta_j$ must be updated **simultaneously** using the *old* values of $\\theta$. Do not use a newly updated $\\theta_0$ to compute the update for $\\theta_1$.\n",
    "\n",
    "## BGD vs. SGD - Trade-offs\n",
    "**Front:** Compare the convergence behavior of BGD and SGD. <br/>\n",
    "**Back:** **BGD:** Converges smoothly to the minimum. **SGD:** Converges with noise; the path is erratic. It can escape shallow local minima but may oscillate near the global minimum.\n",
    "\n",
    "## Normal Equation vs. GD - When to Use\n",
    "**Front:** When might you prefer Gradient Descent over the Normal Equation? <br/>\n",
    "**Back:** When $n$ (number of features) is very large (>10,000), as inverting $X^TX$ ($O(n^3)$) is computationally expensive. GD scales better.\n",
    "\n",
    "## Normal Equation Limitation\n",
    "**Front:** What are two reasons the Normal Equation $\\theta = (X^TX)^{-1}X^Ty$ might fail? <br/>\n",
    "**Back:** 1. **Non-invertibility:** $X^TX$ is singular if features are linearly dependent (redundant). 2. **Computational inefficiency:** If $n$ is very large, the inversion is slow."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
