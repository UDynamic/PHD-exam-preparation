#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

13ML0001	Basic	13ML-LinearClassifiers	Write Bayes' theorem in probability notation. <br/>	<p>\( p(\theta | x) = \frac{p(x | \theta) \cdot p(\theta)}{p(x)} \)</p><ul><li>Posterior = Likelihood * prior / evidence</li><li>Ps(Posterior & Prior) like \(\theta\), likelihood dislikes \(\theta\) : \(\rightarrow\begin{cases} \text{prior} & p(\theta) \\ \text{posterior} & p(\theta|x) \\ \text{likelihood} & p(x|\theta)\end{cases} \)</li></ul>
13ML0002	Basic	13ML-LinearClassifiers	What is \(p(\theta)\) called in Bayes' theorem? <br/>	<p>The <strong>prior probability</strong> - our initial belief about \(\theta\) before seeing any data.</p><ul><li>Ps(Posterior & Prior) like \(\theta\), likelihood dislikes \(\theta\) : \(\rightarrow\begin{cases} \text{prior} & p(\theta) \\ \text{posterior} & p(\theta|x) \\ \text{likelihood} & p(x|\theta)\end{cases} \)</li></ul>
13ML0003	Basic	13ML-LinearClassifiers	What is \(p(\theta | x)\) called in Bayes' theorem? <br/>	<p>The <strong>posterior probability</strong> - our updated belief about parameters \(\theta\) after seeing data \(x\).</p><ul><li>Ps(Posterior & Prior) like \(\theta\), likelihood dislikes \(\theta\) : \(\rightarrow\begin{cases} \text{prior} & p(\theta) \\ \text{posterior} & p(\theta|x) \\ \text{likelihood} & p(x|\theta)\end{cases} \)</li></ul>
13ML0004	Basic	13ML-LinearClassifiers	What is \(p(x | \theta)\) called in Bayes' theorem? <br/>	<p>The <strong>likelihood</strong> - how probable the observed data \(x\) is given parameters \(\theta\).</p><ul><li>Ps(Posterior & Prior) like \(\theta\), likelihood dislikes \(\theta\) : \(\rightarrow\begin{cases} \text{prior} & p(\theta) \\ \text{posterior} & p(\theta|x) \\ \text{likelihood} & p(x|\theta)\end{cases} \)</li></ul>
13ML0005	Basic	13ML-LinearClassifiers	What does MLE find? Write its formula. <br/>	<p>The \(\theta\) that maximizes the likelihood:</p><p>\( \hat{\theta}_{MLE} = \arg\max_{\theta} p(x | \theta) \)</p><p>It assumes \(\theta\) is fixed but unknown.</p>
13ML0006	Basic	13ML-LinearClassifiers	What does MAP find? Write its formula. <br/>	<p>The \(\theta\) that maximizes the posterior:</p><p>\( \hat{\theta}_{MAP} = \arg\max_{\theta} p(\theta | x) = \arg\max_{\theta} p(x | \theta)p(\theta) \)</p><p>It assumes \(\theta\) is random with a prior distribution.</p>
13ML0007	Basic	13ML-LinearClassifiers	What's the fundamental difference between MLE and MAP? <br/>	<p>MLE uses only likelihood: \(\arg\max p(x|\theta)\). MAP adds prior: \(\arg\max p(x|\theta)p(\theta)\). MAP is "MLE with regularization" from prior belief.</p><ul><li>MAP multiplies \(p(\theta)\) (and p(x) is for normalization btw)so: \(\begin{cases} p(x|\theta)p(\theta) & Posterior\\ p(\theta|x) & prior\end{cases} \)</li></ul>
13ML0008	Basic	13ML-LinearClassifiers	What question do generative models answer? <br/>	<p>"What does each class look like?" They model \(p(x|y)\) - the probability of features given the class.</p><ul><li>Generative models learn the data(\(x\)), Discriminants focus on classification(\(y\)) : \(\rightarrow\begin{cases} \text{Generative} & \text{learns} \quad p(\text{data|class})=p(x|y)=p(x|\theta) \\ \text{Discriminative} & \text{learns} \quad p(\text{class|data})=p(y|x)=p(\theta|x)\end{cases} \)</li></ul>
13ML0009	Basic	13ML-LinearClassifiers	What question do discriminative models answer? <br/>	<p>"How do we tell classes apart?" They model \(p(y|x)\) - the probability of class given the features.</p><ul><li>Generative models learn the data(\(x\)), Discriminants focus on classification(\(y\)) : \(\rightarrow\begin{cases} \text{Generative} & \text{learns} \quad p(\text{data|class})=p(x|y)=p(x|\theta) \\ \text{Discriminative} & \text{learns} \quad p(\text{class|data})=p(y|x)=p(\theta|x)\end{cases} \)</li></ul>
13ML0010	Basic	13ML-LinearClassifiers	What does a generative model compute for classification? <br/>	<p> by modeling \(p(x|y)\) and \(p(y)\), then using Bayes' theorem.</p><p>\(p(y|x) = \frac{p(x|y) \cdot p(y)}{p(x)}\)</p><ul><li>Generative models learn the data(\(x\)), Discriminants focus on classification(\(y\)) : \(\rightarrow\begin{cases} \text{Generative} & \text{learns} \quad p(\text{data|class})=p(x|y)=p(x|\theta) \\ \text{Discriminative} & \text{learns} \quad p(\text{class|data})=p(y|x)=p(\theta|x)\end{cases} \)</li></ul>
13ML0011	Basic	13ML-LinearClassifiers	What does a discriminative model directly model? <br/>	<p>It directly models \(p(y|x)\), ignoring \(p(x|y)\) and \(p(x)\).</p><ul><li>Generative models learn the data(\(x\)), Discriminants focus on classification(\(y\)) : \(\rightarrow\begin{cases} \text{Generative} & \text{learns} \quad p(\text{data|class})=p(x|y)=p(x|\theta) \\ \text{Discriminative} & \text{learns} \quad p(\text{class|data})=p(y|x)=p(\theta|x)\end{cases} \)</li></ul>
13ML0012	Basic	13ML-LinearClassifiers	How would a generative model detect spam? <br/>	<p> 1. Learn \(p(\text{words}|\text{spam})\) and \(p(\text{words}|\text{ham})\). 2. Learn \(p(\text{spam})\) and \(p(\text{ham})\). 3. Classify using Bayes: \(p(\text{spam}|\text{words}) \propto p(\text{words}|\text{spam})p(\text{spam})\).</p>
13ML0013	Basic	13ML-LinearClassifiers	How would a discriminative model detect spam? <br/>	<p>Directly learn \(p(\text{spam}|\text{words}) = \sigma(\theta^T \cdot\text{features}(\text{words}))\) where \(\sigma\) is sigmoid. Ignore what spam/ham individually look like.</p>
13ML0014	Basic	13ML-LinearClassifiers	What can generative models do that discriminative ones cannot? <br/>	<p>Generate/sample new data points for each class because they learn \(p(x|y)\).</p>
13ML0015	Basic	13ML-LinearClassifiers	What do discriminative models focus on? <br/>	<p>Only the decision boundary between classes, not the full data distribution.</p>
13ML0016	Basic	13ML-LinearClassifiers	What are the two fundamental approaches for building a classifier? <br/>	<p>1. <strong>Discriminative:</strong> Find a decision boundary that directly separates the classes. 2. <strong>Generative:</strong> Model the class-conditional distribution \(p(x|C_k)\) for each class, then assign a new point to the class with the highest posterior probability \(p(C_k|x)\) using Bayes' theorem.</p>
13ML0017	Basic	13ML-LinearClassifiers	How do generative and discriminative models differ conceptually? Provide one example of each. <br/>	<p><strong>Generative</strong> models learn the joint distribution \(p(x, C_k)\) of inputs and labels (e.g., Naive Bayes, Linear Discriminant Analysis). <strong>Discriminative</strong> models learn the conditional distribution \(p(C_k|x)\) or directly map inputs to decision boundaries (e.g., Logistic Regression/Linear Classifier, Perceptron, SVM).</p>
13ML0018	Basic	13ML-LinearClassifiers	What is the primary condition required for a <em>linear</em> classifier to work perfectly? <br/>	<p>The data must be <strong>linearly separable/classifiable</strong>. This means a hyperplane exists that can perfectly separate the data points of different classes in the feature space.</p>
13ML0019	Basic	13ML-LinearClassifiers	In a 2D feature space, what is the general form of the linear function used for binary classification? <br/>	<p>\(f(x) = w_0 + w_1x_1 + w_2x_2\), where \(x=[x_1, x_2]^T\). The decision boundary is the line defined by \(f(x)=0\). Points are classified based on the sign of \(f(x)\).</p>
13ML0020	Basic	13ML-LinearClassifiers	For a data point \(x\), what does the quantity \(W^Tx\) (or \(w^Tx + w_0\)) represent geometrically? <br/>	<p>It is proportional to the signed <strong>distance</strong> from the point \(x\) to the decision boundary hyperplane, scaled by the norm of \(w\). The sign indicates which side of the boundary the point lies on.</p>
13ML0021	Basic	13ML-LinearClassifiers	Why is directly applying the Sum of Squared Errors (SSE) cost function from regression, \(J(W) = \sum_i (y^{(i)} - W^Tx^{(i)})^2\), problematic for classification? <br/>	<p>It assumes target \(y^{(i)}\) is a continuous value. In binary classification, targets (e.g., +1/-1) are discrete. Minimizing SSE does not guarantee the sign of \(W^Tx^{(i)}\) matches \(y^{(i)}\), which is the primary goal.</p>
13ML0022	Basic	13ML-LinearClassifiers	How can we modify the SSE cost to make it more suitable for classification? What function is introduced? <br/>	<p>Replace the continuous prediction \(W^Tx^{(i)}\) with the discrete class prediction: \(\text{sign}(W^Tx^{(i)})\). The cost becomes \(J(W) = \sum_i (y^{(i)} - \text{sign}(W^Tx^{(i)}))^2\). However, this is non-differentiable and difficult to optimize directly.</p>
13ML0023	Basic	13ML-LinearClassifiers	What is the Perceptron algorithm's fundamental rule for classification? <br/>	<p>For a data point \((x^{(i)}, y^{(i)})\) where \(y^{(i)} \in \{-1, +1\}\), the prediction is \(\hat{y} = \text{sign}(W^Tx^{(i)})\). The model is correct if \(W^Tx^{(i)}y^{(i)} > 0\).</p>
13ML0024	Basic	13ML-LinearClassifiers	What is the Perceptron cost function? Describe it in words and mathematically. <br/>	<p>It sums a measure of "error" over all <strong>misclassified</strong> points only. The cost is \(J(W) = -\sum_{i \in\mathcal{M}} W^Tx^{(i)}y^{(i)}\), where \(\mathcal{M}\) is the set of misclassified points for which \(W^Tx^{(i)}y^{(i)} \le0\).</p>
13ML0025	Basic	13ML-LinearClassifiers	How does the Perceptron cost function incorporate the distance of a misclassified point to the boundary? <br/>	<p>For a misclassified point, \(W^Tx^{(i)}y^{(i)}\) is negative. Its magnitude \(|W^Tx^{(i)}|\) is proportional to the point's distance to the boundary. Therefore, the cost \(J(W)\) sums the <em>signed negative distances</em>, penalizing misclassified points more severely the farther they are from the boundary.</p>
13ML0026	Basic	13ML-LinearClassifiers	Derive the parameter update rule for the Perceptron using (Stochastic) Gradient Descent on its cost function. <br/>	<p><br/></p><p>For a single misclassified point \((x^{(i)}, y^{(i)})\), \(J^{(i)}(W) = -W^Tx^{(i)}y^{(i)}\). <br/></p><p>The gradient is \(\nabla_W J^{(i)} = -x^{(i)}y^{(i)}\).<br/></p><p>The GD update is: \(W := W + \eta x^{(i)}y^{(i)}\), where \(\eta\) is the learning rate.</p><ul><li>Correctly classified points do not trigger an update.</li></ul>
13ML0027	Basic	13ML-LinearClassifiers	What are the conditions for the Perceptron algorithm to converge to a solution with zero training error? <br/>	<p>Two main conditions: 1. The training data must be <strong>linearly separable</strong>. 2. The learning rate \(\eta\) must be sufficiently small. Under these conditions, the Perceptron Convergence Theorem guarantees the algorithm will find a separating hyperplane in a finite number of steps.</p>
13ML0028	Basic	13ML-LinearClassifiers	What are key generalization considerations and pitfalls of the Perceptron? <br/>	<p><strong>Pitfalls:</strong> 1. If data is not linearly separable, the algorithm will never converge (oscillates). 2. It finds <em>a</em> separating plane, not necessarily the <em>best</em> one (max-margin). 3. The zero-one loss (cost) is non-differentiable. 4. The solution depends on the initialization and order of data presentation (SGD). <strong>Consideration:</strong> Simplicity makes it prone to high variance if data is noisy but separable.</p>
13ML0029	Basic	13ML-LinearClassifiers	Are MLE and MAP the same as generative and discriminative models? <br/>	<p><strong>No!</strong> MLE/MAP are <strong>parameter estimation methods</strong>. Generative/discriminative are <strong>model types</strong>. These are independent choices.</p>
13ML0030	Basic	13ML-LinearClassifiers	What do MLE and MAP determine about a model? <br/>	<p>How the model's <strong>parameters are learned</strong> from data. They don't define whether a model is generative or discriminative.</p>
13ML0031	Basic	13ML-LinearClassifiers	What do generative and discriminative determine about a model? <br/>	<p>What <strong>probability distribution</strong> the model learns: \(p(x|y)\) (generative) or \(p(y|x)\) (discriminative).</p>
13ML0032	Basic	13ML-LinearClassifiers	Can a generative model use MLE? Can a discriminative model use MAP? <br/>	<p><strong>Yes to both!</strong> Generative models can use MLE or MAP. Discriminative models can use MLE or MAP. These are separate choices.</p>
13ML0033	Basic	13ML-LinearClassifiers	What estimation methods can Naive Bayes (generative) use? <br/>	<p><strong>MLE</strong>: \(\arg\max p(\text{features}|\text{class})\) <br/></p><p><strong>or MAP</strong>: \(\arg\max p(\text{features}|\text{class})p(\text{class})\)</p>
13ML0034	Basic	13ML-LinearClassifiers	What estimation methods can logistic regression (discriminative) use? <br/>	<p><strong>MLE</strong>: \(\arg\max p(\text{class}|\text{features},\theta)\) <br/></p><p><strong>or MAP</strong>: \(\arg\max p(\text{class}|\text{features},\theta)p(\theta)\) (equivalent to adding regularization)</p>
13ML0035	Basic	13ML-LinearClassifiers	How to check if something is MLE/MAP vs generative/discriminative? <br/>	<p>Ask: 1. "Does it model \(p(x|y)\) or \(p(y|x)\)?" → generative/discriminative. 2. "Does it include a prior on parameters?" → MAP (yes) vs MLE (no).</p>
13ML0036	Basic	13ML-LinearClassifiers	What's wrong with saying "MAP is generative"? <br/>	<p>MAP is just parameter estimation with a prior. A <strong>discriminative</strong> model (like logistic regression) can use MAP estimation when regularized.</p>
13ML0037	Basic	13ML-LinearClassifiers	How does regularization relate to these concepts? <br/>	<p>Adding L2 regularization to discriminative models = MAP estimation with Gaussian prior. So MAP isn't exclusive to generative models.</p>
13ML0038	Basic	13ML-LinearClassifiers	Is Naive Bayes generative or discriminative? Why? <br/>	<p><strong>Generative</strong>. It models \(p(x|y)\) with the "naive" assumption that features are independent given the class: \(p(x|y) = \prod_i p(x_i|y)\).</p>
13ML0039	Basic	13ML-LinearClassifiers	Is logistic regression generative or discriminative? Why? <br/>	<p><strong>Discriminative</strong>. It directly models \(p(y|x) = \sigma(\theta^Tx)\) where \(\sigma\) is the sigmoid function.</p>
13ML0040	Basic	13ML-LinearClassifiers	Is SVM generative or discriminative? Why? <br/>	<p><strong>Discriminative</strong>. It finds the optimal separating hyperplane without modeling class distributions.</p>
13ML0041	Basic	13ML-LinearClassifiers	Is LDA (Linear Discriminant Analysis) generative or discriminative? Why? <br/>	<p><strong>Generative</strong>. It models each class as a Gaussian distribution with different means but same covariance matrix.</p>
13ML0042	Basic	13ML-LinearClassifiers	How is QDA different from LDA? <br/>	<p>QDA is also <strong>generative</strong> but allows each class to have its own covariance matrix, giving quadratic decision boundaries.</p>
13ML0043	Basic	13ML-LinearClassifiers	Are decision trees generative or discriminative? <br/>	<p><strong>Discriminative</strong>. They build rules to separate classes without modeling \(p(x|y)\).</p>
13ML0044	Basic	13ML-LinearClassifiers	Are standard neural networks for classification generative or discriminative? <br/>	<p><strong>Discriminative</strong>. They learn complex \(p(y|x)\) through multiple layers of transformations.</p>
13ML0045	Basic	13ML-LinearClassifiers	Is k-NN generative or discriminative? <br/>	<p>Neither strictly, but <strong>discriminative in spirit</strong>. It's non-parametric and classifies based on nearby examples without modeling distributions.</p>
13ML0046	Basic	13ML-LinearClassifiers	Are Bayesian networks generative or discriminative? <br/>	<p><strong>Generative</strong>. They model the joint probability distribution \(p(x,y)\) using a directed graph structure.</p>
13ML0047	Basic	13ML-LinearClassifiers	Are HMMs generative or discriminative? <br/>	<p><strong>Generative</strong>. They model sequences of observations as being generated by hidden states.</p>
13ML0048	Basic	13ML-LinearClassifiers	Is linear regression generative or discriminative? <br/>	<p><strong>Neither</strong> in traditional sense. It's regression, not classification. But it models \(p(y|x)\) with continuous \(y\), making it discriminative-like.</p>
13ML0049	Basic	13ML-LinearClassifiers	Is PCA (Principal Component Analysis) generative or discriminative? <br/>	<p><strong>Neither</strong>. It's unsupervised dimensionality reduction with no concept of labels or classification boundaries.</p>
13ML0050	Basic	13ML-LinearClassifiers	Are clustering methods (k-means, hierarchical) generative or discriminative? <br/>	<p><strong>Neither</strong>. They're unsupervised. If clusters are treated as "classes," the approach is generative-like as they model \(p(x|\text{cluster})\).</p>
13ML0051	Basic	13ML-LinearClassifiers	Can Parzen window methods be generative or discriminative? <br/>	<p><strong>Generative</strong> when used for density estimation. The Parzen window classifier estimates \(p(x|y)\) for each class.</p>
13ML0052	Basic	13ML-LinearClassifiers	How is L2 regularization in logistic regression related to MAP? <br/>	<p>Adding L2 regularization \(\lambda\|\theta\|^2\) is equivalent to MAP estimation with a Gaussian prior on \(\theta\).</p>
13ML0053	Basic	13ML-LinearClassifiers	When should you prefer generative models? <br/>	<p>1. When you need to generate data. 2. With missing data. 3. Small datasets (priors help). 4. When \(p(x|y)\) is naturally simple.</p>
13ML0054	Basic	13ML-LinearClassifiers	When should you prefer discriminative models? <br/>	<p>1. Pure classification task. 2. Large datasets. 3. When decision boundary is simpler than full distributions.</p>
13ML0055	Basic	13ML-LinearClassifiers	What's a common mistake when choosing between MAP and MLE? <br/>	<p>Using MAP without justifying the prior, or using MLE when you actually have useful prior knowledge about parameters.</p>
13ML0056	Basic	13ML-LinearClassifiers	What's the main limitation of Naive Bayes? <br/>	<p>The "naive" assumption that features are independent given the class is often false in real data, though it still works surprisingly well.</p>
13ML0057	Basic	13ML-LinearClassifiers	Can neural networks be both generative and discriminative? <br/>	<p>Yes! Standard NN classifiers are discriminative. But GANs and VAEs are generative networks that learn data distributions.</p>
13ML0058	Basic	13ML-LinearClassifiers	How do Bayesian methods relate to generative/discriminative categories? <br/>	<p>Bayesian methods provide a framework that's naturally generative, as they model full distributions with priors. But Bayesian logistic regression is discriminative with Bayesian inference on parameters.</p>
13ML0059	Basic	13ML-LinearClassifiers	Quick test: is a model generative or discriminative? <br/>	<p>Ask: "Can it generate/sample new data points for each class?" Yes → Generative. No → Discriminative.</p>