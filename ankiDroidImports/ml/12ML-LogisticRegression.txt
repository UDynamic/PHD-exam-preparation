#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6
12ML0001	Basic	12ML-LogisticRegression	What is \(\theta^T X\) in the context of logistic regression? `<br/>`	It is the **linear combination** (or dot product) between the parameter vector \(\theta\) and the input feature vector \(X\). Mathematically: \(\theta^T X = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n\), where \(\theta_0\) is the bias/intercept term (often with \(x_0 = 1\) included in \(X\)).
12ML0002	Basic	12ML-LogisticRegression	Geometrically, what does \(\theta^T X = 0\) represent? `<br/>`	It defines a **hyperplane** (e.g., a line in 2D) in the feature space. This hyperplane is the **decision boundary** where the model predicts exactly 0.5 probability for both classes.
12ML0003	Basic	12ML-LogisticRegression	In binary logistic regression, what function replaces the linear hypothesis \(h_\theta(x) = \theta^T x\)? `<br/>`	for activation function as \(g(x)\) : \(h_\theta(x) = g(\theta^T x)\)<br><br>* The sigmoid activation function: \(h_\theta(x) = \sigma(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}\).<br><br>\(0\leq h_\theta(x) \leq1\)<br><br>* `<u>`Mind the negative power of e `</u>`
12ML0004	Basic	12ML-LogisticRegression	What is the role of \(\theta^T X\) in the logistic regression hypothesis function \(h_\theta(x)\)? `<br/>`	\(\theta^T X\) serves as the input to the sigmoid function: \(h_\theta(x) = \sigma(\theta^T X) = \frac{1}{1 + e^{-\theta^T X}}\). It is the **linear predictor** or **logit**.
12ML0005	Basic	12ML-LogisticRegression	How is the hypothesis \(h_\theta(x)\) interpreted probabilistically for binary classification? `<br/>`	\(h_\theta(x)\) represents the estimated probability that \(y=1\) given \(x\):<br><br>\(p(y=1|x;\theta) = h_\theta(x)\).
12ML0006	Basic	12ML-LogisticRegression	What is the probability that \(y=0\) given \(x\) in binary logistic regression? `<br/>`	\(p(y=0|x;\theta) = 1 - p(y=1|x;\theta) = 1 - h_\theta(x)\).
12ML0007	Basic	12ML-LogisticRegression	How does logistic regression make binary classification decisions? `<br/>`	It compares the predicted probability \(h_\theta(x) = p(y=1|x)\) to 0.5. Predict \(y=1\) if \(h_\theta(x) \geq0.5\), otherwise predict \(y=0\). Equivalently, predict \(y=1\) if \(\theta^T x \geq0\), since \(\sigma(0) = 0.5\).
12ML0008	Basic	12ML-LogisticRegression	How can we write the probability \(p(y|x;\theta)\) for both classes (\(y=0\) or \(y=1\)) in a single formula? `<br/>`	\(p(y|x;\theta) = (h_\theta(x))^{y} \cdot (1 - h_\theta(x))^{1-y}\).
12ML0009	Basic	12ML-LogisticRegression	What is the decision boundary in logistic regression, and where is it located? `<br/>`	It is the set of points \(x\) where \(p(y=1|x;\theta) = p(y=0|x;\theta) = 0.5\).<br><br>This occurs when \(\theta^T x = 0\).
12ML0010	Basic	12ML-LogisticRegression	For what values of θᵀx will the model predict y=1? `<br/>`	When θᵀx > 0, which means h_θ(x) = σ(θᵀx) > 0.5. The model predicts class 1 in this region.
12ML0011	Basic	12ML-LogisticRegression	For what values of θᵀx will the model predict y=0? `<br/>`	When θᵀx < 0, which means h_θ(x) = σ(θᵀx) < 0.5. The model predicts class 0 in this region.
12ML0012	Basic	12ML-LogisticRegression	For a training set of \(m\) i.i.d. examples, what is the likelihood function \(L(\theta)\)? `<br/>`	The product of individual probabilities: \(L(\theta) = \prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta) = \prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}} (1-h_\theta(x^{(i)}))^{1-y^{(i)}}\).<br><br>* this is a Mximization problem
12ML0013	Basic	12ML-LogisticRegression	Why do we take the log of the likelihood, and what is \(l(\theta) = \log L(\theta)\)? `<br/>`	Log transforms products into sums, simplifying calculus. \(l(\theta) = \sum_{i=1}^m [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]\).<br><br>* this is a Mximization problem
12ML0014	Basic	12ML-LogisticRegression	What is the standard cost function \(J(\theta)\) for logistic regression, and how is it derived? `<br/>`	It is the average negative log-likelihood. `<br/>`<br><br>\(J(\theta) = -l(\theta) = - \sum_{i=1}^m [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]\)<br><br>Average form :`<br/>`<br><br>\(J(\theta) = -\frac{1}{m} l(\theta)\)<br><br>* Minimizing \(J(\theta)\) is equivalent to maximizing the likelihood \(L(\theta)\).
12ML0015	Basic	12ML-LogisticRegression	In linear regression, the cost function is Sum of Squared Errors (SSE). Why would SSE be a poor choice for logistic regression? `<br/>`	SSE \(J(\theta) = \frac{1}{2}\sum (h_\theta(x^{(i)}) - y^{(i)})^2\) applied to \(h_\theta(x)=\sigma(\theta^T x)\) yields a **non-convex** function with many local minima. Gradient descent might not find the global optimum.
12ML0016	Basic	12ML-LogisticRegression	What is the parameter update rule for gradient descent using the cost \(J(\theta) = - l(\theta)\)? `<br/>`	General form: \(\begin{cases} \theta_j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial\theta_j}\\ \frac{\partial J(\theta)}{\partial\theta_j} = \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \end{cases}\)<br><br>\(\Rightarrow\)<br><br>\(\begin{cases} \Delta\theta_0 = - \alpha[\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})] & j=0\\ \Delta\theta_j = - \alpha[\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}] & j \geq1\end{cases}\)
12ML0017	Basic	12ML-LogisticRegression	How is L2 regularization typically added to the logistic regression cost function \(J(\theta)\)? `<br/>`	\(J_{reg}(\theta) = -l(\theta) + \frac{\lambda}{2} \sum_{j=1}^n \theta_j^2\).<br><br>The bias term \(\theta_0\) is often excluded from regularization.
12ML0018	Basic	12ML-LogisticRegression	What is the parameter update rule for gradient descent using L2 regularization? `<br/>`	General form:<br><br>\(\begin{cases} J(\theta) = - l(\theta) + \frac{\lambda}{2} \sum_{j=1}^n \theta^2 \\ \theta_j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial\theta_j}\\ \frac{\partial J(\theta)}{\partial\theta_j} = \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda\theta_j\end{cases}\)\(\Rightarrow\) \(\begin{cases} \Delta\theta_0 = - \alpha[\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})] & j=0\\ \Delta\theta_j = - \alpha[\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda\theta_j] & j \geq1\end{cases}\)
12ML0019	Basic	12ML-LogisticRegression	What is the derivative of the sigmoid function \(\sigma(z)\), and why is this useful? `<br/>`	\(\frac{d}{dz}\sigma(z) = \sigma(z)(1 - \sigma(z))\). This simplifies the gradient calculation for the log-likelihood.
12ML0020	Basic	12ML-LogisticRegression	How does the One-vs-Rest (OvR) strategy work for multi-class classification with logistic regression? `<br/>`	For \(K\) classes, train \(K\) separate binary classifiers. Classifier \(k\) learns to distinguish class \(k\) (positive) from all other classes (negative). Prediction is the class with the highest \(h_\theta^{(k)}(x)\).<br><br>* as there is K classes, there will be K linear classifications.
12ML0021	Basic	12ML-LogisticRegression	How does the One-vs-One (OvO) strategy work? `<br/>`	Train a binary classifier for every pair of classes. This requires \(\binom{K}{2} = \frac{K(K-1)}{2}\) classifiers.<br><br>* Prediction is made by majority vote from all classifiers. in other words The class that appears as the winner in the largest number of pairwise comparisons is selected.
12ML0022	Basic	12ML-LogisticRegression	what is Softmax ? `<br/>`	Softmax takes n numbers like \(z_1, z_2, \dots z_n\) and Outputs n probabilities that sum to 1.<br><br>General formula : \(\text{softmax}_i= \frac{e^{z_i}}{\sum_{k=1}^n e^{z_k}} \)<br><br>* \(\sum_{i=1}^n \text{softmax}_i = 1\)
12ML0023	Basic	12ML-LogisticRegression	What is the direct multi-class generalization of logistic regression called, and what is its hypothesis function? `<br/>`	Softmax Regression. For class \(k\) with parameters \(\theta^{(k)}\), \(p(y=k|x;\Theta) = \frac{e^{\theta^{(k)T} x}}{\sum_{j=1}^K e^{\theta^{(j)T} x}}\), where \(\Theta\) is the matrix of all parameters.
12ML0024	Basic	12ML-LogisticRegression	Before applying the sigmoid or softmax, what do the raw scores \(\theta^T x\) or \(\theta^{(k)T} x\) represent? `<br/>`	They are log-odds (for binary) or logits. They exist on the whole real line and are not probabilities. The activation function converts them to probabilities.
12ML0025	Basic	12ML-LogisticRegression	Why is logistic regression considered a **linear classifier** despite using a nonlinear sigmoid function? `<br/>`	Because its decision boundary is linear. Predict \(y=1\) when \(\theta^Tx > 0\) and \(y=0\) when \(\theta^Tx < 0\). The boundary \(\theta^Tx = 0\) defines a hyperplane in feature space.
12ML0026	Basic	12ML-LogisticRegression	What probability distribution does logistic regression assume for the target variable \(y\) given input \(x\)? `<br/>`	Bernoulli distribution: \(P(y|x; \theta) = h_\theta(x)^y (1 - h_\theta(x))^{1-y}\), where \(h_\theta(x) = \sigma(\theta^Tx)\) is the predicted probability that \(y=1\).
12ML0027	Basic	12ML-LogisticRegression	What type of model is logistic regression—generative or discriminative—and what does it directly model? `<br/>`	Discriminative. It directly models the posterior probability \(P(y|x)\) without modeling the class-conditional distributions \(P(x|y)\) (which would be generative).
12ML0028	Basic	12ML-LogisticRegression	How does the signed distance \(|\theta^Tx|\) relate to a point's position relative to the decision boundary? `<br/>`	\(|\theta^Tx|\) is proportional to the **perpendicular distance** from point \(x\) to the hyperplane \(\theta^Tx = 0\). Larger magnitude means greater distance and higher confidence.