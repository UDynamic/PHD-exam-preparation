#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

gw.ao-4QO+	Basic	ML-Generalization	What is the ideal outcome for a trained machine learning model?	Both training error and test error should be near zero: <br> \( \text{Error}_{train} \approx \text{Error}_{test} \approx 0 \).
G=W$!i>KdB	Basic	ML-Generalization	What does "bias" measure in a machine learning model? <br/>	The error from the model's inability to learn true patterns in the data. High bias means the model is oversimplified.
PmUbP:@@{N	Basic	ML-Generalization	What does "variance" measure in a machine learning model? <br/>	How much the model's predictions change when trained on different datasets. High variance means the model is too sensitive to the training data.
R!7Gkz#0Vq	Basic	ML-Generalization	How can you identify overfitting from error metrics? <br/>	Training error is very low but test error is high: \( Error_{train} \ll Error_{test} \).
S$pmF@L9T2	Basic	ML-Generalization	What are common causes of overfitting? <br/>	Overly complex model, too many features, too little training data, or too much noise in the data.
T9nBc#1XwE	Basic	ML-Generalization	How can you identify underfitting from error metrics? <br/>	Both training and test errors are high: \( Error_{train} \approx Error_{test} \gg 0 \).
U4vQd$3ZrC	Basic	ML-Generalization	What causes underfitting? <br/>	Using a model that is too simple to capture patterns in the data.
V8tYe*5AsD	Basic	ML-Generalization	How does model complexity affect bias and variance? <br/>	Low complexity → high bias, low variance. High complexity → low bias, high variance.
W2rFg^6BpH	Basic	ML-Generalization	What three components make up generalization error? <br/>	\( Generalization\ Error = (Bias)^2 + Variance + Irreducible\ Error \)
X5tGh&7NiJ	Basic	ML-Generalization	Which parts of generalization error can we reduce, and which can't we? <br/>	<strong>Reducible:</strong> Bias and Variance (by improving the model). <strong>Irreducible:</strong> Noise in the data (cannot be eliminated).
Y1uHj*8MkL	Basic	ML-Generalization	How can you reduce high bias in a model? <br/>	Increase model complexity, add more relevant features, or use a more powerful algorithm.
Z6iKq(9OlP	Basic	ML-Generalization	How can you reduce high variance in a model? <br/>	Simplify the model, use regularization, get more training data, or reduce features.
a3bNl)0QmR	Basic	ML-Generalization	How does more training data affect bias and variance? <br/>	More data primarily reduces variance (makes model more stable) but doesn't fix high bias if the model is too simple.
b7cOd!1SnT	Basic	ML-Generalization	How does regularization help with the bias-variance tradeoff? <br/>	It penalizes overly complex models, reducing variance (preventing overfitting) while possibly slightly increasing bias.
c4dPe@2ToU	Basic	ML-Generalization	Why is cross-validation important for managing bias and variance? <br/>	It provides better estimates of test error, helping select models that generalize well (balance bias and variance).
d9eQf#3UpV	Basic	ML-Generalization	Where is the optimal model complexity located? <br/>	At the point where total generalization error is minimized, balancing bias and variance.
e6fRg$4VqW	Basic	ML-Generalization	If both training and test errors are high, what's likely the problem? <br/>	High bias (underfitting) - the model is too simple.
f1gSh%5WrX	Basic	ML-Generalization	If training error is low but test error is high, what's likely the problem? <br/>	High variance (overfitting) - the model is too complex.
g8hTi&6XsY	Basic	ML-Generalization	What causes irreducible error in machine learning? <br/>	Random noise in the data that cannot be predicted, such as measurement errors or inherent randomness.
h5iUj*7YtZ	Basic	ML-Generalization	What practical steps help find the right bias-variance balance? <br/>	1. Start with simple model. 2. Gradually increase complexity while monitoring validation error. 3. Use regularization. 4. Get more data if possible.
j2kVk(8ZuA	Basic	ML-Generalization	What happens to bias and variance as training data size approaches infinity? <br/>	<strong>Variance approaches zero, bias remains unchanged.</strong> With infinite data, the model sees all possible variations, making predictions stable (zero variance). However, bias—the model's fundamental inability to capture the true relationship—is determined by model capacity, not data quantity.
k9lWm)0AvB	Basic	ML-Generalization	What do learning curves show as training size increases toward infinity? <br/>	The gap between training and test error (variance) narrows to zero, and both converge to the same value—the bias of the model. If both errors remain high at convergence, the model has high bias (underfitting).
m0nXo!1BwC	Basic	ML-Generalization	What model should you choose if you have access to infinite data? <br/>	<strong>The most complex model possible</strong> (lowest bias). With infinite data, variance is zero, so the bias-variance tradeoff disappears. Complexity is free—choose the model class that can best approximate the true function.
p3qYp@2CxD	Basic	ML-Generalization	What happens to irreducible error as training data approaches infinity? <br/>	<strong>Irreducible error remains constant.</strong> This is the inherent noise in the data generation process (e.g., measurement error, true stochasticity). No amount of data can eliminate it, setting a lower bound on possible error.
r6sZr#3DyE	Basic	ML-Generalization	What is the ideal outcome for a well-performing machine learning model? <br/>	\( Error_{train} \approx Error_{test} \approx0 \). The model performs equally well on the data it was trained on and on new, unseen data.
t7uAs$4EzF	Basic	ML-Generalization	In the context of model error, what is <em>bias</em>? <br/>	The error arising from the model's inability to represent the true underlying pattern in the data. A high-bias model is too simplistic.
v1wBt%5FaG	Basic	ML-Generalization	In the context of model error, what is <em>variance</em>? <br/>	The error arising from the model's sensitivity to fluctuations in the specific training dataset. A high-variance model is overly complex and fits the noise.
x2yCu&6GbH	Basic	ML-Generalization	What is overfitting, and what are the typical error characteristics? <br/>	The model learns the training data (including noise) too well. \( Error_{train} \) is very low, but \( Error_{test} \) is significantly higher (\( Error_{train} \ll Error_{test} \)). It corresponds to <strong>high variance</strong>.
z3zDv*7HcI	Basic	ML-Generalization	List three common causes of overfitting (high variance). <br/>	1. Model is too complex (e.g., high-degree polynomial). 2. Too many features relative to data points. 3. Training data contains significant noise.
A4aEw(8IdJ	Basic	ML-Generalization	What is underfitting, and what are the typical error characteristics? <br/>	The model fails to learn the underlying pattern in the data. Both \( Error_{train} \) and \( Error_{test} \) are high (\( Error_{train} \approx Error_{test} \gg0 \)). It corresponds to <strong>high bias</strong>.
B5bFx)9JeK	Basic	ML-Generalization	List two common causes of underfitting (high bias). <br/>	1. Model is too simple (e.g., linear model for a complex problem). 2. Too few features to capture the relevant patterns.
C6cGy!0KfL	Basic	ML-Generalization	How does model complexity relate to bias and variance? <br/>	<strong>Low Complexity:</strong> High Bias, Low Variance (underfitting). <strong>High Complexity:</strong> Low Bias, High Variance (overfitting). The goal is to find the optimal complexity that balances both.
D7dHz@1LgM	Basic	ML-Generalization	What is the canonical decomposition of the expected generalization error? <br/>	\( E[(y - \hat{f}(x))^2] = \text{Bias}[\hat{f}(x)]^2 + \text{Var}[\hat{f}(x)] + \sigma^2 \). Where \( \sigma^2 \) is the irreducible error.
E8eIa#2MhN	Basic	ML-Generalization	Distinguish between reducible and irreducible error in the bias-variance decomposition. <br/>	<strong>Reducible:</strong> Bias and Variance. We can reduce these by improving the model. <strong>Irreducible:</strong> \( \sigma^2 \), the inherent noise in the data. It sets a lower bound on total error and cannot be eliminated.
F9fJb$3NiO	Basic	ML-Generalization	What actions can you take if your model is underfitting (high bias)? <br/>	Increase model complexity: Use a more powerful model (e.g., higher degree polynomial, neural network), add more relevant features, or reduce regularization strength.
G0gKc%4OjP	Basic	ML-Generalization	What actions can you take if your model is overfitting (high variance)? <br/>	Simplify the model: Use fewer features, get more training data, increase regularization strength (L1/L2), or use a simpler model class.
H1hLd&5PkQ	Basic	ML-Generalization	As the training dataset size \( m \to\infty \), what happens to bias and variance? <br/>	<strong>Variance \( \to0 \).</strong><strong>Bias remains constant.</strong> With infinite data, the model estimate stabilizes, but its fundamental representational capacity (bias) is unchanged.
I2iMe*6QlR	Basic	ML-Generalization	On a learning curve (error vs. training size), what do the converging training and test errors represent as \( m \to\infty \)? <br/>	They converge to the same value: the <strong>bias</strong> of the model plus the <strong>irreducible error</strong> \( \sigma^2 \). The gap between them (due to variance) vanishes.
J3jNf(7RmS	Basic	ML-Generalization	If you had infinite data, what type of model would you choose and why? <br/>	The most complex, lowest-bias model possible (e.g., a very large neural network). With infinite data, variance is eliminated, so you only need to minimize bias.
K4kOg)8SnT	Basic	ML-Generalization	If both training and test error are high, is the problem always high bias? What's a key consideration? <br/>	Not always. While high bias is a likely cause, it could also be due to <strong>very high variance coupled with poorly representative data</strong>. Check if the training error itself is high (true bias) or if there's a massive gap to test error (variance).
L5lPh!9ToU	Basic	ML-Generalization	Why isn't "always use the most complex model" a good strategy in practice? <br/>	Because in the real world with <strong>finite data</strong>, increasing complexity reduces bias but increases variance. The optimal model is the one that best balances this trade-off for your specific dataset size and problem.