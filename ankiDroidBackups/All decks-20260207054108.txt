#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6
q[6MjIb^kb	Basic	ML-Evaluation	Accuracy	"<span style=""color:gold""> Ts Above all</span><br><br>How often is the classifier correct<br><br><br>\(Accuracy = \frac{TP + TN}{TP + TN + FP + FN}\)<br><br>"	
t%ObJ)|9.!	Basic	ML-Evaluation	Precision	"<span style=""color:gold"">P for Predicted Positive</span><br><br>In all Predicted Positives how many where True<br><br>\(Precision = \frac{TP}{TP + FP}\)<br>"	
P];X+G_=T:	Basic	ML-Evaluation	Recall (Sensitivity)	"<span style=""color:gold"">True Positive</span> over the sum with the opposite<br><br>\(Recall_{Sensitivity} = \frac{TP}{TP + FN} \)<br><br><span style=""color:pink"">\(Recall_{Sensitivity}\) \(\neq\) Specificity </span><br><span style=""color:gold"">RecSen is Smoother</span>"	
r:?h4ZK~4~	Basic	ML-Evaluation	Specificity	"<span style=""color:gold"">True Negative </span>over the sum with the Opposite<br><br>\(Specificity = \frac{TN}{TN + FP}\)<br><br><span style=""color:pink"">Recall_Sensitivity \(\neq\) Specificity</span>"	
jGND$h!Cf(	Basic	ML-Evaluation	F1 Score	"<span style=""color:gold"">2 to Two Positives</span><br><br>\(F1 Score = 2 \frac{Precision * Recall}{Precision + Recall} \)<br><br>\(Precision = \frac{TP}{TP + FP}\)<br><br>\(Recall_{Sensitivity} = \frac{TP}{TP + FN}\) <br>"	
qyb9E1G13b	Basic	ML-MLE&MAP	What is Maximum Likelihood Estimation (MLE)?	MLE is a statistical method used to estimate the parameters of a model based on observed data. <br><br>Goal: Find the parameter values that maximize the probability of observing the given data.	
Qs4-zB6w_G	Basic	ML-MLE&MAP	likelihood function !? 	"<span style=""color:gold"">Conditional probability of seeing data with condition of that parameter</span><br><br>Given independent observations \(x1, x2, ..., x_n \) from a distribution with parameter \( \theta \).<br><br>the likelihood function is: <br>\(\mathcal{L}(\theta \mid x_1, x_2, \dots, x_n) = \prod_{i=1}^n P(x_i \mid \theta) \)"	
JY^(3PR7^z	Basic	ML-MLE&MAP	Log-Likelihood Function !? 	\( \ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log P(x_i \mid \theta)\)	
JOC@m:%wNH	Basic	ML-MLE&MAP	Maximum Likelihood Estimation (MLE)	 We find the value of \( \theta \) that maximizes the log-likelihood: <br><br>\( \ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log P(x_i \mid \theta) \)<br><br>\( \hat{\theta} = \arg\max_{\theta} \ell(\theta) \)	
P.sW0hrA75	Basic	ML-MLE&MAP	MLE for Bernoulli Distribution	"<span style=""color:gold"">All ML Estimators Except Exponential are equal to Average</span> <br>for Exponential is average reversed<br><br>\( \hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} x_i \)<br><hr><br>PMF:<br>\(P(x \mid \theta) = \theta^x (1 - \theta)^{1 - x} \)<br><br>Likelihood function:<br>\( L(p \mid x_1, x_2, ..., x_n) = \prod_{i=1}^{n} P(x_i \mid p) = p^{x_i} (1 - p)^{1 - x_i} \)<br><br>Log-Likelihood: <br>\( \ell(\theta) = \sum_{i=1}^{n} \left[ x_i \log \theta + (1 - x_i) \log (1 - \theta) \right] \)"	
L?%2A3vS;z	Basic	ML-MLE&MAP	PMF for Bernoulli Distribution	"<span style=""color:gold""> Bernoulli is binary</span><br><br>One trial with two outcomes (success/failure) <br><br>\(P(x \mid p) = p^x (1 - p)^{1 - x} \)"	
EF$@Zb9)%=	Basic	ML-MLE&MAP	MLE for Normal Distribution	"<span style=""color:gold"">All ML Estimators Except Exponential are equal to Average</span> <br>for Exponential is average reversed<br><br>\(\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i \)<br><br>\( \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{\mu})^2 \)<br><hr><br>PDF for Normal Distribution <br>\( P(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) \)<br><br><br>Likelihood:<br>\(L(\mu, \sigma^2 \mid x_1, x_2, ..., x_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x_i - \mu)^2}{2\sigma^2} \right) = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2 \right) \)<br><br>Log-Likelihood: <br>\(\ell(\mu, \sigma^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2 \)"	
if-=p!&j{x	Basic	ML-MLE&MAP	PDF for Normal Distribution 	PDF for Normal Distribution <br>\( P(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) \)	
tDL,fNe3Cb	Basic	ML-MLE&MAP	MLE for Poisson Distribution	"<span style=""color:gold"">All ML Estimators Except Exponential are equal to Average</span> <br>for Exponential is average reversed<br><br>PMF: <br>\(P(x \mid \lambda) = \frac{\lambda^xe^{-\lambda} }{x!} \)<br><br>Likelihood:<br>\( L(\lambda \mid x_1, x_2, \dots, x_n) = \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} \)<br><br>Log-Likelihood: <br>\(\ell(\lambda) = \sum_{i=1}^{n} \left[ x_i \ln \lambda - \lambda - \ln(x_i!) \right]\)<br>\(\frac{d\ell(\lambda)}{d\lambda} = \sum_{i=1}^{n} \left( \frac{x_i}{\lambda} - 1 \right)\)<br><br>\(\sum_{i=1}^{n} \left( \frac{x_i}{\lambda} - 1 \right) = 0\)<br><br>\(\sum_{i=1}^{n} \frac{x_i}{\lambda} = n\)<br><br>\( \hat{\lambda} = \frac{1}{n} \sum_{i=1}^{n} x_i \)<br>"	
miDVI<S:lF	Basic	ML-MLE&MAP	PMF for Poisson distribution 	"<span style=""color:gold"">Like Exponential with X to the power and X factorial to the bottom </span><br><br>Number of events in a time interval<br><br>PMF: <br>\(P(x \mid \lambda) = \frac{\lambda^xe^{-\lambda} }{x!} \)"	
K~=.gyJ,|!	Basic	ML-MLE&MAP	MLE for Exponential Distribution	"<span style=""color:gold"">Only Distribution with ML estimator reverse of the average</span><br>allothers are equal to the average<br><br>\(\hat{\lambda} = \frac{1}{\bar{x}} = \frac{n}{\sum x_i} \)<br><hr><br> PDF:<br> \( P(x \mid \lambda) = \lambda e^{-\lambda x} \) for \(x \geq 0 \)<br><br><br>Likelihood:<br>\( L(\lambda \mid x_1, x_2, \dots, x_n) = \prod_{i=1}^{n} \lambda e^{-\lambda x_i} = \lambda^n e^{-\lambda \sum_{i=1}^{n} x_i} \)<br><br><br>Log-Likelihood: <br>\(\ell(\lambda) = n \log \lambda - \lambda \sum_{i=1}^{n} x_i \)<br>"	
gw.ao-4QO+	Basic	ML-MLE&MAP	PDF for Exponential distribution	 PDF:<br> \( P(x \mid \lambda) = \lambda e^{-\lambda x} \) for \(x \geq 0 \)	
G=W$!i>KdB	Basic	ML-LinearAlgebra	For<br><br>\(\mathbf{u} = (u1, u2, \dots, u_n)\)<br><br>Then<br><br>L1 Norm (Manhattan Norm)	\( \|\mathbf{u}\|_1 = |u_1| + |u_2| + \dots + |u_n| \)	
PmUbP:@@{N	Basic	ML-LinearAlgebra	For<br><br>\(\mathbf{u} = (u1, u2, \dots, u_n)\)<br><br>Then<br><br>L2 Norm (Euclidean Norm)	"\(\|\mathbf{u}\|_2 = \sqrt{u_1^2 + u_2^2 + \dots + u_n^2} \)<br><br><span style=""color:pink"">If not mentioned the norm, this is what they mean</span><br><br><span style=""color:cyan"">\(\|\mathbf{u}\|_2 = \|\mathbf{u}\| \)</span>"	
Kc58]hM4[7	Basic	ML-LinearAlgebra	For<br><br>\(\mathbf{u} = (u1, u2, \dots, u_n)\)<br><br>Then<br><br>L\(\infty\) Norm <br>(Maximum or Chebyshev Norm)	\( \|\mathbf{u}\|_\infty = \max(|u_1|, |u_2|, \dots, |u_n|) \)	
G3m1:Ah(H&	Basic	ML-LinearAlgebra	For<br><br>\(\mathbf{u} = (u1, u2, \dots, u_n)\)<br><br>Then<br><br>\(L_p\) Norm <br>(Minkowsky Generalized Norm)<br><br>For any \(p \geq 1 \)	"\( \|\mathbf{u}\|_p = \left( |u_1|^p + |u_2|^p + \dots + |u_n|^p \right)^{1/p} \)<br><br><span style=""color:pink"">\(L_1\), \(L_2\) and \(L_\infty\) as special cases when \(p = 1 \), \(p = 2\) and \(p=\infty\)</span>"	
mm[RLcGzEn	Basic	ML-LinearAlgebra	For<br><br>\(\mathbf{v} = (v1, v2, \dots, v_n)\)<br><br>Then<br><br>Unit vector	A unit vector is a vector with a norm (Magnitude or length) of 1<br><br>\(\hat{\mathbf{v}} = \frac{\mathbf{v}}{\|\mathbf{v}\|} \)	
z8/<!+xri1	Basic	ML-LinearAlgebra	Given two vectors<br> \( \mathbf{x}, \mathbf{y} \in \mathbb{R}^n \), and <br>a positive definite(\(\det(\mathbf{\Sigma}) \neq 0 => Reversible\)) covariance matrix \( \mathbf{\Sigma} \), <br><br>the Mahalanobis distance	"\(D_M(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{y})} \)<br><br><span style=""color:pink"">If  \(\mathbf{\Sigma} = \mathbf{I} \) (identity matrix), the Mahalanobis distance reduces to the Euclidean distance</span>"	
ExL7+mscBV	Basic	ML-LinearAlgebra	"The angle between two vectors  \(\mathbf{u}\)  and  \(\mathbf{v} \)<br><br><span style=""color:lime"">COSINE SIMILARITY</span>"	" \(\cos(\theta) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}\) <br><br><span style=""color:pink"">For Matrices you have to Vectorize them. (Just serialize the elements)</span>"	
F=>JnUE/I>	Basic	ML-LinearAlgebra	For A and B n*n Matrices<br><br>\((AB)^T\)	\(B^TA^T\)	
"nI|#Apk;l#"	Basic	ML-LinearAlgebra	For A a Symmetric matrix<br><br>What is<br><br>\(A^T\)	A	
OcErP,--DT	Basic	ML-LinearAlgebra	What is the difference between the Identity Matrix and a Diagonal Matrix	"The main diameter of diagonal matrix could be any number. <br><br><span style=""color:pink"">All other elements are Zeros</span>"	
FrR>]`,`4$	Basic	ML-LinearAlgebra	What is Determinant for a Diagonal Matrix	"Product of all the main diameter elements<br><br><span style=""color:cyan"">It's the same for an Upper triangular matrix</span>"	
m^S_U%X^4x	Basic	ML-LinearAlgebra	for \(A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \)<br><br>\(A^{-1}\)	"\(A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \)<br><br>\(\text{det}(A) = ad - bc\) <br><br> <span style=""color:pink"">(if not Singular: \(\text{det}(A) \neq 0 \))<br></span><br><br><br>\(A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}\) "	
xUH_y[$3zf	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br>and<br>\(|A|\) as \(\det(A)\)<br>and C as any number<br><br>What is<br>\(|CA|\)	\(C^n|A|\)	
J1$uq&$R{	Basic	ML-LinearAlgebra	For A and B \(n*n\) matrix<br>and<br>\(|A|\) as \(\det(A)\)<br><br>What is<br>\(|AB|\)	\(|A||B|\)	
E:35e@c3z5	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br>and<br>\(|A|\) as \(\det(A)\)<br><br>What is<br>\(|A^T|\)	\(|A|\)	
EGk({VLp>A	Basic	ML-LinearAlgebra	What is the Inverse of a Diagonal Matrix	Inverse Main diameter elements	
"hj[36V#JCA"	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br><br>\(AA^{-1}\)	\(I\)	
Bc`r1n@DI}	Basic	ML-LinearAlgebra	For A and B \(n*n\) matrices<br><br>\((AB)^{-1}\)	\(B^{-1}A^{-1}\)	
H/oc<>v25Q	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br><br>\((A^{-1})^{-1}\)	\(A\)	
Nk=vU)Gm,]	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br><br>\((A^T)^{-1}\)	\((A^{-1})^T\)	
oi()sn^T}F	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br>and<br>and C as any number<br><br>\((cA)^{-1}\)	\(\frac{1}{c} A^{-1}\)	
B}&/E2~BE%	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br><br>Condition for A to be <br><br>Positive definite	"\(\forall \vec{X} \neq 0\)<br><br>\(X^{T}AX > 0\)<br><br><span style=""color:cyan"">Could be tested using unknown parameters for \(x = \{ x_1, x_2, \dots, x_n\}\)</span><br><br><span style=""color:pink"">for A, All the Eigenvalues(\(\lambda\)) are positive</span>"	
"dM^qFhH4#7"	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br><br>Condition for A to be <br><br>Positive semi-definite	"\(\forall \vec{X} \)<br><br>\(X^{T}AX \geq 0\)<br><br><span style=""color:cyan"">Could be tested using unknown parameters for \(x = \{ x_1, x_2, \dots, x_n\}\)</span><br><br><span style=""color:pink"">for A, All the Eigenvalues(\(\lambda\)) are None Negative</span><br>So<br><span style=""color:pink"">Might Not be revertible (one zero Eigenvalue would make determinant zero)</span><br>"	
K$EvtTU&$F	Basic	ML-LinearAlgebra	How would a Positive semi-definite Matrix be Reversable	"If and only if it's <u>Positive Definite</u><br><br><span style=""color:pink"">All Eigenvalues Positive and determinant none Zero</span>"	
Nm^Jv-3&_N	Basic	ML-LinearAlgebra	For A an \(n*n\) matrix<br><br>Eigenvalue	"An eigenvalue  \(\lambda\)  is a scalar such that for \(\forall \vec{v} \neq 0\) (eigenvector)<br>\(A v = \lambda v \rightarrow (A - \lambda I) v = 0\)  <br><br>For this equation to have a non-trivial solution (i.e.,  \(v \neq 0\) ), the matrix \( A - \lambda I \) must be singular, which means:  \(\det(A - \lambda I) = 0\) <br><br><span style=""color:cyan"">Solving this equationgives us the Eigenvalues <br>\(\det(A - \lambda I) = 0\) </span><br><br><span style=""color:cyan"">We can find the corresponding Eigenvectors by Placing each Eigenvalue</span>"	
s{N:t7Nq@C	Basic	ML-Statistics	For A as an Event<br><br>\(A' = not A\)<br><br>\(P(A')\)	\(P(A') = 1 - P(A)\)	
o&Fn.<0R];	Basic	ML-Statistics	\(P(A \cup B))\)	\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)	
gh*)ZO.$t%	Basic	ML-Statistics	\(P(A \cap B)\)	"\(P(A \cap B) = P(A) + P(B) - P(A \cup B)\)<br><br><span style=""color:pink"">\(P(A \cap B) = P(B|A) \cdot P(A)\)</span><br><br><span style=""color:pink"">\(P(A \cap B) = P(A|B) \cdot P(B)\)</span>"	
"tp|c#m/nM6"	Basic	ML-Statistics	<h3>Bayes' Theorem</h3><br><br>\(P(A|B) \)	"\(P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad \text{for } P(B) > 0\)<br><hr><br>Bayes' Theorem<br><span style=""color:pink"">\(P(A \cap B) = P(B|A) \cdot P(A)\)</span><br><br><span style=""color:pink"">\(P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\)</span>"	
LvH!I:7O.l	Basic	ML-Statistics	Bayes' Theorem<br>	\(P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\)	
L7h6yY9MAJ	Basic	ML-Statistics	Bayes' Theorem Namings	"Bayes' Theorem<br>\(P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\)<br><hr><br>\(P(A|B) : \textbf{Posterior}\) <br><span style=""color:gold"">Is</span><br>\(P(B|A) : \textbf{Likelihood}\)<br><span style=""color:gold"">Times</span> <br>\(P(A) : \textbf{Prior}\)<br><span style=""color:gold"">On</span><br>\(P(B) : \textbf{Evidence}\)"	
erH:B_mXp0	Basic	ML-Statistics	<h2>Mathematical Expectation</h2><br><h3>Discrete</h3>	"\(\mathbb{E}[X] = \sum_{x} p(x) \cdot f(x)\)<br><br>\(p(x)\): PDF for \(f(x)\)<br>\(f(x)\): output Event for \(x\)<br><br><span style=""color:pink"">For \(f(x)=x\)<br>\(\mathbb{E}[X] = \sum_{x} x \cdot P(x)\)</span>"	
so|E2%0^GG	Basic	ML-Statistics	<h2>Mathematical Expectation</h2><br><h3>Continuous</h3>	"\(\mathbb{E}[X] = \int_{-\infty}^{\infty} p(x) \cdot f(x) \, dx \)<br><br>\(p(x)\): PMF for \(f(x)\)<br>\(f(x)\): output Event for \(x\)<br><br><span style=""color:pink"">For \(f(x)=x\)<br>\(\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot p(x) \, dx \)</span>"	
pZ8/O^^><c	Basic	ML-Statistics	<h2>Mathematical Expectation</h2><br><h3>Conditional</h3>	"<span style=""color:gold"">Not like Bayes</span><br><br>\(\mathbb{E}_x [f|y] = \sum_{x} p(x|y) \cdot f(x)\)<br><br>\(\mathbb{E}_x [f|y] = \int_{-\infty}^{\infty} p(x|y) \cdot f(x) \, dx\)"	
~&GuLKqb!	Basic	ML-Statistics	<h2>PDF for Uniform Distribution</h2> <br><h3>Discrete</h3>	"for \(a < x < b\)<br><br>\(P(x) = \frac{1}{b-a+1}, \quad x = 1, 2, ..., n\)<br><br><span style=""color:cyan"">\(P(x) = \frac{1}{n}, \quad x = 1, 2, ..., n\)</span>"	
"J#1+_*Mtl5"	Basic	ML-Statistics	<h2>PMF for Uniform Distribution</h2> <br><h3>Continuous</h3>	\(\text{for } a \leq x \leq b\)<br><br>\(P(x) = \frac{1}{b - a}\)	
NKphn,&&NO	Basic	ML-Statistics	<h2>Mathematical Expectation</h2><br><h3>Uniform Distribution - Discrete</h3>	"\(\mathbb{E}[X] = \frac{1}{n} \cdot \sum_{x}f(x)\)<br><br>\(p(x)\): PDF for \(f(x)\)<br>\(f(x)\): output Event for \(x\)<br><br><span style=""color:pink"">For \(f(x)=x\)<br>\(\mathbb{E}[X] = \frac{1}{n} \cdot \sum_{x} x\)</span>"	
"s#rxyYaWC2"	Basic	ML-Statistics	<h2>for \mathbb{E}[X] as Mathematical Expectation of X</h2><br><h3>\(\mathbb{E}[X+Y]\)</h3>	\(\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\)	
kB02KDhS:F	Basic	ML-Statistics	<h3>Mathematical Expectation for </h3><br>\(\mathbb{E}[XY]\)	"Discrete<br>\(\mathbb{E}[XY] = \sum_{x} \sum_{y} x y \cdot P(x,y) \quad \)<br><br>Continuous<br>\(\mathbb{E}[XY] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x y \cdot P(x, y) \, dx \, dy \quad\)<br><br>\(P(x,y)\): Joint probability<br><br><span style=""color:pink"">If X and Y are independent<br>\(p(X, Y) = p(X) \cdot p(Y)\)</span><br>\(\mathbb{E}[XY] = \mathbb{E}[X] \cdot \mathbb{E}[Y]\)<br><br>"	
t{Qa<_qCnb	Basic	ML-Statistics	<h2>for \mathbb{E}[X] as Mathematical Expectation of X and C as a Constant</h2><br><h3>\(\mathbb{E}[c + f(x)]\)</h3>	\(\mathbb{E}[c + f(x)] = c + \mathbb{E}[f(x)]\)	
i(ty9==|Ci	Basic	ML-Statistics	<h2>for \(\mathbb{E}[X]\) as Mathematical Expectation of X also c and b as Constants</h2><br><h3>\(\mathbb{E}[cX + b]\)</h3>	<h3>\(\mathbb{E}[cX + b] = c \mathbb{E}[X] + b\)</h3>	
iiDlMVxyNp	Basic	ML-Statistics	<h2>for \(\mathbb{E}[X]\) as Mathematical Expectation of X also c as a Constant</h2><br><h3>\(\mathbb{E}[c]\)</h3>	\(\mathbb{E}[c] = c\)	
EuJV8(B5uH	Basic	ML-Statistics	\(Var(x)\)	"\(Var(x) = \mathbb{E}[(X-\mu)^2]\)<br>\(= \mathbb{E}[X^2] - \mathbb{E}[X]^2\)<br><br>Matrix form:<br><span style=""color:pink"">\(Var(x) = \mathbb{E}[(X - \mathbb{E}[X])(X - \mathbb{E}[X])^T]\)</span>"	
"yTFe,=#nRl"	Basic	ML-Statistics	<h3>Conditional Mean (Expected Value) of Bivariate Normal Distribution</h3>	"If  \(\mathbf{X} = [x_1, x_2]^T\)  follows a bivariate normal distribution, then:<br>\(\mathbf{X} \sim \mathcal{N}\left( \mu, \Sigma \right)\),<br>\(\mu = [\mu_1, \mu_2]\),<br>\(\Sigma = \begin{bmatrix} \sigma_1^2 & \sigma \\ \sigma & \sigma_2^2 \end{bmatrix}\),<br><br>Then The conditional distribution  \(p(x_1 \mid x_2 = a)\)  is also normal. <br><br>\(\mathbb{E}[x_1 \mid x_2 = a] = \mu_1 + \frac{\sigma}{\sigma_2^2}(a - \mu_2)\) <br><br><span style=""color:cyan"">\(\mathbb{E}[x_2 \mid x_1 = a] = \mu_2 + \frac{\sigma}{\sigma_1^2}(a - \mu_1)\)</span>"	
tGr-c]i_s[	Basic	ML-Statistics	<h3>Conditional Variance of Bivariate Normal Distribution</h3>	"If  \(\mathbf{X} = [x_1, x_2]^T\)  follows a bivariate normal distribution, then:<br>\(\mathbf{X} \sim \mathcal{N}\left( \mu, \Sigma \right)\),<br>\(\mu = [\mu_1, \mu_2]\),<br>\(\Sigma = \begin{bmatrix} \sigma_1^2 & \sigma \\ \sigma & \sigma_2^2 \end{bmatrix}\),<br><br>Then The conditional distribution  \(p(x_1 \mid x_2 = a)\)  is also normal. <br><br>\(\text{Var}(x_1 \mid x_2 = a) = \sigma_1^2 - \frac{\sigma^2}{\sigma_2^2}\)<br><br><span style=""color:cyan"">\(\text{Var}(x_2 \mid x_1 = a) = \sigma_2^2 - \frac{\sigma^2}{\sigma_1^2}\)</span>"	
BIMS;5`wX8	Basic	ML-Statistics	<h3>Marginal probability of \(X\) in a Joint probability of \(P(X, Y)\)</h3>	"Discrete:<br>\(P(X = x) = \sum_{y} P(X = x, Y = y)\)<br><span style=""color:cyan"">\(P(Y = y) = \sum_{x} P(X = x, Y = y)\)</span><br><br>Continuous:<br>\(f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dy\)<br><span style=""color:cyan"">\(f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) \, dx\)</span>"	
iV_yL0Ky9^	Basic	ML-Statistics	<h3>Boundaries of Joint Probability</h3>	\(0 \leq P(X=x, Y=y) \leq 1\)	
h{,FsE}i.g	Basic	ML-Statistics	<h3>Sum of all the Joint Probabilities</h3>	\(\sum_x \sum_y P(X=x, Y=y) = 1\)	
ej=[N>WHVx	Basic	ML-Statistics	<h3>Conditional Probability From the joint probabilities</h3>	"\(P(X = x \mid Y = y) = \frac{P(X = x, Y = y)}{P(Y = y)}\)<br><br><span style=""color:cyan"">\(P(Y = y \mid X = x) = \frac{P(X = x, Y = y)}{P(X = x)}\)</span>"	
NCC5@?`0Me	Basic	ML-Statistics	<h3>Expected ValueFrom the joint distribution</h3>	"Discrete:<br>\(\mathbb{E}[X] = \sum_{x} \sum_{y} x \cdot P(X = x, Y = y)\)<br><span style=""color:cyan"">\(\mathbb{E}[Y] = \sum_{x} \sum_{y} y \cdot P(X = x, Y = y)\)</span><br><br>Continuous:<br>\(\mathbb{E}[X] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \cdot f_{X,Y}(x, y) \, dx \, dy\)<br><span style=""color:cyan"">\(\mathbb{E}[Y] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \cdot f_{X,Y}(x, y) \, dx \, dy\)</span><br><br><span style=""color:pink"">Apparently orders of summation and diffrentiation doesn't matter unless specific boundaries for y or x. Even in that case the answer will be the same</span>"	
"JOBAV<RRV#"	Basic	ML-Statistics	<h3>Covariance between  \(X\)  and  \(Y\)  in Joint probability</h3>	\(\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\)	
"svNoKRwTn#"	Basic	ML-Statistics	<h3>Variance in Joint probability</h3>	"\(\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\)<br><br>Calculations:<br><span style=""color:pink"">\(\mathbb{E}[X] = \sum_{x} \sum_{y} x \cdot P(X = x, Y = y)\)<br>\(\mathbb{E}[X^2] = \sum_{x} \sum_{y} x^2 \cdot P(X = x, Y = y)\)</span>"	
G@eD-a1z$R	Basic	ML-LinearRegression	<h3>Loss function for LR</h3>	Negative of the Maximum Likelihood	
F[1FB-8[;6	Basic	ML-LogisticRegression	<h3>Loss function for LgR</h3>	<br>Negative of the Maximum Likelihood	
"Gm%M%R#L|="	Basic	ML-Statistics	<h3>Covariance of X and Y</h3>	\(\text{Cov}(X,Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]\)	
wPAs/_$g_v	Basic	ML-Statistics	<h3>High Positive \(\text{Cov}(X,Y)\)</h3>	strong positive linear relationship.  	
Hv>=$TiGL8	Basic	ML-Statistics	<h3>\(\text{Cov}(X,Y) ≈ 0\)</h3>	no linear relationship. 	
HQlnnvk$L]	Basic	ML-Statistics	<h3>High Negative \(\text{Cov}(X,Y)\):</h3>	  strong negative linear relationship.	
i,L`7*(N^Z	Basic	ML-Statistics	<h3>Valid Covariance Matrix</h3>	"For \(A_{n*n}\):<br><br>1. Symmetric <br>(\(\sigma_{12}^2 = \sigma_{21}^2\))<br><br>2. Main Diameter None zero<br>(\(Var = \sigma^2\))<br><br>3. Positive Semi_Definite<br>(\(X^T\Sigma X \geq 0\))<br><br>4. \(\det(A) \neq Negative\)<br><span style=""color:pink"">Could be Zero and Irreversible</span><br><br>5. Cauchy-Schwarz inequality:<br>\(|Cov(i, j)| \leq \sigma_i \sigma_j \)<br><br>\(-1 \leq Corr(i, j) = \frac{Cov(i, j)}{\sigma_i \sigma_j} \leq 1\)<br>\(\sigma_i = \sqrt{Var}\)"	
j`J$:i6~~)	Basic	ML-Statistics	<h3>Correlation</h3>	"\(-1 \leq Corr(i, j) = \frac{Cov(i, j)}{\sigma_i \sigma_j} \leq 1\)<br><br><span style=""color:cyan"">Correlation is Covariance Normalized </span><br><br><span style=""color:pink"">\(Cov(i, j) = \sigma_{ij}\)</span><br><span style=""color:pink"">\(stdv : \sigma_i = \sqrt{\sigma_i^2}\)</span><br><span style=""color:pink"">\(Var =\sigma_i^2\)</span>"	
F)&wv+9D~	Basic	ML-Statistics	<h3>\(\text{Var}(\mathbf{x} + \mathbf{b})\)</h3>	"\(\text{Var}(\mathbf{x} + \mathbf{b}) = \text{Var}(\mathbf{x}) = \Sigma\)<br><br><span style=""color:pink"">\(\text{Cov}(\mathbf{x} + \mathbf{b}) = \text{Cov}(\mathbf{x}) = \Sigma\)</span>"	
KFXZ>Xmw8=	Basic	ML-Statistics	\(\text{Var}(\mathbf{A}\mathbf{x})\)	"For Covariance matrix<br>\(\text{Var}(\mathbf{x}) = \Sigma\)<br><br>\(\text{Var}(\mathbf{A}\mathbf{x}) = \mathbf{A} \Sigma \mathbf{A}^\top\)<br><br><span style=""color:pink"">\(\text{Cov}(\mathbf{A}\mathbf{x}) = \mathbf{A} \Sigma \mathbf{A}^\top\)</span><br><br><span style=""color:pink"">\(\text{Cov}(\mathbf{A}\mathbf{x}, \mathbf{A}\mathbf{x}) = \mathbf{A} \Sigma \mathbf{A}^\top\)</span>"	
e}Oczyj;jl	Basic	ML-Statistics	<h3>\(\text{Cov}(\mathbf{x} + \mathbf{a}, \mathbf{y} + \mathbf{b})\)</h3>	\( \text{Cov}(\mathbf{x} + \mathbf{a}, \mathbf{y} + \mathbf{b}) = \text{Cov}(\mathbf{x}, \mathbf{y})\)	
z&I*A>jzYC	Basic	ML-Statistics	<h3>\(\text{Cov}(\mathbf{A}\mathbf{x}, \mathbf{B}\mathbf{y}) \)</h3>	\(\text{Cov}(\mathbf{A}\mathbf{x}, \mathbf{B}\mathbf{y}) = \mathbf{A} \cdot \text{Cov}(\mathbf{x}, \mathbf{y}) \cdot \mathbf{B}^\top\)	
si?x5um!^D	Basic	ML-Statistics	<h3>\(\text{Var}(cX)\)</h3>	"Random variable X and Constant c<br><br>\(\text{Var}(cX) = c^2 \cdot \text{Var}(X)\)<br><br>\(\text{Cov}(cX) = c^2 \cdot \text{Cov}(X)\)<br><br><span style=""color:pink"">For The covariance Matrix is the same too</span>"	
t)K`gu3=CE	Basic	ML-Statistics	<h3>\(\text{Var}(aX + bY)\)</h3>	\(\text{Var}(aX + bY) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \cdot \text{Cov}(X, Y)\)	
xX|!@&2xZV	Basic	ML-Statistics	<h3>\(\text{Cov}(aX, bY)\)</h3>	\(\text{Cov}(aX, bY) = ab \cdot \text{Cov}(X, Y)\)	
N;Y~{7ob=[	Basic	ML-MLE&MAP	<h3>PMF for Binomial Distribution</h3>	The Binomial distribution models the number of successes in a fixed number of independent trials, where each trial has the same probability of success<br><br>\(P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}\)	
A*l)`xW!yA	Basic	ML-MLE&MAP	<h3>Likelihood Function for theBinomial distribution</h3>	"\(L(p; k, n) = \binom{n}{k} p^k (1 - p)^{n - k} \)<br><br><span style=""color:pink"">This is the same as the PMF, but now we treat \(p\) as the unknown variable and \(k\) ,  \(n\) as known</span>"	
"vS&#|)hAvV"	Basic	ML-MLE&MAP	Log-Likelihood Function for Binomial distribution	\(\ell(p; k, n) = \log L(p; k, n)  \)<br>=<br>\( \log \binom{n}{k} + k \log p + (n - k) \log (1 - p)\)	
y@:t6gVPJ+	Basic	ML-MLE&MAP	MLE for Binomial distribution	"PMF: <br>\(P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}\)<br><br>\(L(p) = \prod_{i=1}^{n} \binom{n}{x_i} p^{x_i} (1 - p)^{n - x_i}\)<br><br>\(\ell(p) = \sum_{i=1}^{n} \left[ \ln\binom{n}{x_i} + x_i \ln p + (n - x_i) \ln(1 - p) \right]\) <br><br>\(\frac{d\ell(p)}{dp} = \sum_{i=1}^{n} \left[ \frac{x_i}{p} - \frac{n - x_i}{1 - p} \right] = 0\) <br><br>\(\sum_{i=1}^{n} \left[ x_i (1 - p) - (n - x_i) p \right] = 0\)<br> <br><span style=""color:maroon"">\(\sum_{i=1}^{n} \left[ x_i - n p \right] = 0\)</span><br><br>\(\hat{p} = \frac{k}{n}\) "	
nP1W)rYtSm	Basic	DSA-Series&Growth	Order of \(2^{\frac{1}{n}}\)	For \(n \rightarrow \infty\)<br>\(2^{\frac{1}{n}} = 2^{\frac{1}{\infty}} = 2^{0} = 1\)<br><br>\(2^{\frac{1}{n}} = \Theta(1) \quad  \Theta(c)\)	
n-k`iJ`b$}	Basic	DSA-Series&Growth	Polynomials	"For \(A > 0\):<br><br>\(n^A\)<br><br><span style=""color:pink"">They grow to infinity</span>"	
RgCLwx(I<j	Basic	DSA-Series&Growth	\(\lim_{n \to \infty}(2^{-n})\)	\(2^{-n} = \frac{1}{2^n}\)<br>\(\lim_{n \to \infty}(2^{-n})\) = \(\lim_{n \to \infty}(\frac{1}{\infty})=0\)	
cpQOYwHdYH	Basic	DSA-Series&Growth	\(\lim_{n \to \infty}(n^{-2})\)	\(n^{-2}=\frac{1}{n^2}\)<br>\(\lim_{n \to \infty}(n^{-2})= \lim_{n \to \infty}(\frac{1}{\infty}) = 0\)	
r*,jgss%:A	Basic	DSA-Series&Growth	Exponentials	"For \(A > 1\)<br><br>\(A^n\)<br><br><span style=""color:cyan"">Exponentials grow to \(\infty\)</span><br><br><span style=""color:pink"">For \(|A| < 1\)<br>\(\lim_{n \to \infty}(A^n) = 0\)</span><br><br><span style=""color:pink"">For \(-A > 1\) for application in DSA we assume<br>\(\lim_{n \to \infty}(-A^n) = \Theta(A^n)\)</span>"	
z+!3dmW$Ht	Basic	DSA-Series&Growth	General Growth orders	"<span style=""color:lime"">\(\log(n) < n^A\)</span> <span style=""color:red"">\(< A^n < n!< n^n \)</span>"	
j:z>>%tSx:	Basic	DSA-Series&Growth	\(\lim_{n \to \infty}(n^{-\frac{1}{2}})\)	\(n^{-\frac{1}{2}} \) =\( \frac{1}{\sqrt{n}}\)<br><br>\(\lim_{n \to \infty}(n^{-\frac{1}{2}}) = \frac{1}{\infty} = 0\)	
G|7`M1@hr:	Basic	DSA-Series&Growth	The absorption rule	In polynomials only the biggest \(\Theta\) is considered. <br>Also the Constant Multiplier doesn't matter. 	
b,<<E|fA&;	Basic	DSA-Series&Growth	Growth of \(\log\) family	"\(\log^*(n) <\log(\log(n)) < \log(n) < \log^2(n) \)<br><br><span style=""color:pink"">The base doesn't change the Growth speed</span>"	
f4|v6~%8wn	Basic	DSA-Series&Growth	\( \text{Order of} (\log(n^2))\)	"\(\log(n^2) = 2\log(n)\)<br>\(\rightarrow\)<br> \(\Theta(\log(n))\)<br><br><br><span style=""color:pink"">\(\Theta{\log(n^2)} \text{ vs } \Theta{\log^2(n)}\)</span><br><span style=""color:pink"">\(\log^*(n) <\log(\log(n)) < \log(n) < \log^2(n) \)</span>"	
oy%^fK@H2(	Basic	DSA-Series&Growth	Sterling approximation of \(n!\)	"For \(n \to \infty\)<br>\(n! = (\frac{n}{e})^n(\sqrt{n})(\sqrt{2\pi})\)<br><br><span style=""color:pink"">\(\log(n!) = \Theta(n\log(n))\)</span><br><br><span style=""color:pink"">\(\sum_{i =1}^n \log(i) = \log(n!) = \Theta(n\log(n))\)</span>"	
j_3Y%J|.^<	Basic	DSA-Series&Growth	Growth for Factorials	\(n! < (n+1)!< (n+2)!< \dots\)	
n&&mf>:ZaK	Basic	DSA-Series&Growth	Taylor's estimation of \(e^x\)	\(e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!}\)<br><br>\( = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \cdots\)	
FZg=(Ca|2,	Basic	DSA-Series&Growth	<h3>L’Hospital’s Rule for Limits</h3>	\( \lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)} \)	
t$0g9}REnP	Basic	DSA-Series&Growth	\(\frac{d }{dx} \log(x)\)	\(\frac{d }{dx} \log(x) = \frac{1}{x}\)	
"nN~Vi0Oj2#"	Basic	DSA-Series&Growth	\( \lim_{x \to \infty} \frac{x^n}{e^x} \)	"\( \lim_{x \to \infty} \frac{x^n}{e^x} = 0\)<br><br><span style=""color:pink"">\(e^x\) stays the same, <br>L'Hopital until the Polinomial dies</span>"	
CnJBc9mJw&	Basic	DSA-Series&Growth	\( \lim_{x \to \infty} \frac{\sin(x)}{x} \)	\(-1 \leq \sin(x) \leq 1\)<br><br>\( \lim_{x \to \infty} \frac{\sin(x)}{x} = 0\)	
A;(0!%vWN>	Basic	DSA-Series&Growth	\(e^{\ln(x)}\)	\(e^{\ln(x)}= x^{ln(e)} = x\)	
l?6{43[K4)	Basic	DSA-Series&Growth	<h3>\(\lim_{x \to \infty} x^{1/x}\)</h3>	Indeterminate form of the type  \(\infty^0\)<br>\(y = x^{1/x} \quad, y = e^{\ln(y)}\)<br><br>\(\ln y = \ln\left(x^{1/x}\right) = \frac{1}{x} \ln x\) <br><br>\(\lim_{x \to \infty} \ln y = \frac{\ln x}{x} =\frac{\frac{1}{x}}{1} = 0\)<br><hr><br>\(\lim_{x \to \infty} y = e^0 = 1\)	
7JzPB7<gR	Basic	DSA-Series&Growth	<h3>\(\frac{d}{dx} \log(\log(x))\)</h3>	\(\frac{d}{dx} \log(\log(x)) = \frac{1}{x\log(x)}\)	
"Dv*s.3Z]d#"	Basic	DSA-Series&Growth	<h3>Growth of \(\log(f(x)!)\)</h3>	\(\log(f(x)!)\) = \(\Theta(f(x) \log(f(x))\)	
d%;]7^xa<Y	Basic	DSA-Series&Growth	<h3>Inequality rules for <br>\(0 < a < b \quad 0 < c < d\)</h3>	\( \begin{cases}a + c < b + d,& \text{True}\\ <br>a - c \not< b - d,&\text{Not necessarily}\\<br>ac < bd,&\text{True}\\<br>\frac{a}{c} \not< \frac{b}{d},&Not \text{necessarily}\end{cases}\)	
65(+DyvsY	Basic	DSA-Series&Growth	<h3>\( O(h) \) if<br>\(h(x) = \begin{cases} f(x),& O(f(x))\\ g(x),& O(g(x))\end{cases}\)</h3>	\( O(h) = \min\{O(f(x)), O(g(x))\}\)	
FinV$7JQD:	Basic	DSA-Series&Growth	<h3>\(n^{O(1)}\)</h3>	For \(c > 0\):<br>\(n^{O(1)} = O(n^c)\)	
oZQ]%~<LP0	Basic	DSA-Series&Growth	Master's Theorem	"for \(T(n) = A T(\frac{n}{b} + f(n))\)<br><br>\(p=\log_bA\)<br><br><span style=""color:lime"">We only compare the polynomial degree of \(n^p\) with \(f(n)\) :</span><br><br>\( \begin{cases}n^p > f(n) &\quad T(n)=\Theta(n^p) \\  n^p \leq f(n), & \quad T(n)=\Theta(f(n) \cdot \log(n)) \end{cases}\)<br><br><span style=""color:pink"">If \(f(n) \approx n^p \)  (same polinomial degree) but  \(n^p < f(n)\) we couldn't use the master's if (otherwise it's the second case) </span>"	
"pp#p({8N[@"	Basic	DSA-Series&Growth	<h3>Growth of \( \sum_{i=1}^{n} i^k\)</h3>	"\( \sum_{i=1}^{n} i^k \text{~} \frac{1}{k+1} n^{k+1} = \Theta(n^{k+1})\)<br><br><span style=""color:pink"">\( \sum_{i=1}^{n} i = \frac{n(n+1)}{2} = \Theta(n^2)\)</span><br><br><span style=""color:pink"">\( \sum_{i=1}^{n} i^2 = \frac{n(n+1)(2n+1)}{6} = \Theta(n^3)\)</span><br><br><span style=""color:pink"">\( \sum_{i=1}^{n} i^3 = (\frac{n(n+1)}{2})^2 = \Theta(n^4)\)</span>"	
ee%.ymM|TB	Basic	DSA-Series&Growth	<h3>Growth of \( \sum_{i=1}^{n} \frac{1}{i}\)</h3>	"\( \sum_{i=1}^{n} \frac{1}{i} = \ln(n) = \Theta(\log(n))\)<br><br><span style=""color:pink"">Whenever the Denominator of the elements increase by constant value c the same asymptotic rule applies </span><br><span style=""color:pink"">\(\frac{1}{3}+\frac{1}{5}+\frac{1}{7} + \dots = \Theta(\log(n))\)</span><br><br><span style=""color:pink"">Whenever the Denominator of the elements keeps increasing by bigger values each time, the series is converging to a constant c </span><br><span style=""color:pink"">\(\frac{1}{3}+\frac{1}{8}+\frac{1}{15} + \dots = \Theta(c)\)</span<br><br>"	
wz*nJt-s8&	Basic	DSA-Series&Growth	<h3>\(\Theta\)</h3>	\(=\)	
dkx<=$ngbv	Basic	DSA-Series&Growth	<h3>\(O\)</h3>	\(\leq\)	
gFP6tn/]ja	Basic	DSA-Series&Growth	<h3>\(o\) (small)</h3>	\(<\)	
yIfU^:dN~:	Basic	DSA-Series&Growth	<h3>\(\Omega\)</h3>	\(\geq\)	
A]oPPz)VH%	Basic	DSA-Series&Growth	<h3>\(\omega\)</h3>	\(>\)	
J?zkJ5+g)L	Basic	DSA-Series&Growth	<h3>\(\lim \frac{f(x)}{g(x)} = 0\)</h3>	\(\leftrightarrow\)<br>\(g(n) \in \omega(f(n))\)<br>\(f(n) \in o(g(n))\)	
ikOtYu1SnF	Basic	DSA-Series&Growth	<h3>\(\lim \frac{f(x)}{g(x)} = c \neq 0\)</h3>	\(\leftrightarrow\)<br>\(f(n) \in \Theta(g(n))\)<br>Or<br>\(g(n) \in \Theta(f(n))\)	
fOg8-ZVx}r	Basic	DSA-Series&Growth	<h3>\(\lim \frac{f(x)}{g(x)} = \infty\)</h3>	\(\leftrightarrow\)<br>\(f(n) \in \omega(g(n))\)<br>\(g(n) \in o(f(n))\)	
GAG,8K,zv]	Basic	DSA-Series&Growth	<h3>effect of log on \(f(n) < g(n)\)</h3>	"Could stay \(f(n) < g(n)\)<br><span style=""color:cyan"">\(f(n) \in o(g(n))\)</span><br><br>And maybe \(f(n) \leq g(n)\)<br><span style=""color:cyan"">\(f(n) \in O(g(n))\) or \(\Theta(g(n))\)</span>"	
r%Bq*A.-VW	Basic	DSA-heightOfRTs	<h3>Hight of recursive tree</h3><br>\(T(n) = A \cdot T(n - c) + f(n)\)	\(h = \frac{n}{c} = \Theta(n)\)	
yL`0`Io!dn	Basic	DSA-heightOfRTs	<h3>Hight of recursive tree</h3><br>\(B_{constant} > 1\)<br>\(T(n) = A \cdot T(\frac{n}{B}) + f(n)\)	"\(h = \log_B(n)\)<br><br><span style=""color:cyan"">For B as a Function of B(n)</span><br>\(T(n) = A \cdot T(\frac{n}{B(n)}) + f(n)\)<br><span style=""color:pink"">If \(B(n) \sim B(\frac{n}{B(n)})\)</span><br><br>\(h = \log_{B(n)}(n)\)"	
cFr^FX).Z<	Basic	DSA-heightOfRTs	<h3>Height of the recursive tree</h3><br>For  \(0 < A < 1\) <br>\(T(n) = A \cdot T(n^A) + f(n)\)	"\(h = \log_{\frac{1}{A}} (\log_2(n))\)<br><br><span style=""color:cyan"">examples</span><br>\(A = \frac{1}{2}:\)<br>\(T(n) = A \cdot T(\sqrt{n}) + f(n)\)<br>\(h = \log_2 (\log_2(n))\)<br><br>\(A = \frac{1}{3}:\)<br>\(T(n) = A \cdot T(\sqrt[3]{n}) + f(n)\)<br>\(h = \log_3 (\log_2(n)))\)<br><br>\(A = \frac{2}{3}:\)<br>\(T(n) = A \cdot T(\sqrt[\frac{2}{3}]{n}) + f(n)\)<br>\(h = \log_{\frac{3}{2}} (\log_2(n)))\)<br><hr><br><span style=""color:pink"">\(T(n) = A \cdot T(\frac{\sqrt{n}}{b}) + f(n)\)<br>the b has no effect on the height of the tree and the solution of it<br> \(h = \log_2 (\log_2(n))\)</span>"	
rLqOnT<])B	Basic	ML-MLE&MAP	PMF for Geometric Distribution	Number of trials until the first success.<br>last trial as success<br><br>\(P(X=k) = (1-p)^{k-1} p\)	
Ri`6_@7`j(	Basic	ML-MLE&MAP	PMF for Negative Binomial	Number of trials until  \(r\)  successes<br><br>\(P(X=k) = \binom{k-1}{r-1} p^r (1-p)^{k-r}\)	
J<@/{{dyXH	Basic	ML-MLE&MAP	PDF for Uniform distribution	Equal probability for all values<br><br>Discrete:<br>\(p(x) = \frac{1}{b-a+1}\)<br><br>Continuous:<br>\(p(x) = \frac{1}{b-a}\)	
w~<y4>kQ*^	Basic	ML-MLE&MAP	PMF for Hypergeometric distribution 	Sampling without replacement<br><br>\(P(X=k) = \frac{\binom{K}{k} \binom{N-K}{n-k}}{\binom{N}{n}}\)<br><br>\(N\): Total number of items in the population.<br>\(K\): Number of success states in the population. (wanted outcome population)<br>\(n\): Number of draws (sample size).<br>\(k\): Number of wanted observed successes.	
jA1Le9J%9:	Basic	ML-MLE&MAP	PDF for Gama distribution 	Modeling waiting times for random events (e.g., machine repair time) Time between events in a Poisson process Bayesian statistics and modeling positive data<br><br>\(f(x; k, \theta) = \frac{x^{k-1} e^{-x/\theta}}{\theta^k \Gamma(k)} \quad \text{for } x > 0\) <br><br>Where \(\Gamma(k)\)  is the Gamma function: <br>\(\Gamma(k) = \int_0^\infty x^{k-1} e^{-x} dx\)<br><br> \(k > 0\) : Shape  <br>\(\theta > 0\) : Scale<br>Or sometimes expressed as: <br>\(\alpha\) = k : Shape<br>\(\beta = 1/\theta\) : Rate<br><br>Mean (Expected Value) :<br> \(\mu = k\theta\)<br><br>Variance \(\sigma^2 = k\theta^2\) <br><br>Mode (Most Likely Value) \((k - 1)\theta\)  (for \(k \geq 1\) )	
LVF4$-5ySv	Basic	ML-MLE&MAP	for \(f(x) = c\), <br><br>\(f'\)	"\(f'(x) = 0 \)<br><br><br><span style=""color:pink"">\(f(x) = c \cdot g(x) \to f'(x) = c \cdot g'(x)\)</span>"	
e4+Z5N6%/v	Basic	ML-MLE&MAP	for \(f(x) = x^n\)<br><br>\(f'\)	\(f'(x) = n x^{n-1}\)	
"qv#-;<7bn("	Basic	ML-MLE&MAP	for \(f(x) = g(x) \pm h(x)\) <br><br>\(f'\)	\(f'(x) = g'(x) \pm h'(x)\)	
u~K$_IJqb3	Basic	ML-MLE&MAP	for \(f(x) = g(x) \cdot h(x)\)<br><br>\(f'\)	\(f'(x) = g'(x) \cdot h(x) + g(x) \cdot h'(x)\)	
Lola.R!?)7	Basic	ML-MLE&MAP	for \(f(x) = \frac{g(x)}{h(x)}\)<br><br>\(f'\)	\(f'(x) = \frac{g'(x) \cdot h(x) - g(x) \cdot h'(x)}{[h(x)]^2}\)	
DA`HqR|P*>	Basic	ML-MLE&MAP	for \(f(x)=\frac{1}{g(x)}\)<br><br>\(f'\)	\(f'(x)= - \frac{-g'(x)}{g^2(x)}\)	
j!$*l7nc~?	Basic	ML-MLE&MAP	Chain Rule for \(f(x) = g(h(x))\)<br><br>\(f'\)	\(f'(x) = h'(x) \cdot g'(h(x))\)	
vY,xVVR=iF	Basic	ML-MLE&MAP	Trigonometric Derivatives<br><br>simple	"<span style=""color:gold"">\(d\) clockwise, \(\int\) counter clockwise</span><br><br>\(\frac{d}{dx} \sin(x) = \cos(x)\)<br>\(\frac{d}{dx} \cos(x) = -\sin(x)\)"	
A+yI)LT<>0	Basic	ML-MLE&MAP	Trigonometric Derivatives<br><br>Arc versions	\(\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\sqrt{1-x^2}}\)<br><br>\(\frac{d}{dx} \cos^{-1}(x) = -\frac{1}{\sqrt{1-x^2}}\)<br><br>\(\frac{d}{dx} \tan^{-1}(x) = \frac{1}{\sqrt{1+x^2}}\)	
c[YyY*Mz<b	Basic	ML-MLE&MAP	Trigonometric Derivatives<br><br>sec & csc	\(\sec(x) = \frac{1}{\cos(x)}\)<br>\(\csc(x) = \frac{1}{\sin(x)}\)<br><br>\(\frac{d}{dx} \sec(x) = \sec(x)\tan(x)\)<br>\(\frac{d}{dx} \csc(x) = -\csc(x)\cot(x)\)	
GDE-eJ-<5F	Basic	ML-MLE&MAP	Trigonometric Derivatives<br><br>tan & cot	<br>\(\sec(x) = \frac{1}{\cos(x)}\)<br>\(\csc(x) = \frac{1}{\sin(x)}\)<br><br>\(\frac{d}{dx} \tan(x) = \sec^2(x)\)<br>\(\frac{d}{dx} \cot(x) = -\csc^2(x)\)	
bkNvjuI(7b	Basic	ML-MLE&MAP	for \(f(x) = ax^n\)<br><br>\(f'\)	"\(f'(x) = a (n) x^{n-1}\)<br><br><span style=""color:pink"">\(f(x) = a \sqrt{x} = a x^{-\frac{1}{2}}\) <br>\(\to \)<br>\(f'(x) = -a \cdot \frac{1}{2} \cdot \frac{1}{x^{-\frac{1}{2}}} =  \frac{-a}{2\sqrt{x}}\)</span>"	
OGb=,Y(7*[	Basic	ML-MLE&MAP	for \(f(x) = e^x\)<br><br>\(f'\)	\(f'(x) = e^x\)	
gSSf^8?eYT	Basic	ML-MLE&MAP	for \(f(x) = a^x\)<br><br>\(f'\)	\(f'(x) = \ln(x) a^x\)	
f?~{A5W*DZ	Basic	ML-MLE&MAP	for \(f(x) = \log_a(x)\)<br><br>\(f'\)	"\(f'(x) = \frac{1}{\ln(a)x}\)<br><br><span style=""color:pink"">\(f(x) = \ln(x) \to f'(x) = \frac{1}{x}\) </span>"	
TDpe!V&+I	Basic	ML-MLE&MAP	MLE for Uniform distribution	"(PDF): <br>\(f(x; a, b) = \begin{cases} \frac{1}{b - a}, & \text{if } a \leq x \leq b \\ 0, & \text{otherwise} \end{cases}\)<br><br>\(L(a, b) = \prod_{i=1}^{n} f(x_i; a, b) \)<br>\(L(a, b) =\left( \frac{1}{b - a} \right)^n \quad \text{if } a \leq x_i \leq b \text{ for all } i \)<br><br>\(\hat{a} = \min(x1, x2, \dots, x_n)\)\)<br>\(\hat{b} = \max(x1, x2, \dots, x_n)\)\) <br><br><span style=""color:pink"">One parameter uniform Distribution</span><br>\(f(x; \theta) = \begin{cases} \frac{1}{\theta}, & \text{if } 0 \leq x \leq \theta \\ 0, & \text{otherwise} \end{cases}\)<br><br>\(L(\theta) = \prod_{i=1}^{n} f(x_i; \theta) = \prod_{i=1}^{n} \frac{1}{\theta} = \left( \frac{1}{\theta} \right)^n\)<br><br>\(\hat{\theta} = \max(X_1, X_2, \dots, X_n)\)"	
tyl9j:kge3	Basic	DSA-heightOfRTs	<h3>Height of the recursive tree</h3><br>\(T(n) = A \cdot T(\log(n)) + f(n)\)	\(h = \log^*(n)\)	
