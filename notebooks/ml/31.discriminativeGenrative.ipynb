{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa8d630",
   "metadata": {},
   "source": [
    "## Bayes' Theorem Formula\n",
    "**Front:** Write Bayes' theorem in probability notation. <br/>\n",
    "**Back:** $$p(\\theta | x) = \\frac{p(x | \\theta) \\cdot p(\\theta)}{p(x)}$$\n",
    "\n",
    "* Posterior = Likelihood * prior / evidence\n",
    "* Ps(Posterior & Prior) like $\\theta$, likelihood dislikes $\\theta$ : $\\rightarrow \\begin{cases} \\text{prior} & p(\\theta) \\\\ \\text{posterior} & p(\\theta|x) \\\\ \\text{likelihood} & p(x|\\theta)\\end{cases} $\n",
    "\n",
    "## Prior Probability\n",
    "**Front:** What is $p(\\theta)$ called in Bayes' theorem? <br/>\n",
    "**Back:** The **prior probability** - our initial belief about $\\theta$ before seeing any data.\n",
    "\n",
    "* Ps(Posterior & Prior) like $\\theta$, likelihood dislikes $\\theta$ : $\\rightarrow \\begin{cases} \\text{prior} & p(\\theta) \\\\ \\text{posterior} & p(\\theta|x) \\\\ \\text{likelihood} & p(x|\\theta)\\end{cases} $\n",
    "\n",
    "## Probability Basics\n",
    "**Front:** What is $p(\\theta | x)$ called in Bayes' theorem? <br/>\n",
    "**Back:** The **posterior probability** - our updated belief about parameters $\\theta$ after seeing data $x$.\n",
    "\n",
    "* Ps(Posterior & Prior) like $\\theta$, likelihood dislikes $\\theta$ : $\\rightarrow \\begin{cases} \\text{prior} & p(\\theta) \\\\ \\text{posterior} & p(\\theta|x) \\\\ \\text{likelihood} & p(x|\\theta)\\end{cases} $\n",
    "\n",
    "## Likelihood\n",
    "**Front:** What is $p(x | \\theta)$ called in Bayes' theorem? <br/>\n",
    "**Back:** The **likelihood** - how probable the observed data $x$ is given parameters $\\theta$.\n",
    "\n",
    "* Ps(Posterior & Prior) like $\\theta$, likelihood dislikes $\\theta$ : $\\rightarrow \\begin{cases} \\text{prior} & p(\\theta) \\\\ \\text{posterior} & p(\\theta|x) \\\\ \\text{likelihood} & p(x|\\theta)\\end{cases} $\n",
    "\n",
    "## MLE (Maximum Likelihood Estimation)\n",
    "**Front:** What does MLE find? Write its formula. <br/>\n",
    "**Back:** The $\\theta$ that maximizes the likelihood: $$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} p(x | \\theta)$$\n",
    "It assumes $\\theta$ is fixed but unknown.\n",
    "\n",
    "## MAP (Maximum A Posteriori)\n",
    "**Front:** What does MAP find? Write its formula. <br/>\n",
    "**Back:** The $\\theta$ that maximizes the posterior: $$\\hat{\\theta}_{MAP} = \\arg\\max_{\\theta} p(\\theta | x) = \\arg\\max_{\\theta} p(x | \\theta)p(\\theta)$$\n",
    "It assumes $\\theta$ is random with a prior distribution.\n",
    "\n",
    "## MLE vs MAP Core Difference\n",
    "**Front:** What's the fundamental difference between MLE and MAP? <br/>\n",
    "**Back:** MLE uses only likelihood: $\\arg\\max p(x|\\theta)$. MAP adds prior: $\\arg\\max p(x|\\theta)p(\\theta)$. MAP is \"MLE with regularization\" from prior belief.\n",
    "\n",
    "* MAP multiplies $p(\\theta)$ (and p(x) is for normalization btw)so: $ \\begin{cases} p(x|\\theta)p(\\theta) & Posterior\\\\ p(\\theta|x) & prior\\end{cases} $ \n",
    "\n",
    "## Generative Models\n",
    "**Front:** What question do generative models answer? <br/>\n",
    "**Back:** \"What does each class look like?\" They model $p(x|y)$ - the probability of features given the class.\n",
    "\n",
    "* Generative models learn the data($x$), Discriminants focus on classification($y$) : $\\rightarrow \\begin{cases} \\text{Generative} & \\text{learns} \\quad p(\\text{data|class})=p(x|y)=p(x|\\theta) \\\\ \\text{Discriminative} & \\text{learns} \\quad p(\\text{class|data})=p(y|x)=p(\\theta|x)\\end{cases} $\n",
    "\n",
    "\n",
    "## Discriminative Models\n",
    "**Front:** What question do discriminative models answer? <br/>\n",
    "**Back:** \"How do we tell classes apart?\" They model $p(y|x)$ - the probability of class given the features.\n",
    "\n",
    "* Generative models learn the data($x$), Discriminants focus on classification($y$) : $\\rightarrow \\begin{cases} \\text{Generative} & \\text{learns} \\quad p(\\text{data|class})=p(x|y)=p(x|\\theta) \\\\ \\text{Discriminative} & \\text{learns} \\quad p(\\text{class|data})=p(y|x)=p(\\theta|x)\\end{cases} $\n",
    "\n",
    "## Generative Model Formula\n",
    "**Front:** What does a generative model compute for classification? <br/>\n",
    "**Back:**  by modeling $p(x|y)$ and $p(y)$, then using Bayes' theorem.\n",
    "\n",
    "$p(y|x) = \\frac{p(x|y) \\cdot p(y)}{p(x)}$\n",
    "\n",
    "* Generative models learn the data($x$), Discriminants focus on classification($y$) : $\\rightarrow \\begin{cases} \\text{Generative} & \\text{learns} \\quad p(\\text{data|class})=p(x|y)=p(x|\\theta) \\\\ \\text{Discriminative} & \\text{learns} \\quad p(\\text{class|data})=p(y|x)=p(\\theta|x)\\end{cases} $\n",
    "\n",
    "## Discriminative Model Formula\n",
    "**Front:** What does a discriminative model directly model? <br/>\n",
    "**Back:** It directly models $p(y|x)$, ignoring $p(x|y)$ and $p(x)$.\n",
    "\n",
    "* Generative models learn the data($x$), Discriminants focus on classification($y$) : $\\rightarrow \\begin{cases} \\text{Generative} & \\text{learns} \\quad p(\\text{data|class})=p(x|y)=p(x|\\theta) \\\\ \\text{Discriminative} & \\text{learns} \\quad p(\\text{class|data})=p(y|x)=p(\\theta|x)\\end{cases} $\n",
    "\n",
    "## Generative Model Example: Spam Detection\n",
    "**Front:** How would a generative model detect spam? <br/>\n",
    "**Back:** 1. Learn $p(\\text{words}|\\text{spam})$ and $p(\\text{words}|\\text{ham})$. 2. Learn $p(\\text{spam})$ and $p(\\text{ham})$. 3. Classify using Bayes: $p(\\text{spam}|\\text{words}) \\propto p(\\text{words}|\\text{spam})p(\\text{spam})$.\n",
    "\n",
    "## Discriminative Model Example: Spam Detection\n",
    "**Front:** How would a discriminative model detect spam? <br/>\n",
    "**Back:** Directly learn $p(\\text{spam}|\\text{words}) = \\sigma(\\theta^T \\cdot \\text{features}(\\text{words}))$ where $\\sigma$ is sigmoid. Ignore what spam/ham individually look like.\n",
    "\n",
    "## Generative Models Can Generate Data\n",
    "**Front:** What can generative models do that discriminative ones cannot? <br/>\n",
    "**Back:** Generate/sample new data points for each class because they learn $p(x|y)$.\n",
    "\n",
    "## Discriminative Models Focus on Boundaries\n",
    "**Front:** What do discriminative models focus on? <br/>\n",
    "**Back:** Only the decision boundary between classes, not the full data distribution.\n",
    "\n",
    "## Naive Bayes Classification\n",
    "**Front:** Is Naive Bayes generative or discriminative? Why? <br/>\n",
    "**Back:** **Generative**. It models $p(x|y)$ with the \"naive\" assumption that features are independent given the class: $p(x|y) = \\prod_i p(x_i|y)$.\n",
    "\n",
    "## Logistic Regression Classification\n",
    "**Front:** Is logistic regression generative or discriminative? Why? <br/>\n",
    "**Back:** **Discriminative**. It directly models $p(y|x) = \\sigma(\\theta^Tx)$ where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "## SVM Classification\n",
    "**Front:** Is SVM generative or discriminative? Why? <br/>\n",
    "**Back:** **Discriminative**. It finds the optimal separating hyperplane without modeling class distributions.\n",
    "\n",
    "## LDA (Linear Discriminant Analysis)\n",
    "**Front:** Is LDA generative or discriminative? Why? <br/>\n",
    "**Back:** **Generative**. It models each class as a Gaussian distribution with different means but same covariance matrix.\n",
    "\n",
    "## QDA (Quadratic Discriminant Analysis)\n",
    "**Front:** How is QDA different from LDA? <br/>\n",
    "**Back:** QDA is also **generative** but allows each class to have its own covariance matrix, giving quadratic decision boundaries.\n",
    "\n",
    "## Decision Trees Classification\n",
    "**Front:** Are decision trees generative or discriminative? <br/>\n",
    "**Back:** **Discriminative**. They build rules to separate classes without modeling $p(x|y)$.\n",
    "\n",
    "## Neural Networks Classification\n",
    "**Front:** Are standard neural networks for classification generative or discriminative? <br/>\n",
    "**Back:** **Discriminative**. They learn complex $p(y|x)$ through multiple layers of transformations.\n",
    "\n",
    "## k-Nearest Neighbors\n",
    "**Front:** Is k-NN generative or discriminative? <br/>\n",
    "**Back:** Neither strictly, but **discriminative in spirit**. It's non-parametric and classifies based on nearby examples without modeling distributions.\n",
    "\n",
    "## Bayesian Networks\n",
    "**Front:** Are Bayesian networks generative or discriminative? <br/>\n",
    "**Back:** **Generative**. They model the joint probability distribution $p(x,y)$ using a directed graph structure.\n",
    "\n",
    "## Hidden Markov Models\n",
    "**Front:** Are HMMs generative or discriminative? <br/>\n",
    "**Back:** **Generative**. They model sequences of observations as being generated by hidden states.\n",
    "\n",
    "## Linear Regression\n",
    "**Front:** Is linear regression generative or discriminative? <br/>\n",
    "**Back:** **Neither** in traditional sense. It's regression, not classification. But it models $p(y|x)$ with continuous $y$, making it discriminative-like.\n",
    "\n",
    "## PCA (Principal Component Analysis)\n",
    "**Front:** Is PCA generative or discriminative? <br/>\n",
    "**Back:** **Neither**. It's unsupervised dimensionality reduction with no concept of labels or classification boundaries.\n",
    "\n",
    "## Clustering Methods\n",
    "**Front:** Are clustering methods (k-means, hierarchical) generative or discriminative? <br/>\n",
    "**Back:** **Neither**. They're unsupervised. If clusters are treated as \"classes,\" the approach is generative-like as they model $p(x|\\text{cluster})$.\n",
    "\n",
    "## Parzen Window / Kernel Density Estimation\n",
    "**Front:** Can Parzen window methods be generative or discriminative? <br/>\n",
    "**Back:** **Generative** when used for density estimation. The Parzen window classifier estimates $p(x|y)$ for each class.\n",
    "\n",
    "## Regularization Connection to MAP\n",
    "**Front:** How is L2 regularization in logistic regression related to MAP? <br/>\n",
    "**Back:** Adding L2 regularization $\\lambda\\|\\theta\\|^2$ is equivalent to MAP estimation with a Gaussian prior on $\\theta$.\n",
    "\n",
    "## When to Use Generative Models\n",
    "**Front:** When should you prefer generative models? <br/>\n",
    "**Back:** 1. When you need to generate data. 2. With missing data. 3. Small datasets (priors help). 4. When $p(x|y)$ is naturally simple.\n",
    "\n",
    "## When to Use Discriminative Models\n",
    "**Front:** When should you prefer discriminative models? <br/>\n",
    "**Back:** 1. Pure classification task. 2. Large datasets. 3. When decision boundary is simpler than full distributions.\n",
    "\n",
    "## Common Pitfall: MAP vs MLE Assumptions\n",
    "**Front:** What's a common mistake when choosing between MAP and MLE? <br/>\n",
    "**Back:** Using MAP without justifying the prior, or using MLE when you actually have useful prior knowledge about parameters.\n",
    "\n",
    "## Common Pitfall: Naive Bayes Independence\n",
    "**Front:** What's the main limitation of Naive Bayes? <br/>\n",
    "**Back:** The \"naive\" assumption that features are independent given the class is often false in real data, though it still works surprisingly well.\n",
    "\n",
    "## Special Consideration: Neural Networks Can Be Both\n",
    "**Front:** Can neural networks be both generative and discriminative? <br/>\n",
    "**Back:** Yes! Standard NN classifiers are discriminative. But GANs and VAEs are generative networks that learn data distributions.\n",
    "\n",
    "## Special Consideration: Bayesian Methods Framework\n",
    "**Front:** How do Bayesian methods relate to generative/discriminative categories? <br/>\n",
    "**Back:** Bayesian methods provide a framework that's naturally generative, as they model full distributions with priors. But Bayesian logistic regression is discriminative with Bayesian inference on parameters.\n",
    "\n",
    "## Practical Tip: Testing Model Type\n",
    "**Front:** Quick test: is a model generative or discriminative? <br/>\n",
    "**Back:** Ask: \"Can it generate/sample new data points for each class?\" Yes → Generative. No → Discriminative.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
