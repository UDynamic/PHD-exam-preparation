{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c475676e",
   "metadata": {},
   "source": [
    "Goal of ML: \n",
    "\n",
    "$Error_{train} \\approx Error_{test} \\approx = 0 $\n",
    "\n",
    "Bias: the error of model detecting true patterns in the data. \n",
    "\n",
    "Variance: amount of change in the model's estimate after chainging the training dataset.\n",
    "\n",
    "---\n",
    "\n",
    "Overfitting(High Variance):\n",
    "\n",
    "model is too sensitive to the training data (=> low variance :the model is stable and not overly influenced by the training data)\n",
    "\n",
    "$Error_{train}: \\text{low} \\quad \\text{but} \\quad Error_{test}: \\text{high} $\n",
    "$Error_{train} \\ll Error_{test} $\n",
    "\n",
    "* Overly complex model.(=> complex hypothesis space.)\n",
    "* Too many features.\n",
    "* Too few training data. (=> use simple models for few data)\n",
    "* Too much noize in the data\n",
    "* \n",
    "---\n",
    "\n",
    "Underfitting (High Bias):\n",
    "Model is unable to detect true patterns in the data.(=> Low Bias: the model is complex enough to capture the patterns.)\n",
    "\n",
    "* $Error_{train} \\approx Error_{test} \\gg 0$\n",
    "* when both $Error_{train}$ and $Error_{test}$ is high.\n",
    "* Simpl model.(=> simple hypothesis space.)\n",
    "---\n",
    "Model complexity and Bias variance trade off:\n",
    "\n",
    "* when model is too simple. (Low Complexity → High Bias, Low Variance)\n",
    "* High Complexity → Low Bias, High Variance\n",
    "---\n",
    "Generalization Error: \n",
    "error a model makes on new, unseen data.\n",
    "\n",
    "$Generalization Error = (Bias)^2 + Variance + Irreducible Error$\n",
    "\n",
    "reducible errors: Bias and Variance. we can reduce them by improving the model.\n",
    "Irreducible Error: is due to noise in the data. we can't eliminate it.\n",
    "\n",
    "How to Reduce Generalization Error:\n",
    "1. Increase model complexity (if underfitting).\n",
    "2. Simplify model (if overfitting).\n",
    "3. Use more training data (reduces variance).\n",
    "4. Use regularization (e.g., L1/L2) to penalize overly complex models.\n",
    "5. Cross-validation to evaluate generalization performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Bias-Variance with Infinite Data\n",
    "**Front:** What happens to bias and variance as training data size approaches infinity? <br/>\n",
    "**Back:** **Variance approaches zero, bias remains unchanged.** With infinite data, the model sees all possible variations, making predictions stable (zero variance). However, bias—the model's fundamental inability to capture the true relationship—is determined by model capacity, not data quantity.\n",
    "\n",
    "## Learning Curve Convergence\n",
    "**Front:** What do learning curves show as training size increases toward infinity? <br/>\n",
    "**Back:** The gap between training and test error (variance) narrows to zero, and both converge to the same value—the bias of the model. If both errors remain high at convergence, the model has high bias (underfitting).\n",
    "\n",
    "## Model Selection with Infinite Data\n",
    "**Front:** What model should you choose if you have access to infinite data? <br/>\n",
    "**Back:** **The most complex model possible** (lowest bias). With infinite data, variance is zero, so the bias-variance tradeoff disappears. Complexity is free—choose the model class that can best approximate the true function.\n",
    "\n",
    "## Irreducible Error Persistence\n",
    "**Front:** What happens to irreducible error as training data approaches infinity? <br/>\n",
    "**Back:** **Irreducible error remains constant.** This is the inherent noise in the data generation process (e.g., measurement error, true stochasticity). No amount of data can eliminate it, setting a lower bound on possible error.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
