definition and distinction of different groups of classification methods including parametric vs none parametric and discreminative vs generative and Estimator's and calculators; including but not only these concepts:

1. regressions (linear and logistic) are parametric.
2. parametrics need training data for learning parameters to decide for the new data
3. none parametrics the training is not necessary and the training data is directly used.(almost all the work is done in test time)
4. K-NN is none parametric.
5. both supervised and unsupervised has parametric and none parametric methods
6. if the distribution is known but parameters are unknown we use MLE or MAP
7. Bayes classifier will give the optimal classification (p(\\theta) and p(x|\theta)
8. none parametrics are memory based or instance based
9. none parametrics memorize the training data and predicts \hat{y} = f(x|training data) wich is usually declared according to the similarity of test data x to the training data [elaborate on this one.
10. the best use scenario for none parametric: modeling parametric is hard or the distribution is unknown



focusing on main subject of K-NN and Parzen window:

1. definition of the problem and formulation and parameters : p_n(x) = \frac{k}{nv} & v = (h^d)
2. definition of the window around the samples
3. calculation of v and k:
   1. fixed window (Parzen window): v_n = \frac{1}{\sqert{n}}
   2. fixed number of samples (K-NN): k-n = \sqert{n}
4. the best use scenario for none parametric: modeling parametric is hard or the distribution is unknown
5. this methods deliver flexible none parametric way of estimating the distributy functions
6. these are specially effective when dealing with none conventional or very complex distributions for data
7. these are example based learnings.
8. these are Instance based or memory based?
9. these are lazy. what does it mean?
10.
