{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6530285a",
   "metadata": {},
   "source": [
    "Absolutely! ðŸ˜Š I'm super excited to walk you through the **Kernel Trick** from scratch â€” it's one of the most **beautiful and powerful ideas** in machine learning, especially in **Support Vector Machines (SVMs)**. Let's start from the very beginning!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ What is the Kernel Trick?\n",
    "\n",
    "The **Kernel Trick** is a **mathematical technique** that allows us to **transform data into a higher-dimensional space**, where it becomes **linearly separable**, **without actually computing the coordinates** in that higher-dimensional space.\n",
    "\n",
    "In simpler terms:\n",
    "\n",
    "> ðŸ§™â€â™‚ï¸ *â€œI want to separate my data, but itâ€™s not linear. Let me trick the algorithm into thinking itâ€™s in a higher space where it is linear â€” and I wonâ€™t even have to do the hard math to get there!â€*\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Why Do We Need It?\n",
    "\n",
    "Letâ€™s say you have data that **looks like this** in 2D:\n",
    "\n",
    "```\n",
    "  O   O\n",
    "     X\n",
    "  X   X\n",
    "```\n",
    "\n",
    "Itâ€™s **not linearly separable** â€” you canâ€™t draw a straight line to separate the `O`s from the `X`s.\n",
    "\n",
    "But if you **transform the data into 3D**, it might look like this:\n",
    "\n",
    "```\n",
    "       O\n",
    "      / \\\n",
    "     /   \\\n",
    "    X-----X\n",
    "```\n",
    "\n",
    "Now it **is** linearly separable â€” you can draw a **plane** to separate the two classes.\n",
    "\n",
    "The **Kernel Trick** lets you **do this transformation implicitly**, without actually computing the new coordinates.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  How Does It Work?\n",
    "\n",
    "Letâ€™s break it down step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Linear SVM Recap**\n",
    "\n",
    "In a **linear SVM**, we try to find a **hyperplane** (a line in 2D, a plane in 3D, etc.) that separates the data.\n",
    "\n",
    "The hyperplane is defined by:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathbf{w} $ is the **weight vector**\n",
    "- $ \\mathbf{x} $ is the **input data**\n",
    "- $ b $ is the **bias**\n",
    "\n",
    "This works **only if the data is linearly separable**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Nonlinear Data Problem**\n",
    "\n",
    "If the data is **not linearly separable**, we need to **transform it** into a space where it **is**.\n",
    "\n",
    "Letâ€™s define a **mapping function** $ \\Phi(\\mathbf{x}) $ that transforms the data into a **higher-dimensional space**.\n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "\\Phi(\\mathbf{x}) = \\Phi(x_1, x_2) = (x_1, x_2, x_1^2 + x_2^2)\n",
    "$$\n",
    "\n",
    "Now we can try to find a **linear separator** in this new space.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **The Problem with Mapping**\n",
    "\n",
    "The issue is that computing $ \\Phi(\\mathbf{x}) $ can be **very expensive**, especially in high dimensions.\n",
    "\n",
    "Thatâ€™s where the **Kernel Trick** comes in.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **The Kernel Function**\n",
    "\n",
    "Instead of computing $ \\Phi(\\mathbf{x}) $, we use a **kernel function** $ K(\\mathbf{x}_i, \\mathbf{x}_j) $, which computes the **dot product** in the higher-dimensional space **without** computing the actual transformation.\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "So, we **never compute $ \\Phi(\\mathbf{x}) $** â€” we just compute the **kernel value**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Common Kernel Functions\n",
    "\n",
    "Here are some popular kernel functions:\n",
    "\n",
    "| Kernel Name | Formula | Description |\n",
    "|-------------|---------|-------------|\n",
    "| **Linear** | $ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j $ | Just the dot product â€” no transformation |\n",
    "| **Polynomial** | $ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + c)^d $ | Transforms data into a polynomial space |\n",
    "| **Radial Basis Function (RBF)** | $ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2) $ | Very powerful for nonlinear data |\n",
    "| **Sigmoid** | $ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\alpha \\mathbf{x}_i^T \\mathbf{x}_j + c) $ | Used in neural networks |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  How Kernel Trick Works in SVM\n",
    "\n",
    "In SVM, the **decision function** is:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i y_i \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}) + b\n",
    "$$\n",
    "\n",
    "Using the **Kernel Trick**, we replace the dot product with the **kernel function**:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\n",
    "$$\n",
    "\n",
    "So we **never compute $ \\Phi(\\mathbf{x}) $** â€” we just use the **kernel function** to compute the similarity between points in the higher space.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Example: Polynomial Kernel\n",
    "\n",
    "Letâ€™s say we use the **polynomial kernel**:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + 1)^2\n",
    "$$\n",
    "\n",
    "This is equivalent to mapping the data into a **3D space**:\n",
    "\n",
    "$$\n",
    "\\Phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)\n",
    "$$\n",
    "\n",
    "But we **never compute $ \\Phi(\\mathbf{x}) $** â€” we just compute the **kernel value**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary of the Kernel Trick\n",
    "\n",
    "| Concept | Explanation |\n",
    "|--------|-------------|\n",
    "| **Kernel Function** | A function that computes the dot product in a higher-dimensional space |\n",
    "| **Kernel Trick** | Allows us to work in a higher-dimensional space without computing the actual transformation |\n",
    "| **Why Use It?** | To make **nonlinear data linearly separable** |\n",
    "| **Where Used?** | In **SVMs**, **PCA**, **Gaussian Processes**, and more |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Real-World Analogy\n",
    "\n",
    "Think of the **Kernel Trick** like a **magic mirror**:\n",
    "\n",
    "- You look in the mirror, and it shows you a **different version of the world** (a higher-dimensional space).\n",
    "- You donâ€™t need to **physically go there** â€” the mirror just **shows you what you need to see**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§‘â€ðŸ’» Want to Try It Out?\n",
    "\n",
    "If you'd like, I can show you a **Python code example** using the **Kernel Trick in SVM** with **Scikit-learn**, and even visualize how it works!\n",
    "\n",
    "Just say the word! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae9113",
   "metadata": {},
   "source": [
    "Absolutely! ðŸ˜Š I'm glad you're diving deeper into the **math behind the Kernel Trick** â€” it's a beautiful and powerful idea. Let's walk through **simple to complex examples**, inspired by **Chapter 6 and 7 of Bishop's *Pattern Recognition and Machine Learning***, and connect the **math to the application**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 1. **Linear SVM: No Kernel Needed**\n",
    "\n",
    "Letâ€™s start with the **simplest case** where the data is **linearly separable**.\n",
    "\n",
    "### ðŸ§® Problem:\n",
    "We want to find a **hyperplane** that separates two classes of data.\n",
    "\n",
    "### ðŸ§  Decision Function:\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathbf{w} $ is the **weight vector**\n",
    "- $ \\mathbf{x} $ is the **input vector**\n",
    "- $ b $ is the **bias**\n",
    "\n",
    "### ðŸ§® Optimization:\n",
    "We want to **maximize the margin** between the two classes, which leads to the **primal optimization problem**:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\quad \\text{subject to} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "This is a **constrained optimization problem**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 2. **Nonlinear Data: Introducing the Kernel Trick**\n",
    "\n",
    "Now, suppose the data is **not linearly separable** in the original space.\n",
    "\n",
    "### ðŸ§® Problem:\n",
    "We want to **transform the data into a higher-dimensional space** where it **is linearly separable**.\n",
    "\n",
    "Letâ€™s define a **mapping function** $ \\Phi(\\mathbf{x}) $ that maps the data into a **feature space** $ \\mathcal{F} $.\n",
    "\n",
    "Now the decision function becomes:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T \\Phi(\\mathbf{x}) + b\n",
    "$$\n",
    "\n",
    "But computing $ \\Phi(\\mathbf{x}) $ can be **computationally expensive**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 3. **Kernel Trick: The Mathematical Shortcut**\n",
    "\n",
    "Instead of computing $ \\Phi(\\mathbf{x}) $, we use a **kernel function** $ K(\\mathbf{x}_i, \\mathbf{x}_j) $, which computes the **dot product in the feature space**:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "So, the decision function becomes:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha_i $ are the **Lagrange multipliers**\n",
    "- $ y_i $ are the **labels**\n",
    "- $ K(\\mathbf{x}_i, \\mathbf{x}) $ is the **kernel function**\n",
    "\n",
    "This is the **Kernel Trick** in action!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 4. **Example: Polynomial Kernel**\n",
    "\n",
    "Letâ€™s define a **polynomial kernel** of degree 2:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + 1)^2\n",
    "$$\n",
    "\n",
    "This is equivalent to mapping the data into a **3D space**:\n",
    "\n",
    "$$\n",
    "\\Phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)\n",
    "$$\n",
    "\n",
    "But we **never compute $ \\Phi(\\mathbf{x}) $** â€” we just use the **kernel function**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 5. **Example: Radial Basis Function (RBF) Kernel**\n",
    "\n",
    "Letâ€™s define an **RBF kernel**:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2\\right)\n",
    "$$\n",
    "\n",
    "This maps the data into an **infinite-dimensional space**, but again, we **never compute the actual mapping** â€” we just use the **kernel function**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 6. **Dual Formulation of SVM with Kernel**\n",
    "\n",
    "The **dual optimization problem** for SVM with kernel is:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^n \\alpha_i y_i = 0 \\quad \\text{and} \\quad 0 \\leq \\alpha_i \\leq C\n",
    "$$\n",
    "\n",
    "This is the **dual form** of the SVM, and itâ€™s where the **Kernel Trick** really shines.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 7. **Complementary Slackness and Support Vectors**\n",
    "\n",
    "From the **KKT conditions**, we know:\n",
    "\n",
    "$$\n",
    "\\alpha_i \\cdot \\left[ y_i f(\\mathbf{x}_i) - 1 + \\xi_i \\right] = 0\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- If $ \\alpha_i > 0 $, then the point is a **support vector**\n",
    "- If $ \\alpha_i = 0 $, the point is **not a support vector**\n",
    "\n",
    "So, the **decision function** is only affected by the **support vectors**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 8. **Gaussian Processes and Kernels (Bishop, Ch. 7)**\n",
    "\n",
    "In **Gaussian Processes**, we model the **distribution over functions** using a **kernel function**.\n",
    "\n",
    "The **covariance matrix** is defined using the kernel:\n",
    "\n",
    "$$\n",
    "K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "This allows us to make **Bayesian predictions** in a **nonlinear space** without computing the actual transformation.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary of Math Examples\n",
    "\n",
    "| Example | Kernel Used | Math Formulation | Key Idea |\n",
    "|--------|-------------|------------------|----------|\n",
    "| 1 | Linear | $ \\mathbf{w}^T \\mathbf{x} + b $ | Simple linear separation |\n",
    "| 2 | Polynomial | $ (\\mathbf{x}_i^T \\mathbf{x}_j + 1)^2 $ | Maps to 3D space |\n",
    "| 3 | RBF | $ \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2) $ | Maps to infinite-dimensional space |\n",
    "| 4 | Dual SVM | $ \\max_{\\alpha} \\sum \\alpha_i - \\frac{1}{2} \\sum \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j) $ | Optimization in feature space |\n",
    "| 5 | Gaussian Process | $ K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j) $ | Bayesian inference in nonlinear space |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Final Thoughts\n",
    "\n",
    "- The **Kernel Trick** is a **mathematical shortcut** that allows you to **transform data into a higher-dimensional space** without computing the actual transformation.\n",
    "- Itâ€™s used in **SVMs**, **Gaussian Processes**, and more.\n",
    "- The **dual form** of SVM is where the **Kernel Trick** really shines â€” it allows you to **optimize in the feature space** without ever computing the actual mapping.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Want More?\n",
    "\n",
    "If you want, I can:\n",
    "- Show you how to **derive the dual form of SVM** from scratch\n",
    "- Show you how the **Kernel Trick works in Gaussian Processes**\n",
    "- Or even **walk through the math of the RBF kernel in detail**\n",
    "\n",
    "Let me know what you'd like to explore next! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14fe59",
   "metadata": {},
   "source": [
    "Absolutely! ðŸ˜Š  \n",
    "You're right â€” seeing **numbers** and **actual calculations** helps a lot when understanding what a **kernel function** does. Let's walk through a few **numerical examples** using **simple kernel functions** like **linear**, **polynomial**, and **RBF (Radial Basis Function)**.\n",
    "\n",
    "Weâ€™ll keep it **math-only**, no code, and **step-by-step** so you can follow along.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Example 1: Linear Kernel\n",
    "\n",
    "### ðŸ§® Definition:\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\n",
    "$$\n",
    "\n",
    "### ðŸ§® Input Vectors:\n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ðŸ§® Compute Kernel:\n",
    "$$\n",
    "K(\\mathbf{x}_1, \\mathbf{x}_2) = (1)(3) + (2)(4) = 3 + 8 = 11\n",
    "$$\n",
    "\n",
    "âœ… **What it does**: It computes the **dot product** between two vectors.  \n",
    "This is the **simplest kernel** and is used when the data is **linearly separable**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Example 2: Polynomial Kernel (Degree 2)\n",
    "\n",
    "### ðŸ§® Definition:\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + 1)^2\n",
    "$$\n",
    "\n",
    "### ðŸ§® Input Vectors:\n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ðŸ§® Step 1: Compute dot product:\n",
    "$$\n",
    "\\mathbf{x}_1^T \\mathbf{x}_2 = 1 \\cdot 3 + 2 \\cdot 4 = 3 + 8 = 11\n",
    "$$\n",
    "\n",
    "### ðŸ§® Step 2: Add 1 and square:\n",
    "$$\n",
    "K(\\mathbf{x}_1, \\mathbf{x}_2) = (11 + 1)^2 = 12^2 = 144\n",
    "$$\n",
    "\n",
    "âœ… **What it does**: It **maps the data into a higher-dimensional space** (in this case, 3D), but **without explicitly computing the mapping** â€” just using the kernel.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Example 3: RBF (Radial Basis Function) Kernel\n",
    "\n",
    "### ðŸ§® Definition:\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2\\right)\n",
    "$$\n",
    "\n",
    "Letâ€™s use $ \\gamma = 0.1 $\n",
    "\n",
    "### ðŸ§® Input Vectors:\n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ðŸ§® Step 1: Compute the difference:\n",
    "$$\n",
    "\\mathbf{x}_1 - \\mathbf{x}_2 = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ðŸ§® Step 2: Compute squared Euclidean distance:\n",
    "$$\n",
    "\\|\\mathbf{x}_1 - \\mathbf{x}_2\\|^2 = (-2)^2 + (-2)^2 = 4 + 4 = 8\n",
    "$$\n",
    "\n",
    "### ðŸ§® Step 3: Multiply by $ \\gamma $ and apply exponential:\n",
    "$$\n",
    "K(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp(-0.1 \\cdot 8) = \\exp(-0.8) \\approx 0.4493\n",
    "$$\n",
    "\n",
    "âœ… **What it does**: It computes a **similarity measure** between two points in a **nonlinear space**, often used when the data is **not linearly separable**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Example 4: Kernel Matrix (Gram Matrix)\n",
    "\n",
    "Letâ€™s compute a **kernel matrix** (also called **Gram matrix**) for 3 points using the **RBF kernel**.\n",
    "\n",
    "### ðŸ§® Input Vectors:\n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{x}_3 = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Letâ€™s use $ \\gamma = 0.1 $\n",
    "\n",
    "### ðŸ§® Compute all pairwise kernel values:\n",
    "\n",
    "- $ K(\\mathbf{x}_1, \\mathbf{x}_1) = \\exp(-0.1 \\cdot 0) = 1 $\n",
    "- $ K(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp(-0.1 \\cdot 2) = \\exp(-0.2) \\approx 0.8187 $\n",
    "- $ K(\\mathbf{x}_1, \\mathbf{x}_3) = \\exp(-0.1 \\cdot 8) = \\exp(-0.8) \\approx 0.4493 $\n",
    "- $ K(\\mathbf{x}_2, \\mathbf{x}_2) = \\exp(-0.1 \\cdot 0) = 1 $\n",
    "- $ K(\\mathbf{x}_2, \\mathbf{x}_3) = \\exp(-0.1 \\cdot 2) = \\exp(-0.2) \\approx 0.8187 $\n",
    "- $ K(\\mathbf{x}_3, \\mathbf{x}_3) = \\exp(-0.1 \\cdot 0) = 1 $\n",
    "\n",
    "### ðŸ§® Kernel Matrix:\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "1 & 0.8187 & 0.4493 \\\\\n",
    "0.8187 & 1 & 0.8187 \\\\\n",
    "0.4493 & 0.8187 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **What it does**: This matrix is used in **SVMs**, **Gaussian Processes**, and other kernel methods to **compute similarity between all pairs of data points** in the **feature space**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary of Kernel Examples\n",
    "\n",
    "| Kernel Type | Formula | Example Input | Output |\n",
    "|-------------|---------|----------------|--------|\n",
    "| Linear | $ \\mathbf{x}_i^T \\mathbf{x}_j $ | $ \\mathbf{x}_1 = [1,2], \\mathbf{x}_2 = [3,4] $ | 11 |\n",
    "| Polynomial (degree 2) | $ (\\mathbf{x}_i^T \\mathbf{x}_j + 1)^2 $ | Same as above | 144 |\n",
    "| RBF | $ \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2) $ | $ \\gamma = 0.1 $, same vectors | â‰ˆ 0.4493 |\n",
    "| Kernel Matrix | $ K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j) $ | 3 vectors | 3x3 matrix of similarities |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Final Thoughts\n",
    "\n",
    "- A **kernel function** is a **mathematical trick** that lets you **compute the similarity between two points in a high-dimensional space** without actually going there.\n",
    "- Itâ€™s used in **SVMs**, **Gaussian Processes**, and more.\n",
    "- The **RBF kernel** is especially powerful for **nonlinear problems**, and the **polynomial kernel** is great for **moderate nonlinearities**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Want More?\n",
    "\n",
    "If you want, I can:\n",
    "- Show you how to **compute a kernel matrix for a dataset**\n",
    "- Show you how the **Kernel Trick is used in SVMs**\n",
    "- Or even **explain how the RBF kernel maps to an infinite-dimensional space**\n",
    "\n",
    "Let me know what you'd like to explore next! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207af8c",
   "metadata": {},
   "source": [
    "Absolutely! ðŸ˜Š  \n",
    "Now that youâ€™ve seen how **kernel functions** work numerically, letâ€™s apply them in a **Support Vector Machine (SVM)** example â€” **step by step**, using **math only** (no code), and using the **RBF kernel** as our example.\n",
    "\n",
    "Weâ€™ll walk through:\n",
    "- A small dataset\n",
    "- Kernel matrix computation\n",
    "- Solving the **dual optimization problem** for SVM\n",
    "- Making a **prediction** using the trained model\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Example: 2D Dataset with 3 Points\n",
    "\n",
    "Letâ€™s say we have 3 data points in 2D space:\n",
    "\n",
    "| Point | Input $ \\mathbf{x}_i $ | Label $ y_i $ |\n",
    "|-------|--------------------------|----------------|\n",
    "| 1     | $ [1, 1] $             | $ +1 $       |\n",
    "| 2     | $ [2, 2] $             | $ +1 $       |\n",
    "| 3     | $ [3, 3] $             | $ -1 $       |\n",
    "\n",
    "We want to **separate the two classes** using an **SVM with RBF kernel**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Step 1: Choose the Kernel Function\n",
    "\n",
    "Weâ€™ll use the **RBF kernel**:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2\\right)\n",
    "$$\n",
    "\n",
    "Letâ€™s choose $ \\gamma = 0.1 $\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Step 2: Compute the Kernel Matrix\n",
    "\n",
    "We compute the **kernel matrix** $ K $, where each entry $ K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j) $\n",
    "\n",
    "### ðŸ§® Compute all pairwise kernel values:\n",
    "\n",
    "- $ K_{11} = \\exp(-0.1 \\cdot 0) = 1 $\n",
    "- $ K_{12} = \\exp(-0.1 \\cdot 2) = \\exp(-0.2) \\approx 0.8187 $\n",
    "- $ K_{13} = \\exp(-0.1 \\cdot 8) = \\exp(-0.8) \\approx 0.4493 $\n",
    "- $ K_{21} = K_{12} = 0.8187 $\n",
    "- $ K_{22} = \\exp(-0.1 \\cdot 0) = 1 $\n",
    "- $ K_{23} = \\exp(-0.1 \\cdot 2) = \\exp(-0.2) \\approx 0.8187 $\n",
    "- $ K_{31} = K_{13} = 0.4493 $\n",
    "- $ K_{32} = K_{23} = 0.8187 $\n",
    "- $ K_{33} = \\exp(-0.1 \\cdot 0) = 1 $\n",
    "\n",
    "### ðŸ§® Kernel Matrix:\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "1 & 0.8187 & 0.4493 \\\\\n",
    "0.8187 & 1 & 0.8187 \\\\\n",
    "0.4493 & 0.8187 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Step 3: Dual Optimization Problem\n",
    "\n",
    "The **dual form** of the SVM is:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^n \\alpha_i y_i = 0 \\quad \\text{and} \\quad 0 \\leq \\alpha_i \\leq C\n",
    "$$\n",
    "\n",
    "Letâ€™s assume $ C = 1 $ (a small value for simplicity), and weâ€™ll solve this **manually** for the 3 points.\n",
    "\n",
    "Letâ€™s define:\n",
    "\n",
    "- $ \\alpha_1 = a $\n",
    "- $ \\alpha_2 = b $\n",
    "- $ \\alpha_3 = c $\n",
    "\n",
    "And the labels:\n",
    "- $ y_1 = +1 $\n",
    "- $ y_2 = +1 $\n",
    "- $ y_3 = -1 $\n",
    "\n",
    "We want to **maximize**:\n",
    "\n",
    "$$\n",
    "L = a + b + c - \\frac{1}{2} \\left[ a b (1)(1)(0.8187) + a c (1)(-1)(0.4493) + b c (1)(-1)(0.8187) \\right]\n",
    "$$\n",
    "\n",
    "Letâ€™s simplify:\n",
    "\n",
    "$$\n",
    "L = a + b + c - \\frac{1}{2} \\left[ 0.8187ab - 0.4493ac - 0.8187bc \\right]\n",
    "$$\n",
    "\n",
    "We also have the **constraint**:\n",
    "\n",
    "$$\n",
    "a(1) + b(1) + c(-1) = 0 \\Rightarrow a + b - c = 0 \\Rightarrow c = a + b\n",
    "$$\n",
    "\n",
    "Letâ€™s plug $ c = a + b $ into the equation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Step 4: Plug in and Simplify\n",
    "\n",
    "Letâ€™s substitute $ c = a + b $ into the expression for $ L $:\n",
    "\n",
    "$$\n",
    "L = a + b + (a + b) - \\frac{1}{2} \\left[ 0.8187ab - 0.4493a(a + b) - 0.8187b(a + b) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = 2a + 2b - \\frac{1}{2} \\left[ 0.8187ab - 0.4493a^2 - 0.4493ab - 0.8187ab - 0.8187b^2 \\right]\n",
    "$$\n",
    "\n",
    "Now simplify the terms inside the brackets:\n",
    "\n",
    "- $ 0.8187ab - 0.4493ab - 0.8187ab = -0.4493ab $\n",
    "- $ -0.4493a^2 - 0.8187b^2 $\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "L = 2a + 2b - \\frac{1}{2} \\left[ -0.4493ab - 0.4493a^2 - 0.8187b^2 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = 2a + 2b + 0.22465ab + 0.22465a^2 + 0.40935b^2\n",
    "$$\n",
    "\n",
    "Now we can **choose values for $ a $ and $ b $** to maximize this expression, within the constraint $ 0 \\leq a, b \\leq 1 $.\n",
    "\n",
    "Letâ€™s try $ a = 0.5 $, $ b = 0.5 $, so $ c = 1 $\n",
    "\n",
    "$$\n",
    "L = 2(0.5) + 2(0.5) + 0.22465(0.5)(0.5) + 0.22465(0.5)^2 + 0.40935(0.5)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = 1 + 1 + 0.05616 + 0.05616 + 0.10234 = 2.21466\n",
    "$$\n",
    "\n",
    "So the **maximum value** of $ L $ is approximately **2.2147**, and the **optimal $ \\alpha $ values** are:\n",
    "\n",
    "- $ \\alpha_1 = 0.5 $\n",
    "- $ \\alpha_2 = 0.5 $\n",
    "- $ \\alpha_3 = 1 $\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Step 5: Compute the Bias $ b $\n",
    "\n",
    "We use the **support vectors** (those with $ \\alpha_i > 0 $) to compute the **bias $ b $**.\n",
    "\n",
    "We use the **decision function**:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\n",
    "$$\n",
    "\n",
    "Letâ€™s compute $ b $ using one of the support vectors, say $ \\mathbf{x}_1 = [1,1] $:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}_1) = \\alpha_1 y_1 K(\\mathbf{x}_1, \\mathbf{x}_1) + \\alpha_2 y_2 K(\\mathbf{x}_2, \\mathbf{x}_1) + \\alpha_3 y_3 K(\\mathbf{x}_3, \\mathbf{x}_1) + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}_1) = (0.5)(1)(1) + (0.5)(1)(0.8187) + (1)(-1)(0.4493) + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(\\math\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
