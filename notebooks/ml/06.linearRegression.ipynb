{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e30e58",
   "metadata": {},
   "source": [
    "Hypothesis: \n",
    "\n",
    "$ h_{\\theta}(x) = \\theta_0 + \\theta_1x $\n",
    "\n",
    "$ h_{\\theta}(x) = \\hat{y} $\n",
    "\n",
    "---\n",
    "Cost function + GD for parameters:\n",
    "\n",
    "SSE :Sum squared error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{2} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2 \\Rightarrow \\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "MSE :Mean Sum squared error ($ \\frac{1}{2m} $ instead of $ \\frac{1}{2} $)\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2 \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "RMSE: root of mean squared error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\sqrt{MSE} = \\sqrt{\\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2} \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{2 \\sqrt{ \\frac{1}{2m} \\sum (h - y)^2 }} \\cdot \\frac{1}{m} \\sum (h - y) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{2 \\sqrt{ \\frac{1}{2m} \\sum (h - y)^2 }} \\cdot \\frac{1}{m} \\sum (h - y) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "MAE: mean average error\n",
    "\n",
    "$ J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i}) \\Rightarrow \n",
    "\\begin{cases} \n",
    "\\Delta{\\theta_0} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\text{sign}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\Delta{\\theta_j} = - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\text{sign}(h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "goal: $ \\min{J(\\theta_0, \\theta_1)} $\n",
    "\n",
    "---\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "1. starts with random variables for $\\theta_0, \\theta_1$\n",
    "2. Reapeat untill convergence:\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1) $$\n",
    "\n",
    "$ \\alpha $: learning rate\n",
    "learning rate too small: slow convergence\n",
    "learning rate too big: divergence or slow convergence\n",
    "\n",
    "convergence : $\\Delta{\\theta_1} \\leq \\epsilon$\n",
    "\n",
    "* at each update, both $\\theta_0, \\theta_1$ must be updated simultaniously.\n",
    "$$ \\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1) \\to \\Delta{\\theta_0} = - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "$$ \\theta_1 = \\theta_1 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1) \\to \\Delta{\\theta_1} = - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "---\n",
    "\n",
    "* in linear regression, cost function is a convex function, so in condition of convergence, it's converged to the global optimum.\n",
    "\n",
    "---\n",
    "- epoch: one full pass over all the training data.\n",
    "- iteration: when you update the model parameters once (The number of iterations per epoch depends on the batch size).\n",
    "\n",
    "---\n",
    "Batch Gradient Descent (BGD):\n",
    "uses the entire training dataset to compute the gradient and update the parameters in each iteration.\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\n",
    "$$\n",
    "\n",
    "+ High accuracy\n",
    "+ Guaranteed convergence to the optimal solution (if the cost function is convex)\n",
    "+ Slow (because it processes the entire dataset in each iteration)\n",
    "- High memory usage\n",
    "- Can't escape local minima easily\n",
    "\n",
    "Best for Small datasets where accuracy is critical.\n",
    "\n",
    "---\n",
    "\n",
    "Stochastic Gradient Descent (SGD):\n",
    "In Stochastic Gradient Descent, we use only one training example at a time to compute the gradient and update the parameters.\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\n",
    "$$\n",
    "\n",
    "+ Fast (only one sample is processed per iteration)\n",
    "+ Low memory usage\n",
    "+ Can escape local minima due to high variance\n",
    "- High variance (since it uses only one sample)\n",
    "- Noisy updates (may not converge to the exact minimum)\n",
    "- Need shuffling for each epoch\n",
    "\n",
    "Best for: Large datasets where speed is important.\n",
    "\n",
    "---\n",
    "\n",
    "Mini-Batch Gradient Descent\n",
    "is a compromise between BGD and SGD. We use a small batch of samples (e.g., 32, 64, 128) to compute the gradient and update the parameters.\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{b} \\sum_{i=1}^{b} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\n",
    "$$\n",
    "- $ b $: batch size\n",
    "\n",
    "+ Balances speed and accuracy\n",
    "+ Faster than BGD, more accurate than SGD\n",
    "+ Efficient for large datasets\n",
    "- Requires tuning the batch size\n",
    "\n",
    "Most practical applications, especially in **Deep Learning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdcaab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
