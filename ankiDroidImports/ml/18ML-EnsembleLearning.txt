#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

18ML0001	Basic	18ML-EnsembleLearning	What is the core idea behind ensemble learning in machine learning?	Combine predictions from multiple base models (learners) to create a single, more accurate and robust predictive model. The principle is that a group of weak learners can together form a strong learner.
18ML0002	Basic	18ML-EnsembleLearning	Name common, simple fusion functions used to combine predictions from multiple classifiers in an ensemble.	For regression or classifier confidence scores: Sum, Weighted Sum. For classifier votes: Majority Voting (implicitly uses sum), Median, Minimum, Maximum, Product. Weighted sum is most common for boosting.
18ML0003	Basic	18ML-EnsembleLearning	What is the primary statistical flaw each technique (Bagging & Boosting) aims to reduce? Provide a mnemonic and the correct reason.	**Mnemonic:** "Bag" and "Var" share the 'a' sound; Bagging reduces Variance. Boosting (the other one) reduces Bias.<br>**Truth:** Bagging reduces variance by averaging over models fit to bootstrapped datasets. Boosting reduces bias by sequentially focusing on misclassified examples.
18ML0004	Basic	18ML-EnsembleLearning	What is the fundamental operational difference between sequential (e.g., boosting) and parallel (e.g., bagging) ensemble methods?	Sequential methods build base learners one after another, where each new learner tries to correct the errors of the current ensemble. Parallel methods build all base learners independently, often on different data subsets.
18ML0005	Basic	18ML-EnsembleLearning	What is the fundamental strategy of the boosting family of algorithms?	Sequentially train simple, weak learners (e.g., decision stumps). After each step, increase the weight of training instances that were misclassified, forcing the next learner to focus more on these hard examples. Finally, combine all weak learners via a weighted vote.
18ML0006	Basic	18ML-EnsembleLearning	What is a typical "weak learner" in boosting, and what is a "decision stump"?	A weak learner is any model that performs slightly better than random guessing (error < 0.5). A **decision stump** is a one-level decision tree (a single split/rule), which is a common, simple weak learner.
18ML0007	Basic	18ML-EnsembleLearning	In AdaBoost, how are the training instance weights initialized, and what is the goal for each weak learner's error?	All N training instance weights are initialized equally: \( w_i^{(1)} = \frac{1}{N} \). The goal for weak learner \( h_m \) is to achieve a **weighted classification error \( \epsilon_m \) less than 0.5** (better than random).
18ML0008	Basic	18ML-EnsembleLearning	In AdaBoost iteration *m*, how is the weighted error \( \epsilon_m \) calculated, and how is the importance \( \alpha_m \) of the weak learner \( h_m \) determined?	\( \epsilon_m = \frac{\sum_{i=1}^{N} w_i^{(m)} \cdot \mathbb{I}(h_m(x_i) \ne y_i)}{\sum_{i=1}^{N} w_i^{(m)} } \)<br>* it's the sum of weights for missclassified instances.<br><br>The learner weight is: \( \alpha_m = \frac{1}{2} \ln\left(\frac{1 - \epsilon_m}{\epsilon_m}\right) \). A lower error \( \epsilon_m \) yields a higher \( \alpha_m \).
18ML0009	Basic	18ML-EnsembleLearning	After adding weak learner \( h_m \) with weight \( \alpha_m \), how are the instance weights updated for iteration *m+1* in AdaBoost?	\( w_i^{(m+1)} = w_i^{(m)} \cdot \exp\left(-\alpha_m \cdot y_i \cdot h_m(x_i)\right) \), then normalized. Correctly classified (\( y_i h_m(x_i)=+1 \)) weights decrease. Misclassified (\( y_i h_m(x_i)=-1 \)) weights increase.
18ML0010	Basic	18ML-EnsembleLearning	How does the final AdaBoost classifier \( H \) make a prediction for a new input \( x \)?	It takes a weighted vote of all weak learners:<br>\( H(x) = \text{sign}\left( \sum_{m=1}^{M} \alpha_m h_m(x) \right) \).
18ML0011	Basic	18ML-EnsembleLearning	What loss function does AdaBoost implicitly minimize? Describe its behavior.	AdaBoost minimizes the **exponential loss**: \( L(y, H(x)) = \exp(-y \cdot H(x)) \). It heavily penalizes misclassifications (large negative \( yH(x) \)), driving the algorithm to focus on hard examples.
18ML0012	Basic	18ML-EnsembleLearning	What is the bound on the training error of the AdaBoost ensemble after M rounds?	\( \frac{1}{N} \sum_{i=1}^N \mathbb{I}(H(x_i) \ne y_i) \le \prod_{m=1}^{M} 2\sqrt{\epsilon_m(1-\epsilon_m)} \). Since \( \epsilon_m < 0.5 \), each term is \( <1 \), so the bound (and typically the error) decreases.
18ML0013	Basic	18ML-EnsembleLearning	In AdaBoost's training error bound inequality, what do the left-hand side (LHS) and right-hand side (RHS) represent?	**LHS:** \( \frac{1}{N} \sum_{i=1}^N \mathbb{I}(H(x_i) \ne y_i) \)<br>This is the *actual training error* - the fraction of training examples misclassified by the final ensemble \( H(x) \).<br><br>**RHS:** \( \prod_{m=1}^{M} 2\sqrt{\epsilon_m(1-\epsilon_m)} \)<br>This is an *upper bound* on that training error, calculated from the weighted errors \( \epsilon_m \) of each weak learner.<br><br>**Comparison:** The inequality \( \text{LHS} \le \text{RHS} \) means: "The actual training error is guaranteed to be less than or equal to this product of terms."
18ML0014	Basic	18ML-EnsembleLearning	What are key misunderstandings about AdaBoost's training error bound?	**Pitfall 1:** "The product IS the training error." ❌ Wrong! The product is an *upper bound*. Actual error is lower.<br><br>**Pitfall 2:** "If bound goes to zero, test error goes to zero." ❌ Wrong! This only applies to *training* error. Test error may increase due to overfitting.<br><br>**Pitfall 3:** "Each factor ≈ 1 means no progress." ❌ Misleading! Even if \( 2\sqrt{\epsilon(1-\epsilon)} = 0.99 \), after 100 rounds the bound is \( 0.99^{100} ≈ 0.37 \), still showing guaranteed improvement.<br><br>**Key Insight:** The inequality shows AdaBoost *must* reduce training error if weak learners are better than random, but says nothing about generalization to new data.
18ML0015	Basic	18ML-EnsembleLearning	What are the practical implications and limitations of AdaBoost's training error bound?	**Implications:**<br>1. Guarantees training error reduction if each \( \epsilon_m < 0.5 \).<br>2. Shows error decreases *exponentially* with number of rounds \( M \).<br><br>**Limitations/Pitfalls:**<br>1. **Bound vs. Reality:** The bound may be loose; actual error decreases slower.<br>2. **Test Error:** This is only for *training* error. Test error may increase due to overfitting, especially with noisy data.<br>3. **Weak Learner Quality:** If \( \epsilon_m \) approaches 0.5, the factor approaches 1, and progress slows drastically.<br>4. **Noise Sensitivity:** The bound assumes weak learners can achieve \( \epsilon_m < 0.5 \). With noisy labels, this may fail, causing poor performance.<br><br>**Key Insight:** The product bound shows AdaBoost aggressively minimizes training error, explaining its tendency to eventually overfit on noisy datasets if run for too many rounds.
18ML0016	Basic	18ML-EnsembleLearning	Is AdaBoost prone to overfitting? Under what conditions might it occur?	AdaBoost is often resistant to overfitting, but it can happen with: 1) **Very noisy data** (the algorithm keeps trying to fit outliers), 2) **Too many rounds (M)** on complex data. Remedies: Early stopping, cross-validation to choose M.
18ML0017	Basic	18ML-EnsembleLearning	Define the Bagging algorithm. What statistical property does it primarily improve?	1. Create *B* bootstrap samples (random subsets with replacement) from the training set.<br>2. Train a base learner (e.g., decision tree) on each sample.<br>3. Aggregate predictions via majority vote (classification) or averaging (regression).<br>   It primarily **reduces variance**.
18ML0018	Basic	18ML-EnsembleLearning	What condition is necessary for bagging to effectively improve performance over a single model?	The base learners must be **diverse** (make different errors). If they are identical, bagging offers no improvement. Diversity is achieved by training on different data subsets.
18ML0019	Basic	18ML-EnsembleLearning	What is an "unstable" learner, and why are they ideal for bagging?	An unstable learner is one whose output changes significantly with small changes in the training data (high variance). Bagging stabilizes them by averaging. **Decision trees** are classic unstable learners, making them ideal for bagging.
18ML0020	Basic	18ML-EnsembleLearning	How does the Random Forest algorithm extend basic bagging for decision trees to increase diversity further?	When splitting a node during tree construction, it considers only a **random subset of features** (e.g., \( \sqrt{D} \)) instead of all \( D \) features. This decorrelates the trees more than bagging alone.
18ML0021	Basic	18ML-EnsembleLearning	Compare the sensitivity of Boosting and Bagging to noisy data and outliers.	**Boosting** is more sensitive; it tries to fit hard examples, which can be noise, leading to potential overfitting. **Bagging** is more robust; averaging over bootstraps dilutes the influence of any single outlier.
18ML0022	Basic	18ML-EnsembleLearning	Correct this pitfall: "In boosting, the first weak learner uses the simplest data points, and their probability is decreased for the next round."	This is incorrect. All data points start with equal weight/probability. The algorithm doesn't pre-select "simple" points. It *discovers* hard points (those consistently misclassified) and their weights **increase** over rounds.
18ML0023	Basic	18ML-EnsembleLearning	Correct this pitfall from the notes: "Decision trees are stable classifiers because of Information Gain."	This is incorrect. Decision trees are **unstable** (high-variance) learners. Small data changes can lead to completely different splits/trees, despite using IG. This instability is *why* bagging works so well on them.
18ML0024	Basic	18ML-EnsembleLearning	What is bootstrap sampling in the context of bagging, and what is its purpose?	Bootstrap sampling creates multiple datasets by randomly selecting N samples *with replacement* from the original training set of size N. This produces datasets where some examples appear multiple times and others not at all. The purpose is to introduce diversity among the base learners while keeping the same dataset size.
18ML0025	Basic	18ML-EnsembleLearning	How does Random Forest extend bagging for decision trees through feature randomization?	At each split node during tree construction, instead of considering all D features for the optimal split, Random Forest considers only a random subset of m features (typically m = √D or log₂(D)). This decorrelates the trees more effectively than bagging alone, further reducing variance.
18ML0026	Basic	18ML-EnsembleLearning	What are the two main dimensions of randomization in ensemble methods, and which techniques use them?	1. **Data Randomization:** Creating different training subsets (e.g., bootstrap samples in bagging).<br>2. **Feature Randomization:** Restricting which features are available for learning/splitting (e.g., Random Forest, Random Subspaces).<br><br>**Combination:** Random Forest uses BOTH dimensions: bootstrap sampling (data) AND random feature subsets (features).
18ML0027	Basic	18ML-EnsembleLearning	Name other ensemble methods that use randomization and describe their approach.	- **Random Subspaces:** Train learners on random subsets of features (without bootstrap sampling).<br>- **Extremely Randomized Trees (ExtraTrees):** Randomize both feature selection AND split point selection (choosing random thresholds).<br>- **Random Patches:** Use random subsets of both samples AND features.
18ML0028	Basic	18ML-EnsembleLearning	Is the statement "Bootstrap is random selection of data, Random Forest is random selection of features" accurate? If not, correct it.	**This is incomplete/incorrect.**<br><br>- **Bootstrap Aggregating (Bagging)** uses only data randomization (bootstrap sampling).<br>- **Random Forest** uses BOTH data randomization (bootstrap samples) AND feature randomization (random feature subsets at each split).<br><br>**Correct Statement:** "Bagging randomizes data; Random Forest randomizes both data AND features."
18ML0029	Basic	18ML-EnsembleLearning	What is the fundamental purpose of introducing randomization in ensemble methods?	To create **diverse** base learners that make uncorrelated errors. By training on different data or feature subsets, each learner explores different aspects of the problem. When combined, their errors tend to cancel out, improving robustness and generalization.
18ML0030	Basic	18ML-EnsembleLearning	Why does bagging logistic regression typically yield minimal improvement?	Logistic regression is a **stable, low-variance** model. Its decision boundary is largely deterministic given the training data. Bootstrap samples create very similar models, so the ensemble lacks diversity. Since bagging reduces variance and LR has little variance to reduce, averaging similar predictions offers little benefit. It may even slightly worsen performance due to reduced effective training set size.
18ML0031	Basic	18ML-EnsembleLearning	What is incorrect about this statement: "Each bootstrap sample contains about 63% of the data, meaning it's missing 37% of the data"?	This mischaracterizes the bootstrap process in two ways:<br><br>1. **Size Misconception:** A bootstrap sample has **n entries total** (same size as original), not 63% of n. The "63%" refers to the proportion of *unique* data points, not the sample size.<br>2. **Missing vs. Replaced:** The sample isn't "missing" 37% of entries — those positions are filled with **duplicates** of other points. Every bootstrap draw selects from all n points, so some are selected multiple times while others are not selected at all.<br><br>**Key Clarification:** A bootstrap sample contains approximately 63.2% *unique* original data points, but through repetition, it maintains the original dataset size n.
18ML0032	Basic	18ML-EnsembleLearning	Describe an ensemble method that randomizes only the training data, not the features.	**Bagging (Bootstrap Aggregating):**<br><br>- **Randomization:** Creates multiple bootstrap samples (with replacement) from the original training data. Each learner trains on a different data subset.<br>- **Features:** All features are available to all learners.<br>- **Base Learners:** Typically high-variance models (deep decision trees).<br>- **Purpose:** Reduces variance by averaging over data-induced diversity.<br>- **Example:** Bagged Decision Trees (without feature sampling).
18ML0033	Basic	18ML-EnsembleLearning	Describe an ensemble method that randomizes only the features, not the training data.	**Random Subspace Method:**<br><br>- **Randomization:** For each base learner, randomly select a subset of features (e.g., 50% of features). All learners use the *full* training dataset.<br>- **Data:** The complete training set is used by all learners.<br>- **Base Learners:** Can be any model (decision trees, linear models, etc.).<br>- **Purpose:** Creates diversity through different feature perspectives; especially useful for high-dimensional data.<br>- **Example:** Random Subspace of Logistic Regressions.
18ML0034	Basic	18ML-EnsembleLearning	Describe the canonical ensemble method that randomizes both data AND features.	**Random Forest:**<br><br>- **Randomization 1 (Data):** Bootstrap sampling (like bagging).<br>- **Randomization 2 (Features):** At each split node, consider only a random subset of features (typically m = √D or log₂D).<br>- **Base Learners:** Decision trees (usually grown deep without pruning).<br>- **Purpose:** Maximizes diversity through dual randomization, leading to greater variance reduction and decorrelation between trees.<br>- **Example:** Standard Random Forest algorithm.
18ML0035	Basic	18ML-EnsembleLearning	In bagging or Random Forest with bootstrap sampling (n data points), what fraction of the original data appears in a typical bootstrap sample on average?	On average, each bootstrap sample contains approximately **63.2%** of the *unique* data points from the original dataset. This means about 36.8% of the original data points are *not* included in a given sample (the "out-of-bag" or OOB samples).<br><br>**Calculation:**<br><br>probability for any point being selected is \( \frac{1}{n} \) so it not being selected is \( 1 - \frac{1}{n} \). for a point never being chosen for n random sampling (Probability a specific point is NOT in a bootstrap sample)  = \( (1 - \frac{1}{n})^n \).<br><br>For large n, this ≈ \( e^{-1} ≈ 0.368 \). So probability it IS included ≈ \( 1 - 0.368 = 0.632 \).
18ML0036	Basic	18ML-EnsembleLearning	What is the exact expected number of *unique* data points in a bootstrap sample of size n?	\( E[\text{unique points}] = n \cdot \left(1 - (1 - \frac{1}{n})^n\right) \)<br><br>For large n, this approaches \( n \cdot (1 - e^{-1}) ≈ 0.632n \).<br><br>Example: For n=100, \( (1 - 1/100)^{100} ≈ 0.366 \), so expected unique points ≈ 100 × (1 - 0.366) = 63.4.
18ML0037	Basic	18ML-EnsembleLearning	What are Out-of-Bag (OOB) samples in Random Forest and why are they useful?	OOB samples are the ~36.8% of data points *not* included in a particular tree's bootstrap sample. Since these points were not seen during that tree's training, they can be used as a **built-in validation set** to estimate:<br><br>1. The tree's performance<br>2. The ensemble's generalization error (without needing a separate validation set)<br>3. Feature importance scores
18ML0038	Basic	18ML-EnsembleLearning	In ensemble methods, what happens if we sample *without* replacement instead of bootstrap sampling?	This is called **Pasting** (as opposed to Bagging which uses bootstrap). If we sample m < n points without replacement:<br><br>- Each tree sees completely different data subsets<br>- Lower diversity between trees (no repeated points)<br>- Useful for large datasets where bootstrap is computationally expensive<br>- Typically requires larger m (e.g., m = 0.8n) to maintain tree quality
18ML0039	Basic	18ML-EnsembleLearning	In Random Forest with total D features, if we sample m features at each split, what's the probability a specific feature is considered at a given split?	Probability = \( \frac{m}{D} \)<br><br>This means each feature has a \( \frac{m}{D} \) chance to be available for consideration at any particular split node. Typically m = \( \sqrt{D} \) or \( \log_2(D) \), so this probability is relatively small, enforcing feature diversity.
18ML0040	Basic	18ML-EnsembleLearning	In a Random Forest tree of depth L (with ~\( 2^L \) split nodes), how many times is a specific feature expected to be considered for splitting?	Expected considerations = \( 2^L \cdot \frac{m}{D} \)<br><br>However, once a feature is used for a split, it may not be available again on the same branch depending on implementation. In practice, features higher in the tree block that feature's reuse in child nodes, making the actual expected usage lower.