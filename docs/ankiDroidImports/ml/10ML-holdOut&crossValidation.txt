#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

10ML0001	Basic	10ML-holdOut&crossValidation	What is simple holdout and its main limitation?	A single split (typically 70-80% train, 20-30% test).<br><br>Limitation: High variance in performance estimates due to sensitivity to that one random split.
10ML0002	Basic	10ML-holdOut&crossValidation	Why use a three-way split instead of two-way?	Three splits allow separate phases: Training (fit model), Validation (tune hyperparameters), and Testing (final unbiased evaluation). Prevents data leakage from tuning into final metrics.
10ML0003	Basic	10ML-holdOut&crossValidation	What defines random holdout and when is it appropriate?	Data points are randomly assigned to splits. Appropriate when data is IID (Independent and Identically Distributed) and balanced.
10ML0004	Basic	10ML-holdOut&crossValidation	What is stratified holdout and when is it crucial?	Splits preserve the original class/group proportions in each subset. Crucial for classification with imbalanced datasets to ensure all splits represent all classes.
10ML0005	Basic	10ML-holdOut&crossValidation	How can you diagnose underfitting vs. overfitting using holdout?	High train AND test error \( \to \) Underfitting (High Bias). Low train error but high test error \( \to \) Overfitting (High Variance).
10ML0006	Basic	10ML-holdOut&crossValidation	What is the main statistical trade-off of using holdout?	Reduces BIAS in performance estimation (gives honest, pessimistic estimate) but increases VARIANCE (single split gives unstable estimate). For lower variance, use cross-validation.
10ML0007	Basic	10ML-holdOut&crossValidation	When is holdout preferable over cross-validation?	With very large datasets (\( > \)10k samples), for quick prototyping, or with limited computational resources.
10ML0008	Basic	10ML-holdOut&crossValidation	What is the fundamental limitation of any holdout method?	It wastes data. Part of the dataset is withheld from training the model, which can be problematic with small datasets.
10ML0009	Basic	10ML-holdOut&crossValidation	What is the fundamental idea behind cross-validation?	Repeatedly partitioning data into training and validation sets to use all data points for both training and validation, providing a more robust performance estimate than a single holdout split.
10ML0010	Basic	10ML-holdOut&crossValidation	What problem does cross-validation solve that holdout doesn't?	CV reduces the **high variance** (unstable estimates) of single holdout splits by averaging performance across multiple splits, while maintaining **low bias** (unbiased estimates).
10ML0011	Basic	10ML-holdOut&crossValidation	Describe the process of standard k-fold CV.	1. Randomly shuffle dataset<br>2. Split into k equal-sized folds<br>3. For i=1 to k:<br>   - Use fold i as validation set<br>   - Use remaining k-1 folds as training set<br>   - Train model, evaluate on fold i<br>4. Average all k performance scores
10ML0012	Basic	10ML-holdOut&crossValidation	What are typical k values and the main trade-off?	Common: k=5 or k=10. Trade-off: Higher k \( \to \) less bias (more training data each iteration) but higher computational cost and potentially higher variance in each estimate.
10ML0013	Basic	10ML-holdOut&crossValidation	What is LOOCV and when is it used?	Extreme case where k = n (number of samples). Each iteration uses one sample as validation and n-1 as training. Used for very small datasets to maximize training data.
10ML0014	Basic	10ML-holdOut&crossValidation	What are the main advantages and disadvantages of LOOCV?	<b>Advantage:</b> Virtually unbiased estimate (uses n-1 samples for training each time)<br><br><b>Disadvantages:</b> Extremely computationally expensive (n models), high variance in estimates (each test set is just one point)
10ML0015	Basic	10ML-holdOut&crossValidation	What is stratified k-fold CV and why is it important?	Modified k-fold that preserves class proportions in each fold. Crucial for classification with imbalanced datasets to ensure each fold represents all classes proportionally.
10ML0016	Basic	10ML-holdOut&crossValidation	When must you use stratified k-fold over standard?	Always use stratified for classification problems, especially with imbalanced classes. For regression or balanced classification, standard k-fold may suffice.
10ML0017	Basic	10ML-holdOut&crossValidation	When should you choose CV over holdout?	Choose CV when: dataset is small/medium (\( < \)10k samples), computational cost is acceptable, and you need stable, reliable performance estimates.
10ML0018	Basic	10ML-holdOut&crossValidation	What does the average CV score actually estimate?	It estimates model performance when trained on all available data (since each training set is almost the full dataset), giving better guidance for final model deployment.
10ML0019	Basic	10ML-holdOut&crossValidation	Rank CV methods by computational cost (lowest to highest).	Standard K-Fold \( < \) Stratified K-Fold \( < < \) Leave-One-Out (with LOOCV being n times more expensive than training one model).
10ML0020	Basic	10ML-holdOut&crossValidation	What is Leave-P-Out Cross-Validation (LPOCV)?	A generalized cross-validation method where P data points are held out for validation each iteration, and the model is trained on the remaining n-P points, repeated for ALL possible combinations of P points.
10ML0021	Basic	10ML-holdOut&crossValidation	How does LPOCV relate to LOOCV and k-fold CV?	LOOCV is a special case of LPOCV where P=1. K-fold CV is a practical approximation/sampling of LPOCV space that's computationally feasible.
10ML0022	Basic	10ML-holdOut&crossValidation	How many iterations does LPOCV require for a dataset of size n?	\( C(n, P) = n! / (P! \times (n-P)!) \) iterations, which grows combinatorially.
10ML0023	Basic	10ML-holdOut&crossValidation	What happens when P=1 and P=n/2 in LPOCV?	P=1 \( \to \) LOOCV (n iterations). P=n/2 \( \to \) all possible half/half splits (maximum combinations).
10ML0024	Basic	10ML-holdOut&crossValidation	When might LPOCV actually be feasible?	Only with very small datasets (n\( < \)20) and small P values (P\( \leq \)3), or when computation cost is irrelevant.
10ML0025	Basic	10ML-holdOut&crossValidation	What is Monte Carlo Cross-Validation?	Repeated random splitting of data into train/test sets over many iterations, with no requirement for exhaustive or non-overlapping test sets.
10ML0026	Basic	10ML-holdOut&crossValidation	What other names is Monte Carlo CV known by?	Repeated Random Subsampling or Shuffle-Split Cross-Validation.
10ML0027	Basic	10ML-holdOut&crossValidation	How does Monte Carlo CV differ fundamentally from k-fold CV?	In Monte Carlo, test sets can overlap across iterations and some data points might never be tested, unlike k-fold where each point is tested exactly once.
10ML0028	Basic	10ML-holdOut&crossValidation	What three parameters do you specify for Monte Carlo CV?	Number of iterations, training set percentage/size, and test set percentage/size.
10ML0029	Basic	10ML-holdOut&crossValidation	What's the main disadvantage of Monte Carlo compared to k-fold?	Not exhaustive - some data points might never be included in test sets by random chance.
10ML0030	Basic	10ML-holdOut&crossValidation	When is Monte Carlo CV particularly useful?	With very large datasets where exhaustive testing is unnecessary, or when exploring how performance varies with different random splits.
10ML0031	Basic	10ML-holdOut&crossValidation	How is Monte Carlo CV related to simple holdout?	It's essentially repeated random holdout splits, averaging many single holdout estimates to reduce variance.