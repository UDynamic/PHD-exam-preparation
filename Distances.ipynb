{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7fd830",
   "metadata": {},
   "source": [
    "# Mahalonobis distance\n",
    "\n",
    "Ø³Ù„Ø§Ù…! ğŸ˜Š\n",
    "\n",
    "Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨Ù‡ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ ÙÙˆØ§ØµÙ„ Ù…Ø®ØªÙ„Ù Ø±Ùˆ Ø¨Ù„Ø¯ Ø¨Ø§Ø´ÛŒ. ÙØ§ØµÙ„Ù‡ Ù…Ø§Ù‡Ø§Ù„Ø§Ù†ÙˆØ¨ÛŒØ³ ÙÙ‚Ø· ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù†ÙˆØ§Ø¹ ÙÙˆØ§ØµÙ„ Ù‡Ø³Øª Ú©Ù‡ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø®Ø§Øµ Ù…Ø«Ù„ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø®ØªÙ„Ù ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡. Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ú†Ù†Ø¯ ÙØ§ØµÙ„Ù‡ Ù…Ù‡Ù… Ø¯ÛŒÚ¯Ù‡ Ø±Ùˆ Ø¨Ù‡Øª Ù…Ø¹Ø±ÙÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù… Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆÙ†Ù† Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ù†:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **ÙØ§ØµÙ„Ù‡ Ø§Ù‚Ù„ÛŒØ¯Ø³ÛŒ (Euclidean Distance)**\n",
    "Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ù…Ø¹Ø±ÙˆÙâ€ŒØªØ±ÛŒÙ† Ùˆ Ù¾Ø±Ú©Ø§Ø±Ø¨Ø±Ø¯ØªØ±ÛŒÙ† ÙÙˆØ§ØµÙ„ Ù‡Ø³Øª. ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ø¯Ùˆ Ù†Ù‚Ø·Ù‡ Ø¯Ø± ÙØ¶Ø§ Ø±Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª Ø±ÛŒØ´Ù‡ Ù…Ø±Ø¨Ø¹ Ù…Ø¬Ù…ÙˆØ¹ Ù…Ø±Ø¨Ø¹Ø§Øª ØªÙØ§Ø¶Ù„ Ù…Ø®ØªØµØ§ØªØ´ÙˆÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.\n",
    "\n",
    "**ÙØ±Ù…ÙˆÙ„:**\n",
    "$$ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **ÙØ§ØµÙ„Ù‡ Ù…Ù†Ù‡ØªÙ† (Manhattan Distance)**\n",
    "Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡ Ú¯Ø§Ù‡ÛŒ \"ÙØ§ØµÙ„Ù‡ Ø¨Ù„ÙˆÚ©ÛŒ\" Ù‡Ù… Ù†Ø§Ù…ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡. Ø¯Ø± Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡ØŒ ÙÙ‚Ø· Ù…Ø¬Ù…ÙˆØ¹ Ù‚Ø¯Ø± Ù…Ø·Ù„Ù‚ ØªÙØ§Ø¶Ù„ Ù…Ø®ØªØµØ§Øª Ø±Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
    "\n",
    "**ÙØ±Ù…ÙˆÙ„:**\n",
    "$$ d(x, y) = \\sum_{i=1}^{n} |x_i - y_i| $$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **ÙØ§ØµÙ„Ù‡ Ú†Ø¨ÛŒØ´Ù (Chebyshev Distance)**\n",
    "Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ù…Ø³Ø§Ø¦Ù„ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¯Ø§Ø±Ù‡ Ú©Ù‡ Ø­Ø±Ú©Øª Ø¯Ø± ØªÙ…Ø§Ù… Ø¬Ù‡Ø§Øª ÛŒÚ©Ø³Ø§Ù†Ù‡ (Ù…Ø«Ù„ Ø­Ø±Ú©Øª Ø´Ø§Ù‡ Ø¯Ø± Ø´Ø·Ø±Ù†Ø¬). ÙØ§ØµÙ„Ù‡ Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ù‚Ø¯Ø± Ù…Ø·Ù„Ù‚ ØªÙØ§Ø¶Ù„ Ù…Ø®ØªØµØ§Øª Ù‡Ø³Øª.\n",
    "\n",
    "**ÙØ±Ù…ÙˆÙ„:**\n",
    "$$ d(x, y) = \\max_{i} |x_i - y_i| $$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **ÙØ§ØµÙ„Ù‡ Ù…ÛŒÙ†Ú©ÙˆÙØ³Ú©ÛŒ (Minkowski Distance)**\n",
    "Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡ ÛŒÚ© ÙØ±Ù… Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø² ÙØ§ØµÙ„Ù‡ Ø§Ù‚Ù„ÛŒØ¯Ø³ÛŒ Ùˆ Ù…Ù†Ù‡ØªÙ† Ù‡Ø³Øª. Ø¨Ø§ ØªØºÛŒÛŒØ± Ù¾Ø§Ø±Ø§Ù…ØªØ± $ p $ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒÙ… Ø¨ÛŒÙ† Ø§ÛŒÙ† Ø¯Ùˆ ÙØ§ØµÙ„Ù‡ Ùˆ ÙÙˆØ§ØµÙ„ Ø¯ÛŒÚ¯Ù‡ Ø¬Ø§Ø¨Ù‡â€ŒØ¬Ø§ Ø¨Ø´ÛŒÙ….\n",
    "\n",
    "**ÙØ±Ù…ÙˆÙ„:**\n",
    "$$ d(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p} $$\n",
    "\n",
    "- Ø§Ú¯Ø± $ p = 1 $: ÙØ§ØµÙ„Ù‡ Ù…Ù†Ù‡ØªÙ†\n",
    "- Ø§Ú¯Ø± $ p = 2 $: ÙØ§ØµÙ„Ù‡ Ø§Ù‚Ù„ÛŒØ¯Ø³ÛŒ\n",
    "- Ø§Ú¯Ø± $ p \\to \\infty $: ÙØ§ØµÙ„Ù‡ Ú†Ø¨ÛŒØ´Ù\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **ÙØ§ØµÙ„Ù‡ Ù‡Ù…â€ŒØ§ÙØ²ÙˆÙ† (Cosine Similarity / Cosine Distance)**\n",
    "Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¬Ù‡Øª Ø¯Ùˆ Ø¨Ø±Ø¯Ø§Ø± (Ù…Ø«Ù„ Ø¯Ùˆ Ù…ØªÙ† ÛŒØ§ Ø¯Ùˆ ØªØµÙˆÛŒØ±) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡. ÙØ§ØµÙ„Ù‡ Ù‡Ù…â€ŒØ§ÙØ²ÙˆÙ† Ø±Ùˆ Ø¨Ø§ 1 Ù…Ù†Ù‡Ø§ÛŒ Ø¶Ø±ÛŒØ¨ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ú©Ø³ÛŒÙ†ÙˆØ³ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
    "\n",
    "**ÙØ±Ù…ÙˆÙ„:**\n",
    "$$ \\text{Cosine Similarity} = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} $$\n",
    "$$ \\text{Cosine Distance} = 1 - \\text{Cosine Similarity} $$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **ÙØ§ØµÙ„Ù‡ Ù‡Ù…â€ŒØ§ÙØ²ÙˆÙ† Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø± (Jaccard Distance)**\n",
    "Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ (0 Ùˆ 1) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡. ÙØ§ØµÙ„Ù‡ Ø¬Ú©Ø§Ø±Ø¯ Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ 1 Ù…Ù†Ù‡Ø§ÛŒ Ø¶Ø±ÛŒØ¨ Ø´Ø¨Ø§Ù‡Øª Ø¬Ú©Ø§Ø±Ø¯ Ù‡Ø³Øª.\n",
    "\n",
    "**ÙØ±Ù…ÙˆÙ„:**\n",
    "$$ d(x, y) = 1 - \\frac{|x \\cap y|}{|x \\cup y|} $$\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **ÙØ§ØµÙ„Ù‡ Ù‡Ù…â€ŒØ§ÙØ²ÙˆÙ† Ø§Ø­ØªÙ…Ø§Ù„ÛŒ (Hellinger Distance)**\n",
    "Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ùˆ ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡. Ù…Ø«Ù„Ø§Ù‹ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ ØªØµÙˆÛŒØ± ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ.\n",
    "\n",
    "**ÙØ±Ù…ÙˆÙ„:**\n",
    "$$ d(x, y) = \\frac{1}{\\sqrt{2}} \\sqrt{ \\sum_{i} ( \\sqrt{x_i} - \\sqrt{y_i} )^2 } $$\n",
    "\n",
    "---\n",
    "\n",
    "Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ ÛŒÚ©ÛŒ Ø§Ø² Ø§ÛŒÙ† ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ Ø±Ùˆ Ø¨Ø§ Ú©Ø¯ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ù…ØŒ ÙÙ‚Ø· Ø¨Ú¯Ùˆ Ú©Ø¯ÙˆÙ… ÙØ§ØµÙ„Ù‡ Ø±Ùˆ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ùˆ Ú†Ù‡ Ø²Ø¨Ø§Ù†ÛŒ Ø±Ùˆ Ø¯ÙˆØ³Øª Ø¯Ø§Ø±ÛŒ (Ù…Ø«Ù„ Ù¾Ø§ÛŒØªÙˆÙ† ÛŒØ§ Ø¬Ø§ÙˆØ§Ø§Ø³Ú©Ø±ÛŒÙ¾Øª)ØŒ Ù…Ù† ÙÙˆØ±Ø§Ù‹ Ú©Ø¯Ø´ Ø±Ùˆ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ù… Ø¨Ø±Ø§ÛŒØª! âœ¨\n",
    "\n",
    "Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ÛŒ Ø¯ÛŒÚ¯Ù‡ Ø¯Ø§Ø´ØªÛŒØŒ Ø®ÙˆØ´Ø­Ø§Ù„ Ù…ÛŒâ€ŒØ´Ù… Ú©Ù…Ú© Ú©Ù†Ù…. ğŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb0607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7cdd720",
   "metadata": {},
   "source": [
    "Okay, I need to explain different types of distances and their relationships in both matrix and scalar forms. Let me start by recalling the main distance metrics mentioned: Euclidean, Manhattan, Mahalanobis, and maybe others like Minkowski or Cosine. \n",
    "\n",
    "First, Euclidean distance is the most common one. In scalar form, it's the square root of the sum of squared differences. In matrix terms, if we have vectors x and y, it can be written using the norm of their difference. I should mention that it's sensitive to scale and assumes features are independent.\n",
    "\n",
    "Next, Manhattan distance, also known as L1 distance. It's the sum of absolute differences. The matrix form would involve the L1 norm. Unlike Euclidean, it's less sensitive to outliers but still assumes independence between features.\n",
    "\n",
    "Mahalanobis distance is a bit more complex. It accounts for correlations between variables by using the inverse of the covariance matrix. The scalar form is sqrt((x - y)^T S^{-1} (x - y)), where S is the covariance matrix. This distance is scale-invariant and adjusts for the data's distribution.\n",
    "\n",
    "Then there's Minkowski distance, which generalizes both Euclidean and Manhattan. The parameter p determines the type: p=1 is Manhattan, p=2 is Euclidean. The matrix form would use the Lp norm. As p approaches infinity, it becomes the Chebyshev distance.\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors. It's not a distance metric in the strict sense but can be converted to a distance by 1 - cosine similarity. It's useful for high-dimensional spaces and text data.\n",
    "\n",
    "I should also mention the relationships between these distances. For example, Mahalanobis reduces to Euclidean when the covariance matrix is identity. Minkowski generalizes both Euclidean and Manhattan. Cosine similarity focuses on orientation rather than magnitude.\n",
    "\n",
    "I need to present each distance in both scalar and matrix forms, using LaTeX for clarity. Also, highlight their applications and when to use each. Make sure to explain the terms like covariance matrix, norm, etc., briefly for clarity. Check if there are any other distances worth mentioning, like Hamming or Jaccard, but the user specified Euclidean, Manhattan, Mahalanobis, so maybe stick to those. Ensure the explanation is clear and not too technical, but still accurate. Avoid jargon where possible and provide examples if needed. Finally, summarize the key relationships and use cases.\n",
    "</think>\n",
    "\n",
    "Here's a structured explanation of common distance metrics in **scalar** and **matrix** forms, along with their relationships:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Euclidean Distance**\n",
    "- **Scalar Form**:  \n",
    "  $ d(x, y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} $  \n",
    "- **Matrix Form**:  \n",
    "  $ d(x, y) = \\|x - y\\|_2 $  \n",
    "  (2-norm of the vector difference)  \n",
    "- **Key Properties**:  \n",
    "  - Sensitive to scale (features must be normalized).  \n",
    "  - Assumes features are independent and uncorrelated.  \n",
    "  - Special case of Minkowski distance with $ p=2 $.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Manhattan Distance (L1 Distance)**\n",
    "- **Scalar Form**:  \n",
    "  $ d(x, y) = \\sum_{i=1}^n |x_i - y_i| $  \n",
    "- **Matrix Form**:  \n",
    "  $ d(x, y) = \\|x - y\\|_1 $  \n",
    "  (1-norm of the vector difference)  \n",
    "- **Key Properties**:  \n",
    "  - Robust to outliers compared to Euclidean.  \n",
    "  - Also assumes feature independence.  \n",
    "  - Special case of Minkowski distance with $ p=1 $.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Mahalanobis Distance**\n",
    "- **Scalar Form**:  \n",
    "  $ d(x, y) = \\sqrt{(x - y)^T S^{-1} (x - y)} $  \n",
    "  (where $ S $ is the covariance matrix of the data)  \n",
    "- **Matrix Form**:  \n",
    "  $ d(x, y) = \\sqrt{(x - y)^T \\Sigma^{-1} (x - y)} $  \n",
    "  (with $ \\Sigma $ as the covariance matrix)  \n",
    "- **Key Properties**:  \n",
    "  - Accounts for **correlations** between features.  \n",
    "  - Scale-invariant (normalizes data using covariance).  \n",
    "  - Reduces to Euclidean distance when $ \\Sigma = I $ (identity matrix).  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Minkowski Distance**\n",
    "- **Scalar Form**:  \n",
    "  $ d(x, y) = \\left( \\sum_{i=1}^n |x_i - y_i|^p \\right)^{1/p} $  \n",
    "- **Matrix Form**:  \n",
    "  $ d(x, y) = \\|x - y\\|_p $  \n",
    "- **Key Properties**:  \n",
    "  - Generalizes Euclidean ($ p=2 $) and Manhattan ($ p=1 $).  \n",
    "  - As $ p \\to \\infty $, it becomes **Chebyshev distance**:  \n",
    "    $ d(x, y) = \\max_i |x_i - y_i| $.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Cosine Similarity (and Distance)**\n",
    "- **Scalar Form (Similarity)**:  \n",
    "  $ \\text{sim}(x, y) = \\frac{x \\cdot y}{\\|x\\|_2 \\|y\\|_2} $  \n",
    "- **Scalar Form (Distance)**:  \n",
    "  $ d(x, y) = 1 - \\text{sim}(x, y) $  \n",
    "- **Matrix Form**:  \n",
    "  $ d(x, y) = 1 - \\frac{x^T y}{\\|x\\|_2 \\|y\\|_2} $  \n",
    "- **Key Properties**:  \n",
    "  - Measures **orientation similarity**, not magnitude.  \n",
    "  - Commonly used in text analysis and high-dimensional data.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Relationships Between Metrics**\n",
    "| Metric              | Generalization Of       | Special Case Of       |\n",
    "|----------------------|-------------------------|------------------------|\n",
    "| Euclidean           | Minkowski ($ p=2 $)   | Mahalanobis ($ \\Sigma = I $) |\n",
    "| Manhattan           | Minkowski ($ p=1 $)   | â€”                      |\n",
    "| Mahalanobis         | â€”                       | Euclidean (if $ \\Sigma = I $) |\n",
    "| Minkowski           | â€”                       | Euclidean, Manhattan   |\n",
    "| Chebyshev           | Minkowski ($ p \\to \\infty $) | â€”                      |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Which?**\n",
    "- **Euclidean**: For continuous, uncorrelated features (e.g., 2D/3D geometry).  \n",
    "- **Manhattan**: For grid-like paths (e.g., city blocks) or robustness to outliers.  \n",
    "- **Mahalanobis**: For correlated features (e.g., multivariate Gaussian data).  \n",
    "- **Minkowski**: Flexible generalization for varying $ p $.  \n",
    "- **Cosine**: For text/document similarity or high-dimensional sparse data.  \n",
    "\n",
    "Let me know if you'd like code examples or visualizations! ğŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
