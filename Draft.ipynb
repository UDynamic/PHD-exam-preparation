{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de8a3f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Best Subset Selection\n",
    "**Front:** What is best subset selection and how does it relate to L0? <br/>\n",
    "**Back:** Best subset selection explicitly solves L0 regularization by evaluating all $2^p$ feature subsets and choosing the best one for each model size k. It's the exact implementation of L0 regularization but limited to p â‰¤ ~40 due to computational constraints.\n",
    "\n",
    "## Stepwise Approximation\n",
    "**Front:** How do stepwise selection methods approximate L0 regularization? <br/>\n",
    "**Back:** Forward stepwise adds the best feature at each step; backward stepwise removes the worst. Both greedily approximate the optimal subset without evaluating all $2^p$ combinations, providing practical but suboptimal solutions to L0 regularization.\n",
    "\n",
    "## Bayesian Interpretation\n",
    "**Front:** What Bayesian prior corresponds to L0 regularization? <br/>\n",
    "**Back:** The spike-and-slab prior: a mixture of a point mass at zero (spike) and a continuous distribution (slab). This explicitly models whether each parameter is exactly zero (excluded) or follows some distribution (included), directly implementing L0-style sparsity.\n",
    "\n",
    "## Practical Applications\n",
    "**Front:** When would you use L0 regularization despite its computational cost? <br/>\n",
    "**Back:** When model interpretability or deployment constraints require strict feature limits (e.g., medical diagnosis with limited tests, edge devices with memory constraints, or when each feature has significant measurement cost).\n",
    "\n",
    "## Optimization Challenges\n",
    "**Front:** Why can't we use gradient descent for L0 regularization? <br/>\n",
    "**Back:** The L0 \"norm\" is discontinuous and non-differentiable (derivative is 0 or undefined). More fundamentally, it's a combinatorial counting problem, not a continuous optimization problem. Specialized algorithms like matching pursuit or Bayesian methods are needed.\n",
    "\n",
    "## Regularization Path Behavior\n",
    "**Front:** How does the L0 regularization path differ from L1/L2 paths? <br/>\n",
    "**Back:** The L0 path shows discrete jumps as features enter/exit the model. Unlike L1's smooth coefficient shrinkage or L2's gradual decay, L0 coefficients jump from 0 to some value or vice versa, with model size changing by integer counts.\n",
    "\n",
    "## Information Criteria Connection\n",
    "**Front:** How are AIC and BIC related to L0 regularization? <br/>\n",
    "**Back:** AIC and BIC can be viewed as L0-regularized objectives:\n",
    "AIC: $ \\text{Loss} + 2 \\cdot \\|\\theta\\|_0 $\n",
    "BIC: $ \\text{Loss} + \\log(n) \\cdot \\|\\theta\\|_0 $\n",
    "Both penalize model complexity by counting parameters, with BIC having stronger penalty for large n.\n",
    "\n",
    "## Recent Advances\n",
    "**Front:** What modern methods approximate L0 regularization efficiently? <br/>\n",
    "**Back:** 1. **Learned Thresholding**: Iterative thresholding algorithms\n",
    "2. **Stochastic Gates**: Continuous relaxations using hard concrete distributions\n",
    "3. **SparseMAP**: Sparse posterior inference methods\n",
    "4. **Convex Relaxations**: Using non-convex but continuous penalty functions\n",
    "\n",
    "## Subset Selection Algorithms\n",
    "**Front:** What are the three main approaches to subset selection (L0 optimization)? <br/>\n",
    "**Back:** \n",
    "1. **Filter Methods**: Select features independently of model (e.g., correlation)\n",
    "2. **Wrapper Methods**: Evaluate subsets using model performance (e.g., forward selection)\n",
    "3. **Embedded Methods**: Feature selection during model training (e.g., L1 regularization)\n",
    "L0 is typically implemented via wrapper methods.\n",
    "\n",
    "## Sparsity-Difficulty Tradeoff\n",
    "**Front:** Why is achieving exact sparsity (L0) more difficult than approximate sparsity (L1)? <br/>\n",
    "**Back:** L0 requires solving discrete optimization (feature selection), while L1 solves continuous optimization (shrinkage). The combinatorial nature of \"which features\" vs. \"how much weight\" makes L0 fundamentally harder despite seeming conceptually simpler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
