{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8828664",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff\n",
    "**Front:** What fundamental ML problem do we address with regularization? <br/>\n",
    "**Back:** We address the bias-variance tradeoff. Models with high variance (complex models) overfit to training data, while models with high bias (simple models) underfit. Regularization helps find the sweet spot between these extremes.\n",
    "\n",
    "## Regularization Intuition\n",
    "**Front:** In simple terms, what does regularization do to a machine learning model? <br/>\n",
    "**Back:** It \"simplifies\" the model by penalizing complexity, making it less likely to overfit to noise in the training data and more likely to generalize well to new, unseen data.\n",
    "\n",
    "## Mathematical Framework\n",
    "**Front:** What is the general mathematical form of a regularized objective function? <br/>\n",
    "**Back:** \n",
    "$$\n",
    "J(\\theta) = L(\\theta) + \\lambda R(\\theta)\n",
    "$$\n",
    "Where $L(\\theta)$ is the original loss function, $R(\\theta)$ is the regularization term, and $\\lambda$ is the regularization parameter controlling the penalty strength.\n",
    "\n",
    "## The Regularization Parameter λ\n",
    "**Front:** What happens when λ is set to 0, very small, very large, or extremely large? <br/>\n",
    "**Back:** \n",
    "- λ = 0: No regularization, pure loss minimization\n",
    "- Small λ: Light regularization, slight complexity penalty (complex model that might overfit) \n",
    "- Large λ: Strong regularization, high complexity penalty (simple model that might underfit)\n",
    "- λ → ∞: Complete regularization, model collapses to simplest form (often all zero weights)\n",
    "\n",
    "## L1 Regularization (Lasso)\n",
    "**Front:** What is the mathematical form and primary characteristic of L1 regularization? <br/>\n",
    "**Back:**\n",
    "Mathematical form: $$R(\\theta) = \\sum_{i=1}^{n} |\\theta_i|$$\n",
    "Primary characteristic: **Sparsity** - tends to drive some parameters exactly to zero, performing automatic feature selection.\n",
    "\n",
    "## L2 Regularization (Ridge)\n",
    "**Front:** What is the mathematical form and primary characteristic of L2 regularization? <br/>\n",
    "**Back:**\n",
    "Mathematical form: $$R(\\theta) = \\sum_{i=1}^{n} \\theta_i^2$$\n",
    "Primary characteristic: **Weight shrinkage** - shrinks all parameters toward zero but rarely makes them exactly zero, producing more stable models.\n",
    "* L2 norm put's sum under the $\\sqrt{}$ but not in regularization\n",
    "\n",
    "## Geometric Interpretation\n",
    "**Front:** How can we visualize the difference between L1 and L2 constraints? <br/>\n",
    "**Back:** L1 regularization creates a diamond-shaped feasible region ($\\sum|\\theta_i| \\leq t$), where solutions often land at corners (creating sparsity). L2 creates a circular/spherical region ($\\sum\\theta_i^2 \\leq t$), where solutions land smoothly on the boundary (creating shrinkage).\n",
    "\n",
    "* $R(\\theta) = \\sum_{i=1}^{n} |\\theta_i| < t \\rightarrow \\text{forms a diamond} $\n",
    "* $R(\\theta) = \\sum_{i=1}^{n} \\theta_i^2 < t \\rightarrow \\text{forms a circle} $\n",
    "\n",
    "* in geometric form we \n",
    "## Gradient Descent with Regularization\n",
    "**Front:** How does L2 regularization modify the gradient descent update rule? <br/>\n",
    "**Back:** For parameter θ with learning rate α:<br/>\n",
    "Original: $$\\theta := \\theta - \\alpha \\frac{\\partial L}{\\partial \\theta}$$\n",
    "With L2: $$\\theta := \\theta - \\alpha \\left(\\frac{\\partial L}{\\partial \\theta} + \\lambda\\theta\\right) = \\theta(1 - \\alpha\\lambda) - \\alpha\\frac{\\partial L}{\\partial \\theta}$$\n",
    "The additional $-\\alpha\\lambda\\theta$ term causes \"weight decay\" at each update.\n",
    "\n",
    "## Elastic Net Regularization\n",
    "**Front:** What problem does Elastic Net solve, and what is its mathematical form? <br/>\n",
    "**Back:** Elastic Net combines L1 and L2 to overcome limitations of each: L1 can be unstable with correlated features, L2 doesn't perform feature selection.\n",
    "Mathematical form: \n",
    "$$\n",
    "J(\\theta) = L(\\theta) + \\lambda_1 R_1(\\theta) + \\lambda_2 R_2(\\theta)\n",
    "$$\n",
    "\n",
    "single regularization parameter: $$R(\\theta) = \\alpha\\sum|\\theta_i| + (1-\\alpha)\\sum\\theta_i^2$$\n",
    "Where α controls the mix between L1 and L2.\n",
    "\n",
    "## Early Stopping\n",
    "**Front:** How does early stopping work as an implicit regularization technique? <br/>\n",
    "**Back:** Training is stopped when validation error starts increasing while training error continues decreasing. This prevents the model from over-optimizing to training noise, effectively limiting model complexity by restricting training time.\n",
    "\n",
    "## Dropout Regularization\n",
    "**Front:** How does dropout work in neural networks, and why is it effective? <br/>\n",
    "**Back:** During training, dropout randomly \"drops\" (sets to zero) a percentage of neurons in each layer for each training example. This prevents co-adaptation of neurons, forcing the network to learn robust, redundant features, acting like training an ensemble of many thinned networks.\n",
    "\n",
    "## Weight Decay\n",
    "**Front:** What is weight decay and how does it relate to L2 regularization? <br/>\n",
    "**Back:** Weight decay is another name for L2 regularization. In the weight update equation, it appears as:\n",
    "$$\\theta := \\theta - \\alpha(\\frac{\\partial L}{\\partial \\theta} + \\lambda\\theta) = \\theta(1 - \\alpha\\lambda) - \\alpha\\frac{\\partial L}{\\partial \\theta}$$\n",
    "The $(1 - \\alpha\\lambda)$ factor causes exponential decay of weights toward zero at each update.\n",
    "\n",
    "## Practical Regularization Strategies\n",
    "**Front:** What are three practical considerations when applying regularization? <br/>\n",
    "**Back:** \n",
    "1. **Cross-validation**: Use CV to find optimal λ value\n",
    "2. **Feature scaling**: Regularization assumes features are on similar scales (standardize features first)\n",
    "3. **Don't regularize bias**: Typically exclude the bias/intercept term from regularization penalties\n",
    "\n",
    "## Regularization in Linear Regression\n",
    "**Front:** update rule for L2 in linear regression with SSE cost function? <br/>\n",
    "**Back:**\n",
    "\n",
    "$\\Delta \\theta_0= -\\alpha \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})$\n",
    "\n",
    "$\\Delta \\theta_j= -\\alpha [\\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} + \\lambda \\theta_j]$ <br/>\n",
    "or <br/>\n",
    "$\\theta_j = \\theta_j(1 - \\alpha\\lambda) - \\alpha(\\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)})$\n",
    "## Feature Selection Comparison\n",
    "**Front:** How do L1 and L2 differ in their handling of correlated features? <br/>\n",
    "**Back:** L1 tends to select one feature from a correlated group and zero out the others. L2 tends to distribute weight among correlated features, keeping all but shrinking their coefficients. L1 performs feature selection; L2 performs feature \"grouping.\"\n",
    "\n",
    "## Regularization Strength Path\n",
    "**Front:** What does a regularization path plot show us? <br/>\n",
    "**Back:** It shows how model coefficients change as λ increases from 0 to large values. For L1, we see coefficients gradually shrinking to zero (creating sparsity). For L2, we see all coefficients smoothly shrinking toward zero but rarely reaching exactly zero.\n",
    "\n",
    "## Bayesian Interpretation\n",
    "**Front:** What is the Bayesian interpretation of regularization? <br/>\n",
    "**Back:** Regularization corresponds to placing a prior distribution on the parameters:\n",
    "- L1 regularization ↔ Laplace (double exponential) prior\n",
    "- L2 regularization ↔ Gaussian prior\n",
    "The MAP (maximum a posteriori) estimate with these priors equals the regularized MLE estimate.\n",
    "\n",
    "## Double Descent Phenomenon\n",
    "**Front:** What is the double descent phenomenon and how does it relate to regularization? <br/>\n",
    "**Back:** In modern overparameterized models, test error can decrease even as models become more complex (beyond the interpolation point). Explicit regularization or early stopping helps navigate this curve, avoiding the peak at the interpolation threshold where models fit training data perfectly but generalize poorly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
