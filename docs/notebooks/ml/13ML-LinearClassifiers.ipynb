{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa8d630",
   "metadata": {},
   "source": [
    "## Bayes' Theorem Formula\n",
    "**Front:** Write Bayes' theorem in probability notation. <br/>\n",
    "**Back:** $$p(\\theta | x) = \\frac{p(x | \\theta) \\cdot p(\\theta)}{p(x)}$$\n",
    "\n",
    "* Posterior = Likelihood * prior / evidence\n",
    "* Ps(Posterior & Prior) like $\\theta$, likelihood dislikes $\\theta$ : $\\rightarrow \\begin{cases} \\text{prior} & p(\\theta) \\\\ \\text{posterior} & p(\\theta|x) \\\\ \\text{likelihood} & p(x|\\theta)\\end{cases} $\n",
    "\n",
    "## Prior Probability\n",
    "**Front:** What is $p(\\theta)$ called in Bayes' theorem? <br/>\n",
    "**Back:** The **prior probability** - our initial belief about $\\theta$ before seeing any data.\n",
    "\n",
    "* Ps(Posterior & Prior) like $\\theta$, likelihood dislikes $\\theta$ : $\\rightarrow \\begin{cases} \\text{prior} & p(\\theta) \\\\ \\text{posterior} & p(\\theta|x) \\\\ \\text{likelihood} & p(x|\\theta)\\end{cases} $\n",
    "\n",
    "## Probability Basics\n",
    "**Front:** What is $p(\\theta | x)$ called in Bayes' theorem? <br/>\n",
    "**Back:** The **posterior probability** - our updated belief about parameters $\\theta$ after seeing data $x$.\n",
    "\n",
    "* Ps(Posterior & Prior) like $\\theta$, likelihood dislikes $\\theta$ : $\\rightarrow \\begin{cases} \\text{prior} & p(\\theta) \\\\ \\text{posterior} & p(\\theta|x) \\\\ \\text{likelihood} & p(x|\\theta)\\end{cases} $\n",
    "\n",
    "## Likelihood\n",
    "**Front:** What is $p(x | \\theta)$ called in Bayes' theorem? <br/>\n",
    "**Back:** The **likelihood** - how probable the observed data $x$ is given parameters $\\theta$.\n",
    "\n",
    "* Ps(Posterior & Prior) like $\\theta$, likelihood dislikes $\\theta$ : $\\rightarrow \\begin{cases} \\text{prior} & p(\\theta) \\\\ \\text{posterior} & p(\\theta|x) \\\\ \\text{likelihood} & p(x|\\theta)\\end{cases} $\n",
    "\n",
    "## MLE (Maximum Likelihood Estimation)\n",
    "**Front:** What does MLE find? Write its formula. <br/>\n",
    "**Back:** The $\\theta$ that maximizes the likelihood: $$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} p(x | \\theta)$$\n",
    "It assumes $\\theta$ is fixed but unknown.\n",
    "\n",
    "## MAP (Maximum A Posteriori)\n",
    "**Front:** What does MAP find? Write its formula. <br/>\n",
    "**Back:** The $\\theta$ that maximizes the posterior: $$\\hat{\\theta}_{MAP} = \\arg\\max_{\\theta} p(\\theta | x) = \\arg\\max_{\\theta} p(x | \\theta)p(\\theta)$$\n",
    "It assumes $\\theta$ is random with a prior distribution.\n",
    "\n",
    "## MLE vs MAP Core Difference\n",
    "**Front:** What's the fundamental difference between MLE and MAP? <br/>\n",
    "**Back:** MLE uses only likelihood: $\\arg\\max p(x|\\theta)$. MAP adds prior: $\\arg\\max p(x|\\theta)p(\\theta)$. MAP is \"MLE with regularization\" from prior belief.\n",
    "\n",
    "* MAP multiplies $p(\\theta)$ (and p(x) is for normalization btw)so: $ \\begin{cases} p(x|\\theta)p(\\theta) & Posterior\\\\ p(\\theta|x) & prior\\end{cases} $ \n",
    "\n",
    "## Generative Models\n",
    "**Front:** What question do generative models answer? <br/>\n",
    "**Back:** \"What does each class look like?\" They model $p(x|y)$ - the probability of features given the class.\n",
    "\n",
    "* Generative models learn the data($x$), Discriminants focus on classification($y$) : $\\rightarrow \\begin{cases} \\text{Generative} & \\text{learns} \\quad p(\\text{data|class})=p(x|y)=p(x|\\theta) \\\\ \\text{Discriminative} & \\text{learns} \\quad p(\\text{class|data})=p(y|x)=p(\\theta|x)\\end{cases} $\n",
    "\n",
    "\n",
    "## Discriminative Models\n",
    "**Front:** What question do discriminative models answer? <br/>\n",
    "**Back:** \"How do we tell classes apart?\" They model $p(y|x)$ - the probability of class given the features.\n",
    "\n",
    "* Generative models learn the data($x$), Discriminants focus on classification($y$) : $\\rightarrow \\begin{cases} \\text{Generative} & \\text{learns} \\quad p(\\text{data|class})=p(x|y)=p(x|\\theta) \\\\ \\text{Discriminative} & \\text{learns} \\quad p(\\text{class|data})=p(y|x)=p(\\theta|x)\\end{cases} $\n",
    "\n",
    "## Generative Model Formula\n",
    "**Front:** What does a generative model compute for classification? <br/>\n",
    "**Back:**  by modeling $p(x|y)$ and $p(y)$, then using Bayes' theorem.\n",
    "\n",
    "$p(y|x) = \\frac{p(x|y) \\cdot p(y)}{p(x)}$\n",
    "\n",
    "* Generative models learn the data($x$), Discriminants focus on classification($y$) : $\\rightarrow \\begin{cases} \\text{Generative} & \\text{learns} \\quad p(\\text{data|class})=p(x|y)=p(x|\\theta) \\\\ \\text{Discriminative} & \\text{learns} \\quad p(\\text{class|data})=p(y|x)=p(\\theta|x)\\end{cases} $\n",
    "\n",
    "## Discriminative Model Formula\n",
    "**Front:** What does a discriminative model directly model? <br/>\n",
    "**Back:** It directly models $p(y|x)$, ignoring $p(x|y)$ and $p(x)$.\n",
    "\n",
    "* Generative models learn the data($x$), Discriminants focus on classification($y$) : $\\rightarrow \\begin{cases} \\text{Generative} & \\text{learns} \\quad p(\\text{data|class})=p(x|y)=p(x|\\theta) \\\\ \\text{Discriminative} & \\text{learns} \\quad p(\\text{class|data})=p(y|x)=p(\\theta|x)\\end{cases} $\n",
    "\n",
    "## Generative Model Example: Spam Detection\n",
    "**Front:** How would a generative model detect spam? <br/>\n",
    "**Back:** 1. Learn $p(\\text{words}|\\text{spam})$ and $p(\\text{words}|\\text{ham})$. 2. Learn $p(\\text{spam})$ and $p(\\text{ham})$. 3. Classify using Bayes: $p(\\text{spam}|\\text{words}) \\propto p(\\text{words}|\\text{spam})p(\\text{spam})$.\n",
    "\n",
    "## Discriminative Model Example: Spam Detection\n",
    "**Front:** How would a discriminative model detect spam? <br/>\n",
    "**Back:** Directly learn $p(\\text{spam}|\\text{words}) = \\sigma(\\theta^T \\cdot \\text{features}(\\text{words}))$ where $\\sigma$ is sigmoid. Ignore what spam/ham individually look like.\n",
    "\n",
    "## Generative Models Can Generate Data\n",
    "**Front:** What can generative models do that discriminative ones cannot? <br/>\n",
    "**Back:** Generate/sample new data points for each class because they learn $p(x|y)$.\n",
    "\n",
    "## Discriminative Models Focus on Boundaries\n",
    "**Front:** What do discriminative models focus on? <br/>\n",
    "**Back:** Only the decision boundary between classes, not the full data distribution.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f66220",
   "metadata": {},
   "source": [
    "## Intro to Classification Approaches\n",
    "**Front:** What are the two fundamental approaches for building a classifier? <br/>\n",
    "**Back:** 1. **Discriminative:** Find a decision boundary that directly separates the classes. 2. **Generative:** Model the class-conditional distribution $p(x|C_k)$ for each class, then assign a new point to the class with the highest posterior probability $p(C_k|x)$ using Bayes' theorem.\n",
    "\n",
    "## Generative vs. Discriminative Methods\n",
    "**Front:** How do generative and discriminative models differ conceptually? Provide one example of each. <br/>\n",
    "**Back:** **Generative** models learn the joint distribution $p(x, C_k)$ of inputs and labels (e.g., Naive Bayes, Linear Discriminant Analysis). **Discriminative** models learn the conditional distribution $p(C_k|x)$ or directly map inputs to decision boundaries (e.g., Logistic Regression/Linear Classifier, Perceptron, SVM).\n",
    "\n",
    "## Linear Classifier Main Condition\n",
    "**Front:** What is the primary condition required for a *linear* classifier to work perfectly? <br/>\n",
    "**Back:** The data must be **linearly separable/classifiable**. This means a hyperplane exists that can perfectly separate the data points of different classes in the feature space.\n",
    "\n",
    "## Linear Binary Classification\n",
    "**Front:** In a 2D feature space, what is the general form of the linear function used for binary classification? <br/>\n",
    "**Back:** $f(x) = w_0 + w_1x_1 + w_2x_2$, where $x=[x_1, x_2]^T$. The decision boundary is the line defined by $f(x)=0$. Points are classified based on the sign of $f(x)$.\n",
    "\n",
    "## Classification Score as Distance\n",
    "**Front:** For a data point $x$, what does the quantity $W^Tx$ (or $w^Tx + w_0$) represent geometrically? <br/>\n",
    "**Back:** It is proportional to the signed **distance** from the point $x$ to the decision boundary hyperplane, scaled by the norm of $w$. The sign indicates which side of the boundary the point lies on.\n",
    "\n",
    "## SSE Cost Function for Classification\n",
    "**Front:** Why is directly applying the Sum of Squared Errors (SSE) cost function from regression, $J(W) = \\sum_i (y^{(i)} - W^Tx^{(i)})^2$, problematic for classification? <br/>\n",
    "**Back:** It assumes target $y^{(i)}$ is a continuous value. In binary classification, targets (e.g., +1/-1) are discrete. Minimizing SSE does not guarantee the sign of $W^Tx^{(i)}$ matches $y^{(i)}$, which is the primary goal.\n",
    "\n",
    "## Sign Function Inside SSE\n",
    "**Front:** How can we modify the SSE cost to make it more suitable for classification? What function is introduced? <br/>\n",
    "**Back:** Replace the continuous prediction $W^Tx^{(i)}$ with the discrete class prediction: $\\text{sign}(W^Tx^{(i)})$. The cost becomes $J(W) = \\sum_i (y^{(i)} - \\text{sign}(W^Tx^{(i)}))^2$. However, this is non-differentiable and difficult to optimize directly.\n",
    "\n",
    "## Perceptron Classifier - Core Idea\n",
    "**Front:** What is the Perceptron algorithm's fundamental rule for classification? <br/>\n",
    "**Back:** For a data point $(x^{(i)}, y^{(i)})$ where $y^{(i)} \\in \\{-1, +1\\}$, the prediction is $\\hat{y} = \\text{sign}(W^Tx^{(i)})$. The model is correct if $W^Tx^{(i)}y^{(i)} > 0$.\n",
    "\n",
    "## Perceptron Cost Function\n",
    "**Front:** What is the Perceptron cost function? Describe it in words and mathematically. <br/>\n",
    "**Back:** It sums a measure of \"error\" over all **misclassified** points only. The cost is $J(W) = -\\sum_{i \\in \\mathcal{M}} W^Tx^{(i)}y^{(i)}$, where $\\mathcal{M}$ is the set of misclassified points for which $W^Tx^{(i)}y^{(i)} \\le 0$.\n",
    "\n",
    "## Perceptron and Distance\n",
    "**Front:** How does the Perceptron cost function incorporate the distance of a misclassified point to the boundary? <br/>\n",
    "**Back:** For a misclassified point, $W^Tx^{(i)}y^{(i)}$ is negative. Its magnitude $|W^Tx^{(i)}|$ is proportional to the point's distance to the boundary. Therefore, the cost $J(W)$ sums the *signed negative distances*, penalizing misclassified points more severely the farther they are from the boundary.\n",
    "\n",
    "## Perceptron Learning Rule via Gradient Descent\n",
    "**Front:** Derive the parameter update rule for the Perceptron using (Stochastic) Gradient Descent on its cost function. <br/>\n",
    "**Back:**<br/> \n",
    "For a single misclassified point $(x^{(i)}, y^{(i)})$, $J^{(i)}(W) = -W^Tx^{(i)}y^{(i)}$. <br/>\n",
    "The gradient is $\\nabla_W J^{(i)} = -x^{(i)}y^{(i)}$.<br/>\n",
    "The GD update is: $W := W + \\eta x^{(i)}y^{(i)}$, where $\\eta$ is the learning rate. \n",
    "\n",
    "* Correctly classified points do not trigger an update.\n",
    "\n",
    "## Perceptron Convergence & Optimality\n",
    "**Front:** What are the conditions for the Perceptron algorithm to converge to a solution with zero training error? <br/>\n",
    "**Back:** Two main conditions: 1. The training data must be **linearly separable**. 2. The learning rate $\\eta$ must be sufficiently small. Under these conditions, the Perceptron Convergence Theorem guarantees the algorithm will find a separating hyperplane in a finite number of steps.\n",
    "\n",
    "## Generalization & Pitfalls of the Perceptron\n",
    "**Front:** What are key generalization considerations and pitfalls of the Perceptron? <br/>\n",
    "**Back:** **Pitfalls:** 1. If data is not linearly separable, the algorithm will never converge (oscillates). 2. It finds *a* separating plane, not necessarily the *best* one (max-margin). 3. The zero-one loss (cost) is non-differentiable. 4. The solution depends on the initialization and order of data presentation (SGD). **Consideration:** Simplicity makes it prone to high variance if data is noisy but separable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20f7c6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## MLE vs MAP vs Generative vs Discriminative: Core Confusion\n",
    "**Front:** Are MLE and MAP the same as generative and discriminative models? <br/>\n",
    "**Back:** **No!** MLE/MAP are **parameter estimation methods**. Generative/discriminative are **model types**. These are independent choices.\n",
    "\n",
    "## Parameter Estimation Methods\n",
    "**Front:** What do MLE and MAP determine about a model? <br/>\n",
    "**Back:** How the model's **parameters are learned** from data. They don't define whether a model is generative or discriminative.\n",
    "\n",
    "## Model Types\n",
    "**Front:** What do generative and discriminative determine about a model? <br/>\n",
    "**Back:** What **probability distribution** the model learns: $p(x|y)$ (generative) or $p(y|x)$ (discriminative).\n",
    "\n",
    "## The Independence Clarification\n",
    "**Front:** Can a generative model use MLE? Can a discriminative model use MAP? <br/>\n",
    "**Back:** **Yes to both!** Generative models can use MLE or MAP. Discriminative models can use MLE or MAP. These are separate choices.\n",
    "\n",
    "## Example 1: Naive Bayes\n",
    "**Front:** What estimation methods can Naive Bayes (generative) use? <br/>\n",
    "**Back:** **MLE**: $\\arg\\max p(\\text{features}|\\text{class})$ <br/>\n",
    "**or MAP**: $\\arg\\max p(\\text{features}|\\text{class})p(\\text{class})$\n",
    "\n",
    "## Example 2: Logistic Regression\n",
    "**Front:** What estimation methods can logistic regression (discriminative) use? <br/>\n",
    "**Back:** **MLE**: $\\arg\\max p(\\text{class}|\\text{features},\\theta)$ <br/>\n",
    "**or MAP**: $\\arg\\max p(\\text{class}|\\text{features},\\theta)p(\\theta)$ (equivalent to adding regularization)\n",
    "\n",
    "## Quick Diagnostic Question\n",
    "**Front:** How to check if something is MLE/MAP vs generative/discriminative? <br/>\n",
    "**Back:** Ask: 1. \"Does it model $p(x|y)$ or $p(y|x)$?\" → generative/discriminative. 2. \"Does it include a prior on parameters?\" → MAP (yes) vs MLE (no).\n",
    "\n",
    "## Common Pitfall: Assuming MAP = Generative\n",
    "**Front:** What's wrong with saying \"MAP is generative\"? <br/>\n",
    "**Back:** MAP is just parameter estimation with a prior. A **discriminative** model (like logistic regression) can use MAP estimation when regularized.\n",
    "\n",
    "## Special Case: Regularization Connection\n",
    "**Front:** How does regularization relate to these concepts? <br/>\n",
    "**Back:** Adding L2 regularization to discriminative models = MAP estimation with Gaussian prior. So MAP isn't exclusive to generative models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd9974",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Naive Bayes Classification\n",
    "**Front:** Is Naive Bayes generative or discriminative? Why? <br/>\n",
    "**Back:** **Generative**. It models $p(x|y)$ with the \"naive\" assumption that features are independent given the class: $p(x|y) = \\prod_i p(x_i|y)$.\n",
    "\n",
    "## Logistic Regression Classification\n",
    "**Front:** Is logistic regression generative or discriminative? Why? <br/>\n",
    "**Back:** **Discriminative**. It directly models $p(y|x) = \\sigma(\\theta^Tx)$ where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "## SVM Classification\n",
    "**Front:** Is SVM generative or discriminative? Why? <br/>\n",
    "**Back:** **Discriminative**. It finds the optimal separating hyperplane without modeling class distributions.\n",
    "\n",
    "## LDA (Linear Discriminant Analysis)\n",
    "**Front:** Is LDA generative or discriminative? Why? <br/>\n",
    "**Back:** **Generative**. It models each class as a Gaussian distribution with different means but same covariance matrix.\n",
    "\n",
    "## QDA (Quadratic Discriminant Analysis)\n",
    "**Front:** How is QDA different from LDA? <br/>\n",
    "**Back:** QDA is also **generative** but allows each class to have its own covariance matrix, giving quadratic decision boundaries.\n",
    "\n",
    "## Decision Trees Classification\n",
    "**Front:** Are decision trees generative or discriminative? <br/>\n",
    "**Back:** **Discriminative**. They build rules to separate classes without modeling $p(x|y)$.\n",
    "\n",
    "## Neural Networks Classification\n",
    "**Front:** Are standard neural networks for classification generative or discriminative? <br/>\n",
    "**Back:** **Discriminative**. They learn complex $p(y|x)$ through multiple layers of transformations.\n",
    "\n",
    "## k-Nearest Neighbors\n",
    "**Front:** Is k-NN generative or discriminative? <br/>\n",
    "**Back:** Neither strictly, but **discriminative in spirit**. It's non-parametric and classifies based on nearby examples without modeling distributions.\n",
    "\n",
    "## Bayesian Networks\n",
    "**Front:** Are Bayesian networks generative or discriminative? <br/>\n",
    "**Back:** **Generative**. They model the joint probability distribution $p(x,y)$ using a directed graph structure.\n",
    "\n",
    "## Hidden Markov Models\n",
    "**Front:** Are HMMs generative or discriminative? <br/>\n",
    "**Back:** **Generative**. They model sequences of observations as being generated by hidden states.\n",
    "\n",
    "## Linear Regression\n",
    "**Front:** Is linear regression generative or discriminative? <br/>\n",
    "**Back:** **Neither** in traditional sense. It's regression, not classification. But it models $p(y|x)$ with continuous $y$, making it discriminative-like.\n",
    "\n",
    "## PCA (Principal Component Analysis)\n",
    "**Front:** Is PCA generative or discriminative? <br/>\n",
    "**Back:** **Neither**. It's unsupervised dimensionality reduction with no concept of labels or classification boundaries.\n",
    "\n",
    "## Clustering Methods\n",
    "**Front:** Are clustering methods (k-means, hierarchical) generative or discriminative? <br/>\n",
    "**Back:** **Neither**. They're unsupervised. If clusters are treated as \"classes,\" the approach is generative-like as they model $p(x|\\text{cluster})$.\n",
    "\n",
    "## Parzen Window / Kernel Density Estimation\n",
    "**Front:** Can Parzen window methods be generative or discriminative? <br/>\n",
    "**Back:** **Generative** when used for density estimation. The Parzen window classifier estimates $p(x|y)$ for each class.\n",
    "\n",
    "## Regularization Connection to MAP\n",
    "**Front:** How is L2 regularization in logistic regression related to MAP? <br/>\n",
    "**Back:** Adding L2 regularization $\\lambda\\|\\theta\\|^2$ is equivalent to MAP estimation with a Gaussian prior on $\\theta$.\n",
    "\n",
    "## When to Use Generative Models\n",
    "**Front:** When should you prefer generative models? <br/>\n",
    "**Back:** 1. When you need to generate data. 2. With missing data. 3. Small datasets (priors help). 4. When $p(x|y)$ is naturally simple.\n",
    "\n",
    "## When to Use Discriminative Models\n",
    "**Front:** When should you prefer discriminative models? <br/>\n",
    "**Back:** 1. Pure classification task. 2. Large datasets. 3. When decision boundary is simpler than full distributions.\n",
    "\n",
    "## Common Pitfall: MAP vs MLE Assumptions\n",
    "**Front:** What's a common mistake when choosing between MAP and MLE? <br/>\n",
    "**Back:** Using MAP without justifying the prior, or using MLE when you actually have useful prior knowledge about parameters.\n",
    "\n",
    "## Common Pitfall: Naive Bayes Independence\n",
    "**Front:** What's the main limitation of Naive Bayes? <br/>\n",
    "**Back:** The \"naive\" assumption that features are independent given the class is often false in real data, though it still works surprisingly well.\n",
    "\n",
    "## Special Consideration: Neural Networks Can Be Both\n",
    "**Front:** Can neural networks be both generative and discriminative? <br/>\n",
    "**Back:** Yes! Standard NN classifiers are discriminative. But GANs and VAEs are generative networks that learn data distributions.\n",
    "\n",
    "## Special Consideration: Bayesian Methods Framework\n",
    "**Front:** How do Bayesian methods relate to generative/discriminative categories? <br/>\n",
    "**Back:** Bayesian methods provide a framework that's naturally generative, as they model full distributions with priors. But Bayesian logistic regression is discriminative with Bayesian inference on parameters.\n",
    "\n",
    "## Practical Tip: Testing Model Type\n",
    "**Front:** Quick test: is a model generative or discriminative? <br/>\n",
    "**Back:** Ask: \"Can it generate/sample new data points for each class?\" Yes → Generative. No → Discriminative.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
