## LDA Use Cases

**Front:** What are the two primary use cases of Linear Discriminant Analysis (LDA)?
**Back:**

1. **Supervised Dimensionality Reduction:** Projects data onto a lower-dimensional space while preserving as much class-discriminatory information as possible. The new axes are linear combinations of original features.
2. **Classification:** Provides a simple, linear classifier by defining a decision boundary (hyperplane) based on the projected class means and shared covariance.

## LDA Definition & Goal

**Front:** What is the core goal of Linear Discriminant Analysis (LDA)?
**Back:**
LDA is a supervised method. Its goal is to find a projection (a direction or set of axes) that **maximizes the separation between the means of different classes** while **minimizing the variance (scatter) within each class** in the projected space.

$$
\text{Goal: Find } \mathbf{w} \text{ that maximizes class separation relative to class compactness.}
$$

## Geometric Concept: Projection in Detail

**Front:** In LDA, given $\mathbf{x} \in \mathbb{R}^d$ and $\mathbf{w} \in \mathbb{R}^d$, what does the operation $\mathbf{w}^T\mathbf{x}$ compute geometrically? What are the dimensions of the input and output?
**Back:**
Geometrically, $\mathbf{w}^T\mathbf{x}$ computes the **scalar projection** of the data point $\mathbf{x}$ onto the vector $\mathbf{w}$. This operation:

1. Takes a high-dimensional point $\mathbf{x}$ in the original feature space (dimension $d$).
2. Projects it onto the 1-dimensional line defined by the direction of $\mathbf{w}$.
3. Outputs a **single scalar value** representing the coordinate of that projection along the line.

- **Dimensions:** $\mathbf{x}$: $(d \times 1)$, $\mathbf{w}$: $(d \times 1)$, $\mathbf{w}^T\mathbf{x}$: $(1 \times 1)$ (a scalar)
- If $\mathbf{w}$ is a unit vector ($\|\mathbf{w}\| = 1$), then $\mathbf{w}^T\mathbf{x}$ equals the **signed length/magnitude** of the orthogonal projection.
- The sign indicates on which "side" of the origin (relative to $\mathbf{w}$'s direction) the projection lies.
- This projection reduces dimensionality from $d$ to $1$, which is the core of LDA's dimensionality reduction aspect.

**Visual:** Imagine a 2D cloud of points. $\mathbf{w}$ defines a line through the origin. Each point $\mathbf{x}$ is "dropped" perpendicularly onto this line. $\mathbf{w}^T\mathbf{x}$ is the coordinate of that landing point along the line.

$$
\text{Projection: } \mathbf{w}^T\mathbf{x} = \|\mathbf{w}\|\|\mathbf{x}\|\cos\theta \quad\text{(where } \theta \text{ is the angle between } \mathbf{w} \text{ and } \mathbf{x})
$$

## Visualizing the Optimal W Vector

**Front:** In a 2D scatter plot of two classes, how would you visually identify a good projection vector $\mathbf{w}$ for LDA?
**Back:**
A good $\mathbf{w}$ vector, when the data is projected onto the line in its direction, will result in: 1) Two clusters of projected points (one per class) that are **far apart from each other** (large inter-class distance). 2) Each cluster being as **tight or compact** as possible (small intra-class spread).

## First Rule: Maximizing Mean Separation

**Front:** What is the first, naive objective function for LDA, and what is its major flaw?
**Back:**
The naive rule aims to **maximize the squared distance between projected class means**.

$$
\underset{\mathbf{w}}{\argmax} (\tilde{\mu}_1 - \tilde{\mu}_2)^2 \quad \text{, where } \quad \tilde{\mu}_c = \mathbf{w}^T\mathbf{\mu}_c
$$

**Major Flaw:** It doesn't consider within-class scatter. The solution could simply blow up the scale of $\mathbf{w}$, and it's sensitive to classes with large internal variance.

$$
\underset{\mathbf{w}}{\arg\max} (\mathbf{w}^T\mathbf{\mu}_1 - \mathbf{w}^T\mathbf{\mu}_2)^2 = \underset{\mathbf{w}}{\arg\max} [\mathbf{w}^T(\mathbf{\mu}_1-\mathbf{\mu}_2)(\mathbf{\mu}_1-\mathbf{\mu}_2)^T\mathbf{w}]
$$

## Pros and Cons of the First Rule

**Front:** List one pro and one critical con of the LDA objective $\arg\max (\tilde{\mu}_1 - \tilde{\mu}_2)^2$.
**Back:**
**Pro:** Simple and intuitive. Directly targets class separation.
**Critical Con:** It is **not scale-invariant**. Multiplying $\mathbf{w}$ by a scalar increases the objective without actually improving the separation relative to the data's natural spread. It ignores within-class variance, leading to poor projections if classes have high scatter.

## Second Rule: Fisher's Criterion

**Front:** What is Fisher's Linear Discriminant criterion (J(w)), and how does it fix the first rule's flaw?
**Back:**
Fisher's criterion introduces a normalization term using within-class scatter.

$$
J(\mathbf{w}) = \frac{(\tilde{\mu}_1 - \tilde{\mu}_2)^2}{\tilde{s}_1^2 + \tilde{s}_2^2}
$$

It fixes the scale problem by maximizing the ratio of **between-class variance** to **within-class variance**. The denominator $\tilde{s}_c^2$ measures the scatter (variance) of projected points within class $c$.

## Path to the Generalized Objective Function

**Front:** How do we get from $J(\mathbf{w}) = \frac{(\tilde{\mu}_1 - \tilde{\mu}_2)^2}{\tilde{s}_1^2 + \tilde{s}_2^2}$ to the standard form $J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}$?
**Back:**

1. **Between-class scatter (numerator):** $(\mathbf{w}^T\mathbf{\mu}_1 - \mathbf{w}^T\mathbf{\mu}_2)^2 = \mathbf{w}^T(\mathbf{\mu}_1-\mathbf{\mu}_2)(\mathbf{\mu}_1-\mathbf{\mu}_2)^T\mathbf{w} = \mathbf{w}^T \mathbf{S}_B \mathbf{w}$, where $\mathbf{S}_B$ is the between-class scatter matrix.
2. **Within-class scatter (denominator):** $\tilde{s}_1^2 + \tilde{s}_2^2 = \sum_{n \in C_1}(\mathbf{w}^T\mathbf{x}_n - \tilde{\mu}_1)^2 + \sum_{n \in C_2}(\mathbf{w}^T\mathbf{x}_n - \tilde{\mu}_2)^2 = \mathbf{w}^T \mathbf{S}_W \mathbf{w}$, where $\mathbf{S}_W$ is the total within-class scatter matrix.

## Fisher's Objective Function

**Front:** What is the standard matrix form of Fisher's Linear Discriminant objective function?
**Back:**
The objective is to maximize the ratio of between-class scatter to within-class scatter in the projected space:

* *make the projections on w as **far** as possible and as **compact** as possible*

$$
J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}
$$

where $\mathbf{S}_B$ is the between-class scatter matrix and $\mathbf{S}_W$ is the within-class scatter matrix.

## Optimizing the Ratio

**Front:** To find the optimal projection vector $\mathbf{w}$, what mathematical problem arises from maximizing $J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}$?
**Back:**
Taking the derivative and setting it to zero leads to the generalized eigenvalue problem:

$$
\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w} \\
(\mathbf{S}_W^{-1} \mathbf{S}_B) \mathbf{w} = \lambda \mathbf{w}
$$

* This means the optimal $\mathbf{w}$ is an eigenvector of $\mathbf{S}_W^{-1}\mathbf{S}_B$.

## Two-Class Special Case

**Front:** In the two-class case, what simpler form does the between-class scatter matrix $\mathbf{S}_B$ take?
**Back:**
For two classes, $\mathbf{S}_B$ simplifies to an outer product:

$$
\mathbf{S}_B = (\mathbf{\mu}_1 - \mathbf{\mu}_2)(\mathbf{\mu}_1 - \mathbf{\mu}_2)^T
$$

where $\mathbf{\mu}_1$ and $\mathbf{\mu}_2$ are the class mean vectors.

## Key LDA Solution

**Front:** What is the closed-form solution for the optimal LDA projection direction $\mathbf{w}$ in the two-class case?
**Back:**
For two classes, the solution to $\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w}$ gives:

$$
\mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)
$$

The optimal direction is proportional to $\mathbf{S}_W^{-1}$ times the difference between class means.

## Geometric Interpretation of Solution

**Front:** Why isn't the optimal LDA direction simply the line connecting the class means $(\mathbf{\mu}_1 - \mathbf{\mu}_2)$?
**Back:**
The multiplication by $\mathbf{S}_W^{-1}$ accounts for the shape and correlation of the data. If features are correlated or have different variances, $\mathbf{S}_W^{-1}$ "whitens" the space, stretching directions of low variance and compressing directions of high variance to create spherical classes before finding the best separation.

## 5-Step Algorithm for 2D Binary LDA

**Front:** Outline the 5 key steps to compute the LDA projection vector $\mathbf{w}$ for a 2D, two-class problem.
**Back:**

1. **Compute Class Means:**
   $\mathbf{\mu}_1 = \frac{1}{N_1}\sum_{n \in C_1} \mathbf{x}_n$, $\mathbf{\mu}_2 = \frac{1}{N_2}\sum_{n \in C_2} \mathbf{x}_n$.
2. **Compute Within-Class Scatter Matrices:**
   $\mathbf{S}_1 = \sum_{n \in C_1} (\mathbf{x}_n - \mathbf{\mu}_1)(\mathbf{x}_n - \mathbf{\mu}_1)^T$, $\mathbf{S}_2$ similarly.
3. **Compute Total Within-Class Scatter:**
   $\mathbf{S}_W = \mathbf{S}_1 + \mathbf{S}_2$.
4. **Compute Between-Class Scatter:**
   $\mathbf{S}_B = (\mathbf{\mu}_1 - \mathbf{\mu}_2)(\mathbf{\mu}_1 - \mathbf{\mu}_2)^T$.
5. **Solve for $\mathbf{w}$:** $\mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)$. In practice, compute $\mathbf{w} = \mathbf{S}_W^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)$ and then normalize it (e.g., to unit length).

## Pitfalls & Special Considerations

**Front:** What are two critical pitfalls or considerations when applying LDA?
**Back:**

1. **Singular $\mathbf{S}_W$ Problem:** If the number of samples $N$ is less than the number of features $d$, or if features are linearly dependent, $\mathbf{S}_W$ is singular and cannot be inverted. **Solution:** Use PCA first for dimensionality reduction, or apply regularization (e.g., $\mathbf{S}_W + \lambda\mathbf{I}$).
2. **Assumption of Common Covariance:** LDA assumes all classes share the same covariance structure (i.e., $\mathbf{S}_1 \approx \mathbf{S}_2$). If this is severely violated (classes have very different shapes/spreads), the projection may be suboptimal. QDA (Quadratic DA) relaxes this assumption.
3. **Normalization:** The magnitude of $\mathbf{w}$ from $\mathbf{S}_W^{-1}(\mathbf{\mu}_1-\mathbf{\mu}_2)$ is arbitrary. The *direction* is what matters. For stability, often normalize $\mathbf{w}$ to unit length.
