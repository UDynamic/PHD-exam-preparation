{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c8ae56",
   "metadata": {},
   "source": [
    "Hey there! üòä  \n",
    "Let‚Äôs take a friendly and clear look at the **Steady State** in **Markov chains**, and how it connects to **pattern recognition** and **machine learning**. I‚Äôll also explain the two equations you mentioned:  \n",
    "1. **P‚Çô = P‚ÇÄ √ó T‚Åø**  \n",
    "2. **P √ó T = P**\n",
    "\n",
    "---\n",
    "\n",
    "### üåü What is a Markov Chain?\n",
    "\n",
    "A **Markov chain** is a mathematical system that undergoes transitions from one state to another according to certain probabilistic rules. The key idea is the **Markov property**: the next state depends only on the current state, not on the sequence of events that preceded it.\n",
    "\n",
    "This is super useful in **machine learning** for modeling sequences like:\n",
    "- Speech recognition\n",
    "- Text generation\n",
    "- DNA sequences\n",
    "- Weather patterns\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Equation 1: **P‚Çô = P‚ÇÄ √ó T‚Åø**\n",
    "\n",
    "Let‚Äôs break it down:\n",
    "\n",
    "- **P‚ÇÄ** is the **initial probability vector** ‚Äî it tells you the probability of being in each state at the beginning.\n",
    "- **T** is the **transition matrix** ‚Äî it shows the probability of moving from one state to another.\n",
    "- **T‚Åø** means the transition matrix raised to the power of n ‚Äî this gives you the probabilities after n steps.\n",
    "- **P‚Çô** is the probability vector after n steps.\n",
    "\n",
    "This equation is like a time machine for your system. It shows how the probabilities of being in each state evolve over time.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Equation 2: **P √ó T = P**\n",
    "\n",
    "This is the **Steady State Equation**.\n",
    "\n",
    "- **P** is the **steady-state probability vector**.\n",
    "- **T** is the transition matrix.\n",
    "- The equation says: if you multiply the steady-state vector **P** by the transition matrix **T**, you get the same vector **P** back.\n",
    "\n",
    "This means the system has reached a **steady state** ‚Äî the probabilities no longer change with time. It's like the system has \"settled down\" and is in balance.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why is this important in Machine Learning?\n",
    "\n",
    "In **pattern recognition** and **machine learning**, Markov chains are used to model sequences and predict future states based on the current one.\n",
    "\n",
    "- In **Hidden Markov Models (HMMs)**, we use these concepts to model sequences like speech, DNA, or text.\n",
    "- The **steady state** helps us understand the long-term behavior of a system.\n",
    "- In **reinforcement learning**, steady states can help us find optimal policies.\n",
    "- In **clustering** and **topic modeling**, steady states can reveal the most likely groupings or topics.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Example (Simple 2-State Markov Chain)\n",
    "\n",
    "Let‚Äôs say we have a system with two states: **Sunny** and **Rainy**.\n",
    "\n",
    "The **transition matrix T** might look like this:\n",
    "\n",
    "```\n",
    "T = [0.8  0.2\n",
    "     0.3  0.7]\n",
    "```\n",
    "\n",
    "This means:\n",
    "- If today is **Sunny**, there's an 80% chance it will be **Sunny** tomorrow.\n",
    "- If today is **Rainy**, there's a 70% chance it will be **Rainy** tomorrow.\n",
    "\n",
    "Now, if we keep multiplying the transition matrix by itself (T‚Åø), the probabilities will eventually stabilize ‚Äî that‚Äôs the **steady state**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary Table\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **P‚Çô = P‚ÇÄ √ó T‚Åø** | Shows how the probability distribution evolves over time. |\n",
    "| **P √ó T = P** | The steady-state equation ‚Äî the system has reached a balance. |\n",
    "| **Steady State** | A long-term equilibrium where probabilities no longer change. |\n",
    "| **Markov Property** | The future depends only on the present, not the past. |\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ì Want to Try It Out?\n",
    "\n",
    "If you'd like, I can:\n",
    "- Show you how to compute the steady state using code (Python or MATLAB).\n",
    "- Walk you through a real-world example (like weather prediction).\n",
    "- Help you visualize the transition matrix and its powers.\n",
    "\n",
    "Just say the word! üòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f1189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e76ab79c",
   "metadata": {},
   "source": [
    "Absolutely! Let's dive deeper into the **Steady State** and related concepts in **Markov Chains**, using **mathematical formulations**, **examples**, and **real-world use cases** in **pattern recognition** and **machine learning**. I'll make sure to keep it thorough and precise.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 1. **Markov Chains: A Quick Recap**\n",
    "\n",
    "A **Markov Chain** is a **discrete-time stochastic process** that satisfies the **Markov property**:\n",
    "\n",
    "> The future state depends only on the current state, not on the sequence of events that preceded it.\n",
    "\n",
    "This is mathematically expressed as:\n",
    "\n",
    "$$\n",
    "P(X_{n+1} = x_{n+1} \\mid X_n = x_n, X_{n-1} = x_{n-1}, \\dots, X_0 = x_0) = P(X_{n+1} = x_{n+1} \\mid X_n = x_n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üìê 2. **Mathematical Formulation of Markov Chains**\n",
    "\n",
    "Let‚Äôs define:\n",
    "\n",
    "- **States**: A finite set of states $ S = \\{s_1, s_2, \\dots, s_N\\} $\n",
    "- **Transition Matrix $ T $**: An $ N \\times N $ matrix where $ T_{ij} = P(s_j \\mid s_i) $, the probability of transitioning from state $ s_i $ to $ s_j $.\n",
    "- **Initial State Distribution $ P_0 $**: A row vector where $ P_0(i) = P(X_0 = s_i) $\n",
    "\n",
    "Then, the **state distribution at step $ n $** is given by:\n",
    "\n",
    "$$\n",
    "P_n = P_0 \\cdot T^n\n",
    "$$\n",
    "\n",
    "This is the **first equation you mentioned**:\n",
    "$$\n",
    "P_n = P_0 T^n\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 3. **Steady State (Stationary Distribution)**\n",
    "\n",
    "A **steady state** (or **stationary distribution**) is a probability distribution $ \\pi $ such that:\n",
    "\n",
    "$$\n",
    "\\pi T = \\pi\n",
    "$$\n",
    "\n",
    "This is the **second equation you mentioned**:\n",
    "$$\n",
    "P T = P\n",
    "$$\n",
    "\n",
    "This means that if the system is in the steady state $ \\pi $, it will remain in that state no matter how many transitions occur.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ 4. **Finding the Steady State**\n",
    "\n",
    "To find the steady state $ \\pi $, we solve the system:\n",
    "\n",
    "$$\n",
    "\\pi T = \\pi\n",
    "$$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$$\n",
    "\\pi (T - I) = 0\n",
    "$$\n",
    "\n",
    "Where $ I $ is the identity matrix.\n",
    "\n",
    "Additionally, since $ \\pi $ is a probability distribution, we must have:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\pi_i = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 5. **Example: 2-State Markov Chain**\n",
    "\n",
    "Let‚Äôs take a simple example with two states: **Sunny (S)** and **Rainy (R)**.\n",
    "\n",
    "### Transition Matrix $ T $:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "0.8 & 0.2 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- From **Sunny**, it stays **Sunny** with 80% probability.\n",
    "- From **Rainy**, it stays **Rainy** with 70% probability.\n",
    "\n",
    "Let‚Äôs find the **steady state** $ \\pi = [\\pi_S, \\pi_R] $ such that:\n",
    "\n",
    "$$\n",
    "\\pi T = \\pi\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\pi_S & \\pi_R\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.8 & 0.2 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\pi_S & \\pi_R\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiplying:\n",
    "\n",
    "$$\n",
    "\\pi_S \\cdot 0.8 + \\pi_R \\cdot 0.3 = \\pi_S \\quad \\text{(1)} \\\\\n",
    "\\pi_S \\cdot 0.2 + \\pi_R \\cdot 0.7 = \\pi_R \\quad \\text{(2)}\n",
    "$$\n",
    "\n",
    "From (1):\n",
    "\n",
    "$$\n",
    "0.8\\pi_S + 0.3\\pi_R = \\pi_S \\Rightarrow 0.3\\pi_R = 0.2\\pi_S \\Rightarrow \\pi_R = \\frac{2}{3} \\pi_S\n",
    "$$\n",
    "\n",
    "Using the normalization condition $ \\pi_S + \\pi_R = 1 $:\n",
    "\n",
    "$$\n",
    "\\pi_S + \\frac{2}{3} \\pi_S = 1 \\Rightarrow \\frac{5}{3} \\pi_S = 1 \\Rightarrow \\pi_S = \\frac{3}{5}, \\quad \\pi_R = \\frac{2}{5}\n",
    "$$\n",
    "\n",
    "So the **steady state** is:\n",
    "\n",
    "$$\n",
    "\\pi = \\left[ \\frac{3}{5}, \\frac{2}{5} \\right]\n",
    "$$\n",
    "\n",
    "This means that in the long run, the system will be **Sunny 60% of the time** and **Rainy 40% of the time**, regardless of the starting distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 6. **Use Cases in Pattern Recognition & Machine Learning**\n",
    "\n",
    "### üîπ A. **Hidden Markov Models (HMMs)**\n",
    "\n",
    "In **HMMs**, the system has **hidden states** and **observed outputs**. The steady state helps in:\n",
    "- Estimating the most likely sequence of hidden states.\n",
    "- Training the model using the **Baum-Welch algorithm** (a type of EM algorithm).\n",
    "\n",
    "### üîπ B. **PageRank Algorithm (Google Search)**\n",
    "\n",
    "PageRank is a **Markov-based algorithm** that computes the **steady-state distribution** of a web graph. Each web page is a state, and links are transitions. The steady state gives the **importance** of each page.\n",
    "\n",
    "### üîπ C. **Reinforcement Learning**\n",
    "\n",
    "In **Markov Decision Processes (MDPs)**, the steady state helps in:\n",
    "- Finding the **long-term expected reward**.\n",
    "- Solving for the **optimal policy** using **Bellman equations**.\n",
    "\n",
    "### üîπ D. **Speech Recognition**\n",
    "\n",
    "Markov chains are used in **language models** to predict the next word in a sentence. The steady state helps in understanding the **long-term behavior** of the language model.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 7. **Python Code Example: Compute Steady State**\n",
    "\n",
    "Let‚Äôs compute the steady state for the 2-state example using Python.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Transition matrix\n",
    "T = np.array([\n",
    "    [0.8, 0.2],\n",
    "    [0.3, 0.7]\n",
    "])\n",
    "\n",
    "# Initial guess for steady state\n",
    "pi = np.array([0.5, 0.5])\n",
    "\n",
    "# Iterate until convergence\n",
    "for _ in range(100):\n",
    "    pi = pi @ T\n",
    "    if np.allclose(pi, pi @ T):\n",
    "        break\n",
    "\n",
    "print(\"Steady State (œÄ):\", pi)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Steady State (œÄ): [0.6 0.4]\n",
    "```\n",
    "\n",
    "This matches our earlier calculation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Markov Chain** | A stochastic process where the next state depends only on the current state. |\n",
    "| **Transition Matrix $ T $** | Describes the probability of moving from one state to another. |\n",
    "| **State Distribution $ P_n $** | Probability of being in each state at step $ n $: $ P_n = P_0 T^n $ |\n",
    "| **Steady State $ \\pi $** | A distribution that satisfies $ \\pi T = \\pi $; the system reaches equilibrium. |\n",
    "| **Use Cases** | HMMs, PageRank, Reinforcement Learning, Speech Recognition, etc. |\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ì Want to Go Further?\n",
    "\n",
    "If you're interested, I can:\n",
    "- Show you how to compute steady states for larger matrices.\n",
    "- Provide a **MATLAB/Python implementation** of HMMs.\n",
    "- Walk you through **Markov Decision Processes (MDPs)** and **Bellman equations**.\n",
    "- Help you simulate a **weather prediction system** using Markov chains.\n",
    "\n",
    "Let me know what you'd like to explore next! üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0cb696",
   "metadata": {},
   "source": [
    "Absolutely! Here's a **concise and comprehensive summary** of the **Steady State in Markov Chains**, including the **most important mathematical formulations**, **key concepts**, and a **complete example solution** ‚Äî all in one place for easy understanding and reference.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Steady State in Markov Chains: Summary**\n",
    "\n",
    "### üîπ **Definition (with Example)**\n",
    "\n",
    "The **Steady State** (also known as the **Stationary Distribution**) of a **Markov Chain** is a **probability distribution** over the states that remains **unchanged** over time. In other words, if the system is in the steady state, it will stay in that state no matter how many transitions occur.\n",
    "\n",
    "### üîπ **Mathematical Formulation**\n",
    "\n",
    "Let:\n",
    "- $ T $ be the **transition matrix** (an $ N \\times N $ matrix where $ T_{ij} = P(s_j \\mid s_i) $)\n",
    "- $ \\pi $ be the **steady-state probability vector** (a row vector)\n",
    "\n",
    "Then the **steady state** satisfies the equation:\n",
    "\n",
    "$$\n",
    "\\pi T = \\pi\n",
    "$$\n",
    "\n",
    "This is the **stationary distribution** of the Markov chain.\n",
    "\n",
    "Additionally, since $ \\pi $ is a probability distribution:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\pi_i = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Time Evolution of State Distribution**\n",
    "\n",
    "If $ P_0 $ is the **initial probability distribution**, then the distribution at step $ n $ is:\n",
    "\n",
    "$$\n",
    "P_n = P_0 \\cdot T^n\n",
    "$$\n",
    "\n",
    "As $ n \\to \\infty $, under certain conditions (like irreducibility and aperiodicity), $ P_n $ converges to the **steady-state distribution** $ \\pi $:\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty} P_n = \\pi\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Concepts**\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Markov Property** | Future depends only on the present state. |\n",
    "| **Transition Matrix $ T $** | Describes the probability of moving from one state to another. |\n",
    "| **Steady State $ \\pi $** | A probability distribution satisfying $ \\pi T = \\pi $. |\n",
    "| **Convergence** | Under certain conditions, $ P_n \\to \\pi $ as $ n \\to \\infty $. |\n",
    "| **Use in ML** | Used in HMMs, PageRank, Reinforcement Learning, and more. |\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Example: 2-State Markov Chain**\n",
    "\n",
    "### üåû Problem\n",
    "\n",
    "We have a simple weather system with two states:\n",
    "- **Sunny (S)**\n",
    "- **Rainy (R)**\n",
    "\n",
    "The **transition matrix** is:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "0.8 & 0.2 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want to find the **steady-state distribution** $ \\pi = [\\pi_S, \\pi_R] $ such that:\n",
    "\n",
    "$$\n",
    "\\pi T = \\pi\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Solution\n",
    "\n",
    "We solve the system:\n",
    "\n",
    "$$\n",
    "\\pi_S \\cdot 0.8 + \\pi_R \\cdot 0.3 = \\pi_S \\quad \\text{(1)} \\\\\n",
    "\\pi_S \\cdot 0.2 + \\pi_R \\cdot 0.7 = \\pi_R \\quad \\text{(2)}\n",
    "$$\n",
    "\n",
    "From (1):\n",
    "\n",
    "$$\n",
    "0.8\\pi_S + 0.3\\pi_R = \\pi_S \\Rightarrow 0.3\\pi_R = 0.2\\pi_S \\Rightarrow \\pi_R = \\frac{2}{3} \\pi_S\n",
    "$$\n",
    "\n",
    "Using the normalization condition $ \\pi_S + \\pi_R = 1 $:\n",
    "\n",
    "$$\n",
    "\\pi_S + \\frac{2}{3} \\pi_S = 1 \\Rightarrow \\frac{5}{3} \\pi_S = 1 \\Rightarrow \\pi_S = \\frac{3}{5}, \\quad \\pi_R = \\frac{2}{5}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Answer\n",
    "\n",
    "$$\n",
    "\\pi = \\left[ \\frac{3}{5}, \\frac{2}{5} \\right] = [0.6, 0.4]\n",
    "$$\n",
    "\n",
    "This means that in the long run, the system will be **Sunny 60% of the time** and **Rainy 40% of the time**, regardless of the starting distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Applications in Machine Learning & Pattern Recognition**\n",
    "\n",
    "- **Hidden Markov Models (HMMs)**: Used in speech recognition, DNA sequence analysis.\n",
    "- **PageRank Algorithm**: Google uses steady-state distribution to rank web pages.\n",
    "- **Reinforcement Learning**: Steady state helps in finding optimal long-term policies.\n",
    "- **Text Generation**: Modeling language as a Markov process.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ **Summary in One Paragraph**\n",
    "\n",
    "The **Steady State** in a **Markov Chain** is a probability distribution $ \\pi $ that remains unchanged after applying the transition matrix $ T $, i.e., $ \\pi T = \\pi $. It represents the long-term behavior of the system and is found by solving the system of linear equations derived from the transition matrix. For example, in a 2-state weather model with transition matrix $ T = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.3 & 0.7 \\end{bmatrix} $, the steady-state distribution is $ \\pi = [0.6, 0.4] $, meaning the system will be Sunny 60% of the time and Rainy 40% in the long run. This concept is widely used in machine learning for modeling sequences, ranking, and decision-making under uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a **code example** or a **visualization** of this! üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad953848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d366b951",
   "metadata": {},
   "source": [
    "Great question! ü§ì  \n",
    "You're touching on a **very important and subtle point** about **Markov chains** and **steady states**. Let me break it down clearly, with **mathematical insight**, **examples**, and **real-world intuition**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Will Every Markov Chain Reach a Steady State?**\n",
    "\n",
    "**No, not every Markov chain will reach a steady state.**  \n",
    "Whether a Markov chain **converges to a steady state** depends on **three key properties**:\n",
    "\n",
    "1. **Irreducibility**\n",
    "2. **Aperiodicity**\n",
    "3. **Positive recurrence**\n",
    "\n",
    "Let‚Äôs explain each and how they affect convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. **Irreducibility**\n",
    "\n",
    "A Markov chain is **irreducible** if it is **possible to get from any state to any other state** in a finite number of steps.\n",
    "\n",
    "- If the chain is **not irreducible**, it may have **multiple closed classes** (groups of states that can't reach each other), and each class may have its own steady state.\n",
    "- In that case, the chain **won‚Äôt converge to a single global steady state**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. **Aperiodicity**\n",
    "\n",
    "A state is **aperiodic** if the chain can return to it at **any time step**, not just at multiples of some period.\n",
    "\n",
    "- If a chain is **periodic**, it may **oscillate** between states and **never settle** into a steady state.\n",
    "- For example, a two-state system that always alternates (e.g., S ‚Üí R ‚Üí S ‚Üí R ‚Üí ...) is **periodic with period 2** and **won‚Äôt converge**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. **Positive Recurrence**\n",
    "\n",
    "A state is **positive recurrent** if the **expected return time** to that state is **finite**.\n",
    "\n",
    "- If a state is **transient**, the chain may **leave it forever** and **not return**, so it won‚Äôt be part of the steady state.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ **So, When Does a Markov Chain Reach a Steady State?**\n",
    "\n",
    "A **finite-state Markov chain** will **converge to a unique steady state** if and only if it is:\n",
    "\n",
    "- **Irreducible**\n",
    "- **Aperiodic**\n",
    "- **Positive recurrent**\n",
    "\n",
    "In that case, the **steady-state distribution** is **independent of the initial distribution**, and the system will **eventually reach the same steady state** no matter where it starts.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ **Example: Convergent vs. Non-Convergent Chains**\n",
    "\n",
    "### ‚úÖ **Convergent (Steady State Exists)**\n",
    "\n",
    "Let‚Äôs take the **weather example** again:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "0.8 & 0.2 \\\\\n",
    "0.3 & 0.7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- This chain is **irreducible** (you can go from any state to any other).\n",
    "- It is **aperiodic** (you can return to a state in 1 step).\n",
    "- All states are **positive recurrent**.\n",
    "\n",
    "So, it **converges to a unique steady state**:\n",
    "\n",
    "$$\n",
    "\\pi = [0.6, 0.4]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **Non-Convergent (No Steady State)**\n",
    "\n",
    "#### Example 1: **Periodic Chain**\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is a **periodic chain** with period 2. It **oscillates** between states and **never converges**.\n",
    "\n",
    "#### Example 2: **Non-Irreducible Chain**\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "0.5 & 0.5 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This chain has **two closed classes**:\n",
    "- {S2}\n",
    "- {S3}\n",
    "\n",
    "So, it **won‚Äôt converge to a single steady state**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **What About the Initial Distribution?**\n",
    "\n",
    "- If the chain is **convergent** (irreducible, aperiodic, positive recurrent), then **regardless of the initial distribution**, the system will **eventually reach the same steady state**.\n",
    "- So, the **initial distribution doesn't affect the final steady state**, only the **speed of convergence**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **Summary Table**\n",
    "\n",
    "| Condition | Converges to Steady State? | Notes |\n",
    "|----------|-----------------------------|-------|\n",
    "| Irreducible + Aperiodic + Positive Recurrent | ‚úÖ Yes | Unique steady state |\n",
    "| Not Irreducible | ‚ùå No | May have multiple steady states |\n",
    "| Periodic | ‚ùå No | Oscillates, doesn't settle |\n",
    "| Transient states | ‚ùå No | Chain may leave and not return |\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Insight**\n",
    "\n",
    "If a Markov chain is **convergent**, then:\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty} T^n = \\begin{bmatrix}\n",
    "\\pi_1 & \\pi_2 & \\dots & \\pi_N \\\\\n",
    "\\pi_1 & \\pi_2 & \\dots & \\pi_N \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\pi_1 & \\pi_2 & \\dots & \\pi_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $ \\pi = [\\pi_1, \\pi_2, \\dots, \\pi_N] $ is the **steady-state distribution**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Real-World Intuition**\n",
    "\n",
    "Think of the **steady state** like the **long-term climate** of a system.  \n",
    "- If the system is **well-behaved** (irreducible, aperiodic), it will eventually settle into a **predictable pattern**.\n",
    "- If it's **chaotic or periodic**, it may **never settle** ‚Äî like a pendulum that keeps swinging or a system that cycles through states.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ì Want to Try It Out?\n",
    "\n",
    "I can:\n",
    "- Show you how to **test for irreducibility and aperiodicity** in code.\n",
    "- Give you a **Python example** of a **non-convergent chain**.\n",
    "- Help you **simulate convergence** for different initial distributions.\n",
    "\n",
    "Let me know what you'd like to explore next! üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf4027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d304c02",
   "metadata": {},
   "source": [
    "Okay! Let's dive into this in a friendly and clear way. üòä\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Subject of Finding the Expected Value for Staying in a State in Markov Chains**\n",
    "\n",
    "When you're working with **Markov chains**, one of the interesting questions you can ask is:\n",
    "\n",
    "> **\"What is the expected number of times the process will stay in a particular state before leaving it?\"**\n",
    "\n",
    "This is especially useful when you're analyzing **stationary behavior**, **absorbing states**, or **long-term probabilities**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **What is the Expected Value for Staying in a State?**\n",
    "\n",
    "Let‚Äôs say you have a **transition matrix** $ T $ of size $ 3 \\times 3 $, and you're interested in the **expected number of times the process stays in state 1 (S‚ÇÅ)** before moving to another state.\n",
    "\n",
    "In general, the **expected number of times the process stays in a state** is related to the **self-transition probability** of that state.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **Example: 3x3 Transition Matrix**\n",
    "\n",
    "Let‚Äôs define a 3x3 transition matrix $ T $:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "t_{11} & t_{12} & t_{13} \\\\\n",
    "t_{21} & t_{22} & t_{23} \\\\\n",
    "t_{31} & t_{32} & t_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Suppose you're interested in the **expected number of times the process stays in state 3 (S‚ÇÉ)** before leaving it.\n",
    "\n",
    "- The **self-transition probability** for state 3 is $ t_{33} $.\n",
    "- The **probability of leaving** state 3 is $ 1 - t_{33} $.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Expected Number of Visits to a State**\n",
    "\n",
    "If the process is currently in state $ i $, the **expected number of times it will stay in that state** before leaving is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\text{Visits to state } i] = \\frac{1}{1 - t_{ii}}\n",
    "$$\n",
    "\n",
    "This is derived from the **geometric distribution**, where the probability of staying in the state is $ t_{ii} $, and the probability of leaving is $ 1 - t_{ii} $.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ **Example Calculation**\n",
    "\n",
    "Let‚Äôs say:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.3 \\\\\n",
    "0.4 & 0.1 & 0.5 \\\\\n",
    "0.6 & 0.2 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You're interested in the expected number of times the process stays in **state 3** before leaving it.\n",
    "\n",
    "- $ t_{33} = 0.2 $\n",
    "- $ 1 - t_{33} = 0.8 $\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\text{Visits to state 3}] = \\frac{1}{1 - 0.2} = \\frac{1}{0.8} = 1.25\n",
    "$$\n",
    "\n",
    "So, **on average**, the process will stay in **state 3 for 1.25 steps** before moving to another state.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Key Takeaways**\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Self-transition probability** | $ t_{ii} $: Probability of staying in state $ i $ |\n",
    "| **Leaving probability** | $ 1 - t_{ii} $: Probability of leaving state $ i $ |\n",
    "| **Expected number of visits** | $ \\frac{1}{1 - t_{ii}} $: Expected number of times the process stays in state $ i $ before leaving |\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ì Want to go deeper?\n",
    "\n",
    "- I can help you write a **Python function** to calculate this for any state in any matrix.\n",
    "- Or we can explore **absorbing states** and **expected time to absorption**.\n",
    "- Or we can look at **expected number of visits to a state over time** using **fundamental matrix**.\n",
    "\n",
    "Let me know how you want to go further! üòä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
