#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

16ML0001	Basic	16ML-DecisionTrees	What is a Decision Tree in machine learning?	A Decision Tree is a non-parametric supervised learning model used for classification and regression. It structures decisions and their possible consequences as a tree, with internal nodes representing tests on features, branches representing outcomes of tests, and leaf nodes representing final class labels or values.
16ML0002	Basic	16ML-DecisionTrees	What are the three types of nodes in a standard Decision Tree?	1. **Root Node:** The topmost node, representing the entire dataset and the first feature test.<br>2. **Internal/Decision Nodes:** Nodes that split the data based on a feature's value.<br>3. **Leaf/Terminal Nodes:** Final nodes that assign a class label (or regression value) to a subset of the data that has been routed to it.
16ML0003	Basic	16ML-DecisionTrees	A Decision Tree classifier can be expressed as a logical statement. What is the general form of this statement?	A Decision Tree is equivalent to a **disjunction (OR) of conjunctions (ANDs)**, where each path from the root to a leaf forms one conjunction. The entire tree is the OR of all these paths.<br>**Logical Form:** `(Path1_Conditions) OR (Path2_Conditions) OR ...`<br>Each path is: `(Feature_A op Value_A) AND (Feature_B op Value_B) AND ... -> Class_Label`
16ML0004	Basic	16ML-DecisionTrees	In a Decision Tree, must every available feature from the dataset be used in the final tree model?	**No.** The tree-building algorithm selects splits based on criteria like Information Gain. Features that do not contribute significantly to reducing impurity may **never be used** in any split. The final tree uses only a subset of the original features, performing implicit feature selection.
16ML0005	Basic	16ML-DecisionTrees	Can the same feature be used to split the data at nodes located on different branches of the tree?	**Yes.** A single feature can appear in **multiple, distinct paths** within the tree. For example, the feature "Age" might be used in the left subtree (with one threshold) and again in the right subtree (with a different threshold), as each path addresses a different data subset.
16ML0006	Basic	16ML-DecisionTrees	Within a single path from the root to a leaf, how many times can a specific feature be tested?	**At most once.** A fundamental rule in a standard Decision Tree is that any given feature appears **no more than once** along any single path. Once a split is made on a feature, the resulting data subsets are considered with respect to remaining features, preventing redundant or contradictory tests on the same path.
16ML0007	Basic	16ML-DecisionTrees	What is the ID3 algorithm in machine learning?	ID3 (Iterative Dichotomiser 3) is a classic, **greedy algorithm** used to generate a Decision Tree from a dataset. It was introduced by Ross Quinlan and operates in a **top-down, recursive** manner, using **Information Gain** as the criterion to select the best attribute to split the data at each node.
16ML0008	Basic	16ML-DecisionTrees	What are the base cases (stopping criteria) that halt the recursion in the ID3 algorithm?	Recursion stops when one of these conditions is met:<br><br>1. **Pure Node:** All instances in the current subset belong to the same class. A leaf node is created with that class label.<br>2. **No Attributes Left:** There are no more features to split on. A leaf node is created with the **majority class** of the subset.<br>3. **Empty Subset:** The generated subset is empty (no instances match the split). A leaf node is created with the majority class of the *parent* node.
16ML0009	Basic	16ML-DecisionTrees	Describe the high-level, recursive steps of the ID3 algorithm.	1. **Check Base Cases:** Stop if the node is pure, no attributes remain, or the subset is empty.<br>2. **Select Best Attribute:** For each remaining attribute, calculate its **Information Gain** when splitting the current data subset.<br>3. **Create Root Node:** Choose the attribute with the **highest Information Gain** as the splitting criterion for the current node.<br>4. **Recurse:** For each unique value of the chosen attribute, create a new branch and a corresponding subset of data. Call ID3 recursively on that subset with the selected attribute removed.
16ML0010	Basic	16ML-DecisionTrees	In the ID3 algorithm, how is the "best" attribute for a split quantitatively chosen?	The attribute \(A^*\) that **maximizes Information Gain** is selected:<br><br>\( A^* = \arg\max_{A \in \text{Attributes}} IG(S, A) \)<br><br>Where \(IG(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v)\). The algorithm computes this for all candidate attributes at the current node.
16ML0011	Basic	16ML-DecisionTrees	When selecting a split in ID3, we seek to maximize Information Gain (IG). What does this imply about the desired entropy of the resulting child nodes?	Maximizing IG is equivalent to **minimizing the weighted average entropy of the child nodes**. A high IG split creates child nodes that are, on average, **much purer (have much lower entropy)** than the parent node. The ideal split sends all instances of each class to a distinct, perfectly pure (zero-entropy) child node.
16ML0012	Basic	16ML-DecisionTrees	What is a key assumption of the basic ID3 algorithm regarding the nature of input features?	The basic ID3 algorithm is designed to work **only with categorical (discrete) attributes**. It creates one branch for **each possible value** of the selected categorical attribute. It cannot natively handle continuous (numerical) features without prior discretization (binning).
16ML0013	Basic	16ML-DecisionTrees	What is the major overfitting pitfall of the basic ID3 algorithm?	ID3 grows the tree **until all leaf nodes are pure** (or until the stopping criteria are met on the training data). This often results in a **very deep, complex tree that perfectly fits the training data**, including its noise. Such a tree has high variance and poor generalization to unseen data (overfitting).
16ML0014	Basic	16ML-DecisionTrees	Why does ID3's attribute selection criterion (Information Gain) create a bias, and what is its consequence?	Information Gain is strongly biased towards **attributes with many unique values** (high cardinality), such as ID numbers or dates. These attributes can split data into many small, pure subsets, yielding very high IG but **non-generalizable, meaningless splits**. This is a primary weakness of pure IG.
16ML0015	Basic	16ML-DecisionTrees	What major improvement did the C4.5 algorithm (ID3's successor) introduce to combat its pitfalls?	C4.5 introduced the **Gain Ratio** as the default splitting criterion. Gain Ratio normalizes Information Gain by the **intrinsic information (split entropy)** of the attribute, which penalizes attributes with a large number of values, thereby mitigating the high-cardinality bias.<br><br>\( \text{GainRatio}(S, A) = \frac{IG(S, A)}{IV(A)} \)<br><br>where \(IV(A) = -\sum_v \frac{|S_v|}{|S|} \log_2 \frac{|S_v|}{|S|}\) (Split Information).
16ML0016	Basic	16ML-DecisionTrees	What is a fundamental difference between the tree structure produced by ID3 and that produced by CART (Classification and Regression Trees)?	- **ID3 (and C4.5):** Produces **multi-way splits** (\(n\) branches for an attribute with \(n\) values). The tree is generally not binary.<br>- **CART:** Produces **strictly binary trees**. Each split is of the form `Attribute <= threshold` (for continuous) or `Attribute in subset` (for categorical), creating exactly two child nodes.
16ML0017	Basic	16ML-DecisionTrees	How does the basic ID3 algorithm handle missing values in the training data?	The **basic ID3 algorithm does not have a built-in mechanism** for handling missing attribute values. It requires a complete dataset. In practice, preprocessing (like imputation or removal of instances with missing values) is necessary before applying ID3.
16ML0018	Basic	16ML-DecisionTrees	What is the fundamental goal when choosing a split at an internal node in a decision tree?	The goal is to find the feature and split point that **maximizes the purity** (or homogeneity) of the resulting child nodes. We measure impurity to quantify how mixed the classes are within a node. A good split creates child nodes where one class dominates.
16ML0019	Basic	16ML-DecisionTrees	In the context of Decision Trees, what is Entropy?	Entropy is a measure of **impurity, disorder, or uncertainty** in a set of data points. For a classification node, it quantifies how mixed the class labels are. High entropy means the node is highly impure (classes are evenly mixed). Low entropy means the node is pure (contains mostly one class).
16ML0020	Basic	16ML-DecisionTrees	What is the formula for the Entropy \(H(S)\) of a discrete set \(S\) with \(K\) classes?	For a node \(S\) with class proportions \(p_i\) for \(i = 1, ..., K\):<br><br>\( H(S) = -\sum_{i=1}^{K} p_i \log_2 p_i \)<br><br>where \(0 \log_2 0\) is defined as \(0\). The base-2 logarithm means entropy is measured in **bits**.
16ML0021	Basic	16ML-DecisionTrees	For a binary classification problem, what are the minimum and maximum possible values of Entropy? Describe the class distribution in each case.	- **Minimum Entropy (0 bits):** Occurs when \(p_i = 1\) for one class and \(0\) for the other (e.g., all True or all False). The node is perfectly pure.<br>- **Maximum Entropy (1 bit):** Occurs when \(p_i = 0.5\) for both classes (perfectly balanced 50/50 split). The node is maximally impure.<br><br>\( H_{max} = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \)
16ML0022	Basic	16ML-DecisionTrees	What is Information Gain (IG) and how is it used in building a Decision Tree?	Information Gain is the **expected reduction in entropy** achieved by splitting a node on a particular feature. It measures how much "information" a feature gives us about the class. The tree-building algorithm (like ID3) selects the feature that provides the **highest Information Gain** to split on at each step.
16ML0023	Basic	16ML-DecisionTrees	What is the formula for Information Gain \(IG(S, A)\) of splitting set \(S\) using feature \(A\)?	Let \(S\) be the parent node and \(A\) be the feature with \(V\) distinct values. \(S_v\) is the subset of \(S\) where feature \(A\) has value \(v\).<br><br>\( IG(S, A) = H(S) - \sum_{v=1}^{V} \frac{|S_v|}{|S|} H(S_v) \)<br><br>The term \(\frac{|S_v|}{|S|}\) is the weight (proportion) of data points going to child node \(v\).
16ML0024	Basic	16ML-DecisionTrees	A parent node S has 10 samples: 5 'Yes', 5 'No'. Splitting on feature 'Wind' gives: Weak branch (7 samples: 4 Yes, 3 No), Strong branch (3 samples: 1 Yes, 2 No). Calculate IG.	1. **Parent Entropy:** \(H(S) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1\) bit.<br>2. **Child Entropies:**<br>   \(H(S_{Weak}) = - (\frac{4}{7}\log_2\frac{4}{7} + \frac{3}{7}\log_2\frac{3}{7}) \approx 0.985\)<br>   \(H(S_{Strong}) = - (\frac{1}{3}\log_2\frac{1}{3} + \frac{2}{3}\log_2\frac{2}{3}) \approx 0.918\)<br>3. **Weighted Average Child Entropy:**<br>   \(\frac{7}{10} * 0.985 + \frac{3}{10} * 0.918 \approx 0.965\)<br>4. **Information Gain:** \(IG = 1 - 0.965 = 0.035\) bits.
16ML0025	Basic	16ML-DecisionTrees	For the dataset `X=[[6,3],[2,7],[9,6],[4,2]]`, `Y=[1,0,1,0]`, which proposed root split yields the maximum Information Gain?<br><br>**Proposed splits:** `x1 > 2`, `x2 > 3`, `x1 > 4`, `x2 > 6`	We calculate **parent entropy** \(H(S)\) first. Parent has 4 samples: 2 class '1', 2 class '0'.<br><br>\( H(S) = -\left(\frac{2}{4}\log_2\frac{2}{4} + \frac{2}{4}\log_2\frac{2}{4}\right) = 1 \text{ bit} \)<br><br>**Conclusion:** The split **`x1 > 4`** yields the maximum Information Gain (1 bit). (Full calculation omitted for brevity but process shown in source).
16ML0026	Basic	16ML-DecisionTrees	What is the relationship between the depth (or number of nodes) of a Decision Tree and the risk of overfitting?	**Deep trees** (many splits) can fit the training data perfectly, capturing noise and specific patterns that do not generalize. This leads to **overfitting** (low bias, high variance). **Shallow trees** are simpler and may **underfit** (high bias, low variance). The depth is a key hyperparameter controlled via **pruning**, setting a maximum depth, or minimum samples per leaf.
16ML0027	Basic	16ML-DecisionTrees	Why does allowing a Decision Tree's depth to approach the length of the feature vector typically lead to poor performance?	If the tree depth is close to the number of features, the tree can isolate individual training points (or very small groups) into pure leaf nodes. This means it is **modeling the noise and random fluctuations** specific to the training set, rather than the generalizable underlying pattern. The model's variance becomes very high, and its predictions on new data become unreliable (overfitting).
16ML0028	Basic	16ML-DecisionTrees	For a *continuous probability distribution* with known mean and variance, which common distribution has the maximum entropy?	Given a fixed mean and variance, the **Gaussian (Normal) distribution** has the **maximum differential entropy** among all continuous distributions. Other distributions (e.g., Exponential, Bernoulli, Binomial) have lower entropy under their respective constraints.<br><br>\( H(X) = \frac{1}{2} \ln(2\pi e \sigma^2) \quad \text{(for Gaussian)} \)
16ML0029	Basic	16ML-DecisionTrees	For a *discrete* distribution over K outcomes, which distribution maximizes entropy? For a *continuous* distribution with fixed mean and variance, which one does?	- **Discrete (K outcomes):** The **uniform distribution** (\(p_i = 1/K\) for all i) maximizes entropy.<br>  \( H_{max} = \log_2 K \)<br>- **Continuous (fixed mean \(\mu\) and variance \(\sigma^2\)):** The **Gaussian (Normal) distribution** \(\mathcal{N}(\mu, \sigma^2)\) maximizes differential entropy.<br>  \( h_{max} = \frac{1}{2} \ln(2\pi e \sigma^2) \)<br><br>This illustrates the **Maximum Entropy Principle**: the distribution making the fewest assumptions, given the constraints, has the highest entropy.
16ML0030	Basic	16ML-DecisionTrees	What is the Principle of Maximum Entropy in probability theory?	The Principle of Maximum Entropy states that, among all probability distributions consistent with a given set of **known constraints** (e.g., mean, variance, support), the distribution that best represents the current state of knowledge is the one with the **largest entropy**. It is a method for assigning probabilities that is maximally non-committal to missing information.
16ML0031	Basic	16ML-DecisionTrees	Which distribution maximizes entropy under the constraint of a fixed mean \(\mu\)?	Given **only a fixed mean \(\mu\)** (and support on \([0, \infty)\)), the **Exponential distribution** maximizes entropy.<br><br>\( p(x) = \lambda e^{-\lambda x}, \quad x \geq 0 \)<br><br>where \(\lambda = 1/\mu\). This distribution is "the most random" given we only know the average.
16ML0032	Basic	16ML-DecisionTrees	Which distribution maximizes entropy under the constraints of a fixed mean \(\mu\) and variance \(\sigma^2\)?	Given **fixed mean \(\mu\) and variance \(\sigma^2\)**, the **Gaussian (Normal) distribution** \(\mathcal{N}(\mu, \sigma^2)\) maximizes entropy.<br><br>\( p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \)<br><br>This is the most common and important maximum entropy result, underlying many assumptions in statistics and machine learning.
16ML0033	Basic	16ML-DecisionTrees	Which distribution maximizes entropy under the constraint of a finite, fixed support \([a, b]\)?	Given **only a fixed interval \([a, b]\)** (with no moment constraints), the **Continuous Uniform distribution** over that interval maximizes entropy.<br><br>\( p(x) = \frac{1}{b-a}, \quad a \leq x \leq b \)<br><br>It is the most "spread out" or "uncertain" distribution possible when only the bounds are known.
16ML0034	Basic	16ML-DecisionTrees	What is a major pitfall of using raw Information Gain as a splitting criterion?	Information Gain is **biased toward features with many unique values or high cardinality** (e.g., IDs, continuous features binned into many intervals). It can unfairly favor splits that create many small, pure child nodes but do not generalize. This leads to overfitting. Solutions: Use **Gain Ratio** (normalizes IG) or **criteria based on Gini impurity**.
16ML0035	Basic	16ML-DecisionTrees	What is the Gini Impurity (or Gini Index) in the context of Decision Trees?	Gini Impurity is an **alternative measure of node impurity** to Entropy. It measures the probability of **incorrectly classifying** a randomly chosen element from the node if it were labeled randomly according to the class distribution in the node. Lower Gini means a purer node.
16ML0036	Basic	16ML-DecisionTrees	What is the formula for the Gini Impurity \(G(S)\) of a node \(S\) with \(K\) classes?	For a node \(S\) with class proportions \(p_i\) for \(i = 1, ..., K\):<br><br>\( G(S) = 1 - \sum_{i=1}^{K} p_i^2 \)<br><br>For a binary classification problem (\(p\) = proportion of class 1):<br><br>\( G(S) = 1 - p^2 - (1-p)^2 = 2p(1-p) \)
16ML0037	Basic	16ML-DecisionTrees	What are the minimum and maximum values of Gini Impurity? Describe the node purity in each case.	- **Minimum (0.0):** Occurs when the node is **pure** (all instances belong to one class, e.g., \(p_i=1\) for one class). This is the best possible impurity.<br>- **Maximum (\(1 - 1/K\)):** For \(K\) classes, the maximum occurs when the class distribution is perfectly **uniform** (\(p_i = 1/K\) for all \(i\)). For binary classification (\(K=2\)), the maximum is \(0.5\) (\(p=0.5\)).
16ML0038	Basic	16ML-DecisionTrees	How do Gini Impurity and Entropy compare qualitatively as splitting criteria?	Both are **very similar in practice** and often lead to similar trees.<br><br>- **Entropy:** Rooted in information theory (bits of information). Slightly more sensitive to changes in node purity.<br>- **Gini Impurity:** Computationally slightly **more efficient** as it doesn't require computing logarithms. Some libraries (like scikit-learn) use Gini by default.<br>  Both are **convex functions** minimized at pure nodes.
16ML0039	Basic	16ML-DecisionTrees	For binary classification, plot Gini Impurity \(G(p)=2p(1-p)\) and Entropy \(H(p)\) vs. \(p\) (proportion of class 1). How do they compare?	Both are **symmetric, concave functions** maximized at \(p=0.5\).<br><br>- **Gini:** A **parabola**. \(G(0.5)=0.5\).<br>- **Entropy:** A **logarithmic curve**. \(H(0.5)=1\) bit.<br>  While scaled differently, their shapes are similar, leading to comparable splits. Gini is often described as slightly more "peakier," favoring splits that create nodes with one dominant class.
16ML0040	Basic	16ML-DecisionTrees	What is the analogous measure to Information Gain when using Gini Impurity?	Instead of Information Gain, we use the **reduction in Gini Impurity**, sometimes called Gini Gain. For a split on feature \(A\) creating subsets \(S_v\):<br><br>\( \text{Gini Gain}(S, A) = G(S) - \sum_{v} \frac{|S_v|}{|S|} G(S_v) \)<br><br>The algorithm selects the split that **maximizes this Gini Gain**.
16ML0041	Basic	16ML-DecisionTrees	Does Gini Impurity share the same major pitfall as Information Gain regarding high-cardinality features?	**Yes.** Like Information Gain, Gini Gain is also **biased towards features with many unique values** (high cardinality). It can lead to splits that create many small, pure child nodes that do not generalize. This is not a unique flaw of Gini; it's a property of impurity reduction metrics. Solutions like **Gain Ratio** or setting minimum samples per leaf are still needed.
16ML0042	Basic	16ML-DecisionTrees	In practical terms, when might you choose Gini Impurity as your splitting criterion?	1. **Computational Efficiency:** When training on very large datasets, the slight speed advantage of Gini (no log computations) can be beneficial.<br>2. **Default Choice:** In many implementations (e.g., scikit-learn's `DecisionTreeClassifier`), Gini is the default as it often produces similar results to Entropy with less computation.<br>3. **No strong theoretical preference:** The choice rarely has a major impact on final model performance; tree depth and pruning parameters are far more influential.
16ML0043	Basic	16ML-DecisionTrees	Can the Gini Impurity measure be used for building Regression Trees (predicting continuous values)?	**No.** Gini Impurity and Entropy are defined for **classification** (discrete classes). Regression Trees use splitting criteria based on **variance reduction** of the target variable. For a node, the impurity is measured by the **Mean Squared Error (MSE)**, and the split aims to minimize the weighted average MSE of the child nodes.
16ML0044	Basic	16ML-DecisionTrees	How can the Gini Impurity measure be used to derive feature importance scores in a trained Decision Tree?	**Gini Importance** (or Mean Decrease in Impurity) for a feature is calculated as the **total reduction in Gini Impurity** (Gini Gain) contributed by that feature, averaged over all trees in a forest (for Random Forest) or summed across all nodes in a single tree. It's a common metric for understanding which features were most influential in building the model.
16ML0045	Basic	16ML-DecisionTrees	Does the standard top-down, greedy approach to building a Decision Tree guarantee an optimal final tree?	**No.** The algorithm makes the **locally optimal choice** (best split at the current node) at each step without considering future splits. This greedy heuristic does not guarantee a **globally optimal tree** (smallest possible tree for the data). Finding the optimal tree is an NP-complete problem.
16ML0046	Basic	16ML-DecisionTrees	What is the computational complexity class of the problem of finding the smallest, most accurate tree?	Finding the **optimal** Decision Tree (e.g., the smallest tree that perfectly fits the training data) is known to be an **NP-complete** problem. This means no known algorithm can solve it efficiently (in polynomial time) for all cases, which justifies the use of efficient greedy heuristics in practice.
16ML0047	Basic	16ML-DecisionTrees	In the context of finding an optimal Decision Tree, what is the distinction between an NP-hard and an NP-complete problem?	- **NP-Hard:** A problem is NP-hard if it is **at least as hard as the hardest problems in NP**. Finding the optimal Decision Tree (minimal size, perfect accuracy) is NP-hard.<br>- **NP-Complete:** A problem is NP-complete if it is **both NP-hard and in NP** (solutions can be verified quickly). The specific optimal Decision Tree problem is often classified as NP-complete, as a candidate tree's accuracy and size can be verified efficiently against the data.
16ML0048	Basic	16ML-DecisionTrees	How do Decision Trees typically handle categorical and continuous features differently?	- **Categorical:** For an \(m\)-ary feature, the split can be a multi-way split (\(m\) children) or a binary split based on a subset of categories. IG is computed over these groupings.<br>- **Continuous/Ordinal:** The algorithm searches for the **optimal split point** (threshold) that maximizes IG (e.g., "Age \(\leq\) 30.5"). This is computationally more intensive than for categorical features.
16ML0049	Basic	16ML-DecisionTrees	Under what theoretical condition can a Decision Tree (like one built by ID3) achieve zero error on its training dataset?	Consistent dataset. A Decision Tree can achieve **zero training error** if, in the training data, **no two instances with identical feature vectors belong to different classes**. That is, each unique point in the feature space must be associated with only one label. Given enough depth, the tree can isolate every training point into its own pure leaf node.
16ML0050	Basic	16ML-DecisionTrees	What is pruning in the context of Decision Trees, and what is its primary goal?	Pruning is the process of **removing parts of a Decision Tree** (subtrees or branches) that provide little predictive power. Its primary goal is to **reduce overfitting** by simplifying the tree, thereby improving its **generalization ability** (accuracy on unseen data) at the cost of a slight decrease in training accuracy. It trades variance for bias.
16ML0051	Basic	16ML-DecisionTrees	What is pre-pruning (early stopping) in Decision Tree learning?	Pre-pruning involves **halting the tree growth process early** by setting constraints *before* the tree is fully grown. Common constraints include:<br><br>- Maximum tree depth<br>- Minimum number of samples required to split a node<br>- Minimum number of samples required at a leaf node<br>- Minimum impurity decrease (e.g., a threshold on Information Gain)<br>  It's faster but can lead to **underfitting** if constraints are too strict.
16ML0052	Basic	16ML-DecisionTrees	What is post-pruning, and how does a common method like Cost-Complexity Pruning (in CART) work?	Post-pruning involves **first growing the tree to its maximum size** (fitting the training data perfectly), then **removing subtrees** and replacing them with leaf nodes. **Cost-Complexity Pruning** minimizes:<br><br>\( R_\alpha(T) = R(T) + \alpha |\tilde{T}| \)<br><br>where \(R(T)\) is the misclassification error, \(|\tilde{T}|\) is the number of leaf nodes, and \(\alpha\) is a complexity parameter. Subtrees with a high cost for their added complexity are pruned.
16ML0053	Basic	16ML-DecisionTrees	Compare the computational speed and simplicity of pre-pruning vs. post-pruning.	- **Pre-pruning (Early Stopping):** **Faster and simpler.** The tree is never fully expanded, saving significant computation during the training phase. However, finding the right stopping parameters can require careful tuning via cross-validation.<br>- **Post-pruning:** **Slower and more computationally intensive.** It requires growing the full tree first (which can be large) and then evaluating subtrees for removal. The process is more complex but often yields better-performing trees.
16ML0054	Basic	16ML-DecisionTrees	Which pruning approach generally yields a more accurate final model on unseen data, and why?	**Post-pruning (e.g., Cost-Complexity) typically yields a more accurate and robust model.** This is because the early stopping criteria in pre-pruning are **local**â€”they cannot see the potential usefulness of subsequent splits that might have led to a simpler, more powerful global structure. Post-pruning makes decisions with a **global view** of the fully grown tree, often finding a better bias-variance trade-off.
16ML0055	Basic	16ML-DecisionTrees	What is a key risk or trade-off inherent to any pruning method?	The key risk is **removing a subtree that actually captures a meaningful, generalizable pattern in the data**, especially if the pattern is subtle or requires multiple splits to express. This is an **information loss**, potentially increasing bias (underfitting). The goal of pruning is to remove branches that model noise, not signal, which is a non-trivial distinction.
16ML0056	Basic	16ML-DecisionTrees	For a dataset with \(m\) instances, each described by \(d\) binary features, what is the theoretical maximum height (or depth) of a fully-grown, non-pruned decision tree?	The maximum possible depth is **\(\min(m-1, d)\)**.<br><br>- **Bounded by features (\(d\)):** You cannot split on a feature more than once on a path, so a path's length cannot exceed \(d\).<br>- **Bounded by data (\(m-1\)):** To isolate all \(m\) training points into unique leaves (a pure tree), you need at least \(m\) leaves. In the worst-case growth pattern, the tree height could be up to \(m-1\).
16ML0057	Basic	16ML-DecisionTrees	For a dataset with \(m\) instances and \(d\) binary features, what is the theoretical maximum number of leaf nodes in a fully-grown, non-pruned decision tree?	The theoretical maximum number of leaves is **\(\min(m, 2^d)\)**.<br><br>- **Bounded by data (\(m\)):** You cannot have more leaves than there are distinct training instances.<br>- **Bounded by features (\(2^d\)):** With \(d\) binary features, the feature space defines at most \(2^d\) unique "cells". A leaf represents one such cell.
16ML0058	Basic	16ML-DecisionTrees	Does feature scaling (e.g., normalization, standardization) affect the structure or performance of a standard Decision Tree?	**No.** Decision Trees are **invariant to monotonic transformations** of the features, including scaling and shifting. The split rule \(x_j \leq t\) only compares values within the same feature; multiplying or adding a constant does not change the relative order of samples. Therefore, normalization does **not** change the chosen splits, tree shape, decision boundary, or final predictions.