#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

15ML0001	Basic	15ML-PCA	What is the fundamental purpose of the method of Lagrange multipliers in optimization?	It is a strategy for finding the local maxima and minima of a function subject to equality constraints (e.g., \(g(x) = 0\)), without needing to solve the constraint explicitly for one variable.
15ML0002	Basic	15ML-PCA	For a scalar optimization problem: maximize \(f(x)\) subject to \(g(x) = 0\), how is the Lagrangian function \(\mathcal{L}\) defined?	Introduce a new scalar variable \(\lambda\), called the Lagrange multiplier. The Lagrangian is:<br><br>\( \mathcal{L}(x, \lambda) = f(x) - \lambda g(x) \)<br><br>The term \(-\lambda g(x)\) incorporates the constraint into the objective function.
15ML0003	Basic	15ML-PCA	What are the necessary conditions for a point \((x^*, \lambda^*)\) to be a constrained extremum of \(f\) subject to \(g(x)=0\)?	Take the partial derivatives of the Lagrangian \(\mathcal{L}(x, \lambda) = f(x) - \lambda g(x)\) and set them to zero:<br><br>1. \(\frac{\partial \mathcal{L}}{\partial x} = 0 \quad \Rightarrow \quad f'(x) - \lambda g'(x) = 0\)<br>2. \(\frac{\partial \mathcal{L}}{\partial \lambda} = 0 \quad \Rightarrow \quad g(x) = 0\)<br><br>This yields a system of equations to solve for \(x^*\) and \(\lambda^*\).
15ML0004	Basic	15ML-PCA	For optimizing \(f(\mathbf{x})\) with \(m\) constraints \(g_i(\mathbf{x})=0\), how is the Lagrangian \(\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda})\) defined (\(\mathbf{x} \in \mathbb{R}^D\))?	Introduce a vector of multipliers \(\boldsymbol{\lambda} = (\lambda_1, ..., \lambda_m)\). The Lagrangian is the sum:<br><br>\( \mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}) = f(\mathbf{x}) - \sum_{i=1}^m \lambda_i g_i(\mathbf{x}) \)<br><br>The solution satisfies \(\nabla_{\mathbf{x}} \mathcal{L} = \mathbf{0}\) and \(\nabla_{\boldsymbol{\lambda}} \mathcal{L} = \mathbf{0}\).
15ML0005	Basic	15ML-PCA	Summarize the 3-step workflow for solving a constrained optimization problem using Lagrange multipliers.	1. <b>Form Lagrangian:</b> \(\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}) = f(\mathbf{x}) - \sum_i \lambda_i g_i(\mathbf{x})\).<br>2. <b>Take Partial Derivatives:</b> Set \(\nabla_{\mathbf{x}} \mathcal{L} = 0\) and \(\nabla_{\boldsymbol{\lambda}} \mathcal{L} = 0\) (i.e., \(g_i(\mathbf{x})=0\)).<br>3. <b>Solve System:</b> Solve the resulting (often nonlinear) system of equations for \(\mathbf{x}^*\) and \(\boldsymbol{\lambda}^*\).
15ML0006	Basic	15ML-PCA	At a constrained optimum, what is the geometric relationship between \(\nabla f\) and \(\nabla g\)?	Their gradients are parallel: \(\nabla f = \lambda \nabla g\). The Lagrange multiplier \(\lambda\) is the scaling factor. This means the level set of \(f\) is tangent to the constraint surface defined by \(g(x)=0\).
15ML0007	Basic	15ML-PCA	Does satisfying the Lagrange conditions (\(\nabla \mathcal{L}=0\)) guarantee a global maximum or minimum?	No. It only identifies <b>stationary points</b> (critical points) of the Lagrangian, which include constrained maxima, minima, and saddle points. Further tests (e.g., second-order conditions, checking boundaries) are needed to determine the nature of the extremum.
15ML0008	Basic	15ML-PCA	For a dataset \(X\) of \(N\) samples in \(D\) dimensions, what is the goal of dimensionality reduction?	Find a representation in \(M\) dimensions (where \(M < D\)) that preserves the essential structure/information (e.g., variance, class separability) of the original data. This combats the curse of dimensionality.
15ML0009	Basic	15ML-PCA	What is Principal Component Analysis (PCA)? State its core linear algebra idea.	PCA is an unsupervised linear transformation. It finds a new orthogonal coordinate system (principal components) for the <b>centered</b> data such that the first axis (PC1) captures the maximum variance, the second (PC2) captures the next maximum variance orthogonal to PC1, and so on.<br>> Improves model performance buy :
15ML0010	Basic	15ML-PCA	In reducing dimensions from \(D\) to \(M\), contrast Feature Selection and Feature Extraction.	- <b>Feature Selection:</b> Selects \(M\) of the original \(D\) features. The new feature space is a subset of the original axes.<br>- <b>Feature Extraction (like PCA):</b> Creates \(M\) new features, each a function (linear combination) of all \(D\) original features. The new axes lie in the original \(D\)-dimensional space.
15ML0011	Basic	15ML-PCA	For 2D data, PCA finds a 1D subspace (line). What two equivalent criteria define the optimal line?	1. <b>Maximize Variance:</b> Maximize the variance of the projected data points.<br>2. <b>Minimize Error:</b> Minimize the mean squared distance (reconstruction error) between the original points and their projections onto the line.
15ML0012	Basic	15ML-PCA	Let \(X\) be an \(N \times D\) data matrix (\(N\) samples, \(D\) features). What is the first step of PCA and what does \(\bar{X}\) represent?	<b>Center the data:</b> Subtract the mean of each feature (column) from all samples. \(\bar{X}\) is the \(N \times D\) centered data matrix, where \(\sum_{n=1}^N \bar{x}_{n,j} = 0\) for each feature \(j\).
15ML0013	Basic	15ML-PCA	For centered data \(\bar{X}\), we project onto a unit vector \(\mathbf{v} \in \mathbb{R}^D\). The projection of sample \(n\) is \(z_n = \mathbf{v}^T \bar{\mathbf{x}}_n\). Formulate the variance maximization problem.	Maximize the sample variance of the projections \(\{z_1,...,z_N\}\):<br><br>\( \text{Maximize } \frac{1}{N} \sum_{n=1}^N (z_n)^2 = \frac{1}{N} \sum_{n=1}^N (\mathbf{v}^T \bar{\mathbf{x}}_n)^2 = \mathbf{v}^T \left( \frac{1}{N} \bar{X}^T \bar{X} \right) \mathbf{v} \)<br><br>subject to the constraint \(\mathbf{v}^T \mathbf{v} = 1\).<br><br>> <i>The matrix \(\frac{1}{N}\bar{X}^T \bar{X}\) is the \(D \times D\) sample covariance matrix \(C\).</i>
15ML0014	Basic	15ML-PCA	The PCA objective is \(\max \mathbf{v}^T C \mathbf{v}\) s.t. \(\mathbf{v}^T \mathbf{v}=1\). How is the Lagrange function \(\mathcal{L}\) defined?	Introduce a Lagrange multiplier \(\lambda\) for the equality constraint. The Lagrangian function is:<br><br>\( \mathcal{L}(\mathbf{v}, \lambda) = \mathbf{v}^T C \mathbf{v} - \lambda (\mathbf{v}^T \mathbf{v} - 1) \)<br><br>We find the stationary points by taking derivatives w.r.t. \(\mathbf{v}\) and \(\lambda\) and setting them to zero.
15ML0015	Basic	15ML-PCA	Taking the derivative of \(\mathcal{L}(\mathbf{v}, \lambda) = \mathbf{v}^T C \mathbf{v} - \lambda (\mathbf{v}^T \mathbf{v} - 1)\) with respect to the vector \(\mathbf{v}\), what equation do we get?	Using vector calculus, \(\frac{\partial \mathcal{L}}{\partial \mathbf{v}} = 2C\mathbf{v} - 2\lambda \mathbf{v}\). Setting this to zero yields the critical condition:<br><br>\( C \mathbf{v} = \lambda \mathbf{v} \)<br><br>This is the eigenvector equation for the covariance matrix \(C\).
15ML0016	Basic	15ML-PCA	From \(C \mathbf{v} = \lambda \mathbf{v}\), what is the direct interpretation for \(\lambda\) and \(\mathbf{v}\) in the context of PCA?	- \(\mathbf{v}\) is an eigenvector of the covariance matrix \(C\).<br>- \(\lambda\) is its corresponding eigenvalue, which equals the variance of the data projected onto \(\mathbf{v}\).<br><br>> <b>The optimal (first principal component) is the eigenvector corresponding to the largest eigenvalue.</b>
15ML0017	Basic	15ML-PCA	Given a \(D \times D\) covariance matrix \(C\), list the 5-step process to find its dominant eigenpair \((\lambda_1, \mathbf{v}_1)\).	1. Write eigen-equation: \(C\mathbf{v} = \lambda \mathbf{v}\).<br>2. Rearrange: \((C - \lambda I) \mathbf{v} = 0\).<br>3. Solve for \(\lambda\): Require non-trivial \(\mathbf{v}\), so \(\det(C - \lambda I) = 0\). Find largest root \(\lambda_1\).<br>4. Solve for \(\mathbf{v}\): Substitute \(\lambda_1\) into \((C - \lambda_1 I) \mathbf{v} = 0\) and solve for \(\mathbf{v}_1\).<br>5. Normalize: Set \(\mathbf{v}_1 \leftarrow \mathbf{v}_1 / \|\mathbf{v}_1\|\).
15ML0018	Basic	15ML-PCA	For an \(N \times D\) data matrix \(X\), list the complete steps to perform PCA and obtain \(M\)-dimensional projections.	1. <b>Center:</b> Compute column means \(\bar{\mathbf{x}}\), form \(\bar{X} = X - \mathbf{1}_N \bar{\mathbf{x}}^T\).<br>2. <b>Covariance:</b> Compute \(D \times D\) covariance \(C = \frac{1}{N} \bar{X}^T \bar{X}\).<br>3. <b>Eigendecomposition:</b> Solve \(C V = V \Lambda\), where \(V\) is \(D \times D\) orthogonal matrix of eigenvectors (columns), \(\Lambda\) is diagonal of eigenvalues \(\lambda_1 \ge ... \ge \lambda_D\).<br>4. <b>Select:</b> Choose first \(M\) columns of \(V\) as \(V_M\) (\(D \times M\)).<br>5. <b>Project:</b> The reduced data is \(Z = \bar{X} V_M\) (\(N \times M\)). Columns of \(Z\) are principal component scores.
15ML0019	Basic	15ML-PCA	Given centered data \(\bar{X}\) (\(N \times D\)) and a PCA projection matrix \(V_M\) (\(D \times M\)), what are the formulas for projection and approximate reconstruction?	- <b>Projection (to lower \(M\)-D):</b> \(Z = \bar{X} V_M\)  (\(N \times M\)).<br>- <b>Reconstruction (back to original \(D\)-D):</b> \(\tilde{X} = Z V_M^T = \bar{X} V_M V_M^T\).<br>  Note: \(\bar{X} \approx \tilde{X}\) if \(M\) is close to \(D\). Perfect reconstruction if \(M = D\).
15ML0020	Basic	15ML-PCA	What are the two primary ways PCA can improve model performance?	1. <b>Complexity Reduction:</b> Reduces dimensions from \(D\) to \(M\), lowering model parameters to combat overfitting and the curse of dimensionality.<br>2. <b>Noise Reduction:</b> Assumes low-variance directions are noise; discarding them (by choosing a lower \(M\)) yields cleaner data.
15ML0021	Basic	15ML-PCA	What is the core trade-off when choosing \(M\), the number of principal components to keep?	- <b>High M</b> (close to \(D\)): Retains more information (signal) and better reconstruction, but offers less complexity/noise reduction.<br>- <b>Low M</b> (much less than \(D\)): Offers strong complexity/noise reduction, but risks losing meaningful signal (higher bias).
15ML0022	Basic	15ML-PCA	How does the choice of \(M\) affect the bias-variance profile of a downstream model?	- <b>Low M:</b> High Bias, Low Variance. Discards signal (bias↑) but removes noise (variance↓). Risks underfitting.<br>- <b>High M:</b> Low Bias, High Variance. Retains signal (bias↓) but keeps noise/complexity (variance↑). Risks overfitting.<br>  Optimal \(M\) balances this.
15ML0023	Basic	15ML-PCA	What is a scree plot and what is the significance of its "elbow point"?	A scree plot graphs eigenvalues \(\lambda_i\) in descending order. The <b>elbow point</b> is the transition from a steep slope to a flat tail.<br><br>- <b>Before elbow:</b> Components with large \(\lambda_i\), representing major <b>signal</b>. Adding them gives large variance gains.<br>- <b>After elbow:</b> Components with small, similar \(\lambda_i\), likely <b>noise</b>. Adding them gives diminishing returns.
15ML0024	Basic	15ML-PCA	How is the scree plot's elbow point used as a heuristic to choose \(M\)?	Choose \(M\) at the elbow point. This keeps components in the steep, high-signal region and discards those in the flat, high-noise region. It aims to retain maximal signal while performing aggressive dimensionality reduction and noise filtering.<br><br>\( \text{Optimal } M \approx \text{ index at the elbow} \)
15ML0025	Basic	15ML-PCA	What is the mathematical link between reconstruction error and the choice of \(M\)?	The squared reconstruction error equals the sum of eigenvalues of discarded components: \(\sum_{i=M+1}^{D} \lambda_i\). By choosing an \(M\) that discards small \(\lambda_i\) (noise), you minimize the noise contribution to the reconstruction error.
15ML0026	Basic	15ML-PCA	What are the key benefits and a major drawback of using PCA as a preprocessing step for another model?	<b>Benefits:</b><br>1. Removes multicollinearity (components are orthogonal).<br>2. Reduces overfitting via lower dimensionality.<br>3. Speeds up training.<br>   <b>Major Drawback:</b> Loss of interpretability; new features are linear combos of all original features.
15ML0027	Basic	15ML-PCA	What is a key risk when discarding low-variance principal components?	Some low-variance directions may contain subtle but discriminative signal (e.g., for classification). Blindly truncating at a preset \(M\) or elbow can harm performance. Always validate model performance with cross-validation.
15ML0028	Basic	15ML-PCA	In a scree plot for data with isotropic noise, what characterizes the signal and noise regions?	- <b>Signal Region:</b> Steep drop in eigenvalues. Each component explains significantly less variance than the last.<br>- <b>Noise Region:</b> Flat tail. Eigenvalues are small and roughly equal (≈ noise variance). The elbow marks the transition.
15ML0029	Basic	15ML-PCA	A 100D dataset's scree plot shows a sharp drop until component 5, then a flat line. Interpret this and suggest \(M\).	The data's intrinsic dimension is likely ~5. Components 1-5 (before elbow) are signal. Components 6-100 (after elbow) are noise. Choose <b>\(M=5\) (low M)</b>. This gives strong dimensionality reduction (to 5D) and effective noise removal.
15ML0030	Basic	15ML-PCA	What is the fundamental difference in objective between PCA (unsupervised) and LDA (supervised)?	- <b>PCA:</b> Maximizes the total <b>variance</b> of the projected data (preserves overall data structure).<br>- <b>LDA:</b> Maximizes the ratio of <b>between-class scatter</b> to <b>within-class scatter</b> (enhances class separability for classification).
15ML0031	Basic	15ML-PCA	For which primary task is LDA typically better than PCA, and why?	<b>LDA is typically better for classification.</b><br>It directly optimizes for class separation by finding projections that maximize the distance between class means while minimizing within-class spread. PCA ignores class labels and may project data onto directions with high variance but poor class separation.
15ML0032	Basic	15ML-PCA	For a \(K\)-class problem, what is the maximum dimensionality LDA can reduce to, and how does this compare to PCA?	- <b>LDA:</b> Can reduce to at most \(K-1\) dimensions. For a 2-class problem, it produces only 1 discriminating direction.<br>- <b>PCA:</b> Can reduce to any \(M \leq D\) dimensions (where \(D\) is the original dimensionality). It offers more flexibility in choosing the target dimension.
15ML0033	Basic	15ML-PCA	In what scenario might PCA be a more practical choice than LDA despite LDA's theoretical advantage for classification?	<b>When you have very few data points (samples) per class.</b><br>LDA requires estimating within-class and between-class scatter matrices, which need sufficient data for stable inversion. With small \(N\), PCA (which only estimates a total covariance matrix) is more numerically stable and less prone to overfitting.
15ML0034	Basic	15ML-PCA	Why might the first principal component be a poor feature for classification, even if it captures maximum variance?	If the classes <b>overlap significantly</b> in the direction of maximum variance, projecting onto PC1 will <b>mix the classes</b> in the projected space. PCA finds directions of spread, not separation. A direction with slightly less total variance but better class separation (like LDA finds) would yield better classification accuracy.
15ML0035	Basic	15ML-PCA	Sketch 2D data from two overlapping classes with similar variance. What would PCA's PC1 vs. LDA's LD1 look like?	- <b>PCA PC1:</b> Would align with the direction of greatest combined spread, which likely runs through the overlap region, <b>failing to separate classes</b>.<br>- <b>LDA LD1:</b> Would align perpendicular to the overlap, maximizing the distance between class means relative to their spread, <b>creating better separation</b> for classification.
15ML0036	Basic	15ML-PCA	What key matrices does each method optimize?	- <b>PCA:</b> Maximizes \(\mathbf{v}^T \mathbf{S}_T \mathbf{v}\) where \(\mathbf{S}_T\) is the <b>total covariance matrix</b>.<br>- <b>LDA:</b> Maximizes \(\frac{\mathbf{v}^T \mathbf{S}_B \mathbf{v}}{\mathbf{v}^T \mathbf{S}_W \mathbf{v}}\) where \(\mathbf{S}_B\) is <b>between-class scatter</b> and \(\mathbf{S}_W\) is <b>within-class scatter</b> matrix.
15ML0037	Basic	15ML-PCA	What is the single most important factor in choosing between PCA and LDA for dimensionality reduction?	<b>The availability and use of class labels.</b><br>- If you have labels and your goal is <b>classification</b>, use <b>LDA</b> (or a regularized version if \(N\) is small).<br>- If you have no labels or your goal is <b>exploration, visualization, or noise reduction</b>, use <b>PCA</b>.
15ML0038	Basic	15ML-PCA	Why must you often standardize data (mean=0, variance=1 per feature) before PCA? Give an example.	PCA is variance-sensitive. A feature with large scale (e.g., salary in $10,000s) dominates one with small scale (e.g., age). PC1 will align nearly with that high-variance feature, potentially missing important structure. Standardizing puts all features on equal footing.
15ML0039	Basic	15ML-PCA	What are two key conceptual limitations of standard PCA?	1. <b>Linear Assumption:</b> It only captures linear correlations. Nonlinear manifold structures are lost (use Kernel PCA).<br>2. <b>Interpretability:</b> Principal components are linear combos of all original features, making them harder to explain than individual selected features.