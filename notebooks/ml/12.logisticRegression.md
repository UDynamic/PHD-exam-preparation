## Linear Combination Definition

**Front:** What is $\theta^T X$ in the context of logistic regression? `<br/>`

**Back:** It is the **linear combination** (or dot product) between the parameter vector $\theta$ and the input feature vector $X$. Mathematically: $\theta^T X = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n$, where $\theta_0$ is the bias/intercept term (often with $x_0 = 1$ included in $X$).

## Geometric Interpretation

**Front:** Geometrically, what does $\theta^T X = 0$ represent? `<br/>`

**Back:** It defines a **hyperplane** (e.g., a line in 2D) in the feature space. This hyperplane is the **decision boundary** where the model predicts exactly 0.5 probability for both classes.

## Hypothesis Function

**Front:** In binary logistic regression, what function replaces the linear hypothesis $h_\theta(x) = \theta^T x$? `<br/>`

**Back:** for activation function as $g(x)$ : $h_\theta(x) = g(\theta^T x)$

* The sigmoid activation function: $h_\theta(x) = \sigma(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}$.

$0\leq h_\theta(x) \leq1$

* `<u>`Mind the negative power of e`</u>`

## Role in Hypothesis Function

**Front:** What is the role of $\theta^T X$ in the logistic regression hypothesis function $h_\theta(x)$? `<br/>`

**Back:** $\theta^T X$ serves as the input to the sigmoid function: $h_\theta(x) = \sigma(\theta^T X) = \frac{1}{1 + e^{-\theta^T X}}$. It is the **linear predictor** or **logit**.

## Probabilistic Interpretation (Class 1)

**Front:** How is the hypothesis $h_\theta(x)$ interpreted probabilistically for binary classification? `<br/>`

**Back:** $h_\theta(x)$ represents the estimated probability that $y=1$ given $x$:

$p(y=1|x;\theta) = h_\theta(x)$.

## Probabilistic Interpretation (Class 0)

**Front:** What is the probability that $y=0$ given $x$ in binary logistic regression? `<br/>`

**Back:**

$p(y=0|x;\theta) = 1 - p(y=1|x;\theta) = 1 - h_\theta(x)$.

## Logistic Regression Classification Rule

**Front:** How does logistic regression make binary classification decisions? `<br/>`

**Back:** It compares the predicted probability $h_\theta(x) = p(y=1|x)$ to 0.5. Predict $y=1$ if $h_\theta(x) \geq0.5$, otherwise predict $y=0$. Equivalently, predict $y=1$ if $\theta^T x \geq0$, since $\sigma(0) = 0.5$.

## Compact Probability Expression

**Front:** How can we write the probability $p(y|x;\theta)$ for both classes ($y=0$ or $y=1$) in a single formula? `<br/>`

**Back:**

$p(y|x;\theta) = (h_\theta(x))^{y} \cdot (1 - h_\theta(x))^{1-y}$.

## Decision Boundary Definition

**Front:** What is the decision boundary in logistic regression, and where is it located? `<br/>`

**Back:** It is the set of points $x$ where $p(y=1|x;\theta) = p(y=0|x;\theta) = 0.5$.

This occurs when $\theta^T x = 0$.

## Region of Positive Prediction

**Front:** For what values of θᵀx will the model predict y=1? `<br/>`

**Back:** When θᵀx > 0, which means h_θ(x) = σ(θᵀx) > 0.5. The model predicts class 1 in this region.

## Region of Negative Prediction

**Front:** For what values of θᵀx will the model predict y=0? `<br/>`

**Back:** When θᵀx < 0, which means h_θ(x) = σ(θᵀx) < 0.5. The model predicts class 0 in this region.

## Likelihood Function

**Front:** For a training set of $m$ i.i.d. examples, what is the likelihood function $L(\theta)$? `<br/>`

**Back:** The product of individual probabilities: $L(\theta) = \prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta) = \prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}} (1-h_\theta(x^{(i)}))^{1-y^{(i)}}$.

* this is a Mximization problem

## Log-Likelihood

**Front:** Why do we take the log of the likelihood, and what is $l(\theta) = \log L(\theta)$? `<br/>`

**Back:** Log transforms products into sums, simplifying calculus. $l(\theta) = \sum_{i=1}^m [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]$.

* this is a Mximization problem

## Cost Function (Negative Log-Likelihood)

**Front:** What is the standard cost function $J(\theta)$ for logistic regression, and how is it derived? `<br/>`

**Back:** It is the average negative log-likelihood. `<br/>`

$J(\theta) = -l(\theta) = - \sum_{i=1}^m [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]$

Average form :`<br/>`

$J(\theta) = -\frac{1}{m} l(\theta)$

* Minimizing $J(\theta)$ is equivalent to maximizing the likelihood $L(\theta)$.

## Sum of Squared Errors (SSE) Problem

**Front:** In linear regression, the cost function is Sum of Squared Errors (SSE). Why would SSE be a poor choice for logistic regression? `<br/>`

**Back:** SSE $J(\theta) = \frac{1}{2}\sum (h_\theta(x^{(i)}) - y^{(i)})^2$ applied to $h_\theta(x)=\sigma(\theta^T x)$ yields a **non-convex** function with many local minima. Gradient descent might not find the global optimum.

## Gradient Descent Update Rule

**Front:** What is the parameter update rule for gradient descent using the cost $J(\theta) = - l(\theta)$? `<br/>`

**Back:**

General form: $\begin{cases} \theta_j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial\theta_j}\\ \frac{\partial J(\theta)}{\partial\theta_j} = \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \end{cases}$

$\Rightarrow$

$\begin{cases} \Delta\theta_0 = - \alpha[\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})] & j=0\\ \Delta\theta_j = - \alpha[\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}] & j \geq1\end{cases}$

## Special Consideration: Regularization in Cost

**Front:** How is L2 regularization typically added to the logistic regression cost function $J(\theta)$? `<br/>`

**Back:** $J_{reg}(\theta) = -l(\theta) + \frac{\lambda}{2} \sum_{j=1}^n \theta_j^2$.

The bias term $\theta_0$ is often excluded from regularization.

## Gradient Descent Update Rule with L2 regularization

**Front:** What is the parameter update rule for gradient descent using L2 regularization? `<br/>`

**Back:**

General form:

$\begin{cases} J(\theta) = - l(\theta) + \frac{\lambda}{2} \sum_{j=1}^n \theta^2

\\ \theta_j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial\theta_j}

\\ \frac{\partial J(\theta)}{\partial\theta_j} = \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda\theta_j\end{cases}$

$\Rightarrow$

$\begin{cases} \Delta\theta_0 = - \alpha[\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})] & j=0\\ \Delta\theta_j = - \alpha[\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda\theta_j] & j \geq1\end{cases}$

## Derivative of the Sigmoid

**Front:** What is the derivative of the sigmoid function $\sigma(z)$, and why is this useful? `<br/>`

**Back:** $\frac{d}{dz}\sigma(z) = \sigma(z)(1 - \sigma(z))$. This simplifies the gradient calculation for the log-likelihood.

## Multi-class: One-vs-Rest (OvR)

**Front:** How does the One-vs-Rest (OvR) strategy work for multi-class classification with logistic regression? `<br/>`

**Back:** For $K$ classes, train $K$ separate binary classifiers. Classifier $k$ learns to distinguish class $k$ (positive) from all other classes (negative). Prediction is the class with the highest $h_\theta^{(k)}(x)$.

* as there is K classes, there will be K linear classifications.

## Multi-class: One-vs-One (OvO)

**Front:** How does the One-vs-One (OvO) strategy work? `<br/>`

**Back:** Train a binary classifier for every pair of classes. This requires $\binom{K}{2} = \frac{K(K-1)}{2}$ classifiers.

* Prediction is made by majority vote from all classifiers. in other words The class that appears as the winner in the largest number of pairwise comparisons is selected.

## what is Softmax

**Front:** what is Softmax ? `<br/>`

**Back:** Softmax takes n numbers like $z_1, z_2, \dots z_n$ and Outputs n probabilities that sum to 1.

General formula : $\text{softmax}_i= \frac{e^{z_i}}{\sum_{k=1}^n e^{z_k}} $

* $\sum_{i=1}^n \text{softmax}_i = 1$

## Multi-class: Softmax Regression

**Front:** What is the direct multi-class generalization of logistic regression called, and what is its hypothesis function? `<br/>`

**Back:** Softmax Regression. For class $k$ with parameters $\theta^{(k)}$, $p(y=k|x;\Theta) = \frac{e^{\theta^{(k)T} x}}{\sum_{j=1}^K e^{\theta^{(j)T} x}}$, where $\Theta$ is the matrix of all parameters.

## Pitfall: Interpreting Raw Scores

**Front:** Before applying the sigmoid or softmax, what do the raw scores $\theta^T x$ or $\theta^{(k)T} x$ represent? `<br/>`

**Back:** They are log-odds (for binary) or logits. They exist on the whole real line and are not probabilities. The activation function converts them to probabilities.

## Linear Classifier

**Front:** Why is logistic regression considered a **linear classifier** despite using a nonlinear sigmoid function? `<br/>`

**Back:** Because its decision boundary is linear. Predict $y=1$ when $\theta^Tx > 0$ and $y=0$ when $\theta^Tx < 0$. The boundary $\theta^Tx = 0$ defines a hyperplane in feature space.

## Bernoulli Distribution Assumption

**Front:** What probability distribution does logistic regression assume for the target variable $y$ given input $x$? `<br/>`

**Back:** Bernoulli distribution: $P(y|x; \theta) = h_\theta(x)^y (1 - h_\theta(x))^{1-y}$, where $h_\theta(x) = \sigma(\theta^Tx)$ is the predicted probability that $y=1$.

## Discriminative Model

**Front:** What type of model is logistic regression—generative or discriminative—and what does it directly model? `<br/>`

**Back:** Discriminative. It directly models the posterior probability $P(y|x)$ without modeling the class-conditional distributions $P(x|y)$ (which would be generative).

## Distance to Decision Boundary

**Front:** How does the signed distance $|\theta^Tx|$ relate to a point's position relative to the decision boundary? `<br/>`

**Back:** $|\theta^Tx|$ is proportional to the **perpendicular distance** from point $x$ to the hyperplane $\theta^Tx = 0$. Larger magnitude means greater distance and higher confidence.
