#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

18ML0001	Basic	18ML-EnsembleLearning	What is the core idea behind ensemble learning in machine learning?	Combine predictions from multiple base models (learners) to create a single, more accurate and robust predictive model. The principle is that a group of weak learners can together form a strong learner.
18ML0002	Basic	18ML-EnsembleLearning	Name common, simple fusion functions used to combine predictions from multiple classifiers in an ensemble.	For regression or classifier confidence scores: Sum, Weighted Sum. For classifier votes: Majority Voting (implicitly uses sum), Median, Minimum, Maximum, Product. Weighted sum is most common for boosting.
18ML0003	Basic	18ML-EnsembleLearning	What is the primary statistical flaw each technique (Bagging & Boosting) aims to reduce? Provide a mnemonic and the correct reason.	**Mnemonic:** "Bag" and "Var" share the 'a' sound; Bagging reduces Variance. Boosting (the other one) reduces Bias.<br>**Truth:** Bagging reduces variance by averaging over models fit to bootstrapped datasets. Boosting reduces bias by sequentially focusing on misclassified examples.
18ML0004	Basic	18ML-EnsembleLearning	What is the fundamental operational difference between sequential (e.g., boosting) and parallel (e.g., bagging) ensemble methods?	Sequential methods build base learners one after another, where each new learner tries to correct the errors of the current ensemble. Parallel methods build all base learners independently, often on different data subsets.
18ML0005	Basic	18ML-EnsembleLearning	What is the fundamental strategy of the boosting family of algorithms?	Sequentially train simple, weak learners (e.g., decision stumps). After each step, increase the weight of training instances that were misclassified, forcing the next learner to focus more on these hard examples. Finally, combine all weak learners via a weighted vote.
18ML0006	Basic	18ML-EnsembleLearning	What is a typical "weak learner" in boosting, and what is a "decision stump"?	A weak learner is any model that performs slightly better than random guessing (error < 0.5). A **decision stump** is a one-level decision tree (a single split/rule), which is a common, simple weak learner.
18ML0007	Basic	18ML-EnsembleLearning	In AdaBoost, how are the training instance weights initialized, and what is the goal for each weak learner's error?	All N training instance weights are initialized equally: \( w_i^{(1)} = \frac{1}{N} \). The goal for weak learner \( h_m \) is to achieve a **weighted classification error \( \epsilon_m \) less than 0.5** (better than random).
18ML0008	Basic	18ML-EnsembleLearning	In AdaBoost iteration *m*, how is the weighted error \( \epsilon_m \) calculated, and how is the importance \( \alpha_m \) of the weak learner \( h_m \) determined?	\( \epsilon_m = \frac{\sum_{i=1}^{N} w_i^{(m)} \cdot \mathbb{I}(h_m(x_i) \ne y_i)}{\sum_{i=1}^{N} w_i^{(m)} } \)<br>* it's the sum of weights for missclassified instances.<br><br>The learner weight is: \( \alpha_m = \frac{1}{2} \ln\left(\frac{1 - \epsilon_m}{\epsilon_m}\right) \). A lower error \( \epsilon_m \) yields a higher \( \alpha_m \).
18ML0009	Basic	18ML-EnsembleLearning	After adding weak learner \( h_m \) with weight \( \alpha_m \), how are the instance weights updated for iteration *m+1* in AdaBoost?	\( w_i^{(m+1)} = w_i^{(m)} \cdot \exp\left(-\alpha_m \cdot y_i \cdot h_m(x_i)\right) \), then normalized. Correctly classified (\( y_i h_m(x_i)=+1 \)) weights decrease. Misclassified (\( y_i h_m(x_i)=-1 \)) weights increase.
18ML0010	Basic	18ML-EnsembleLearning	How does the final AdaBoost classifier \( H \) make a prediction for a new input \( x \)?	It takes a weighted vote of all weak learners:<br>\( H(x) = \text{sign}\left( \sum_{m=1}^{M} \alpha_m h_m(x) \right) \).
18ML0011	Basic	18ML-EnsembleLearning	What loss function does AdaBoost implicitly minimize? Describe its behavior.	AdaBoost minimizes the **exponential loss**: \( L(y, H(x)) = \exp(-y \cdot H(x)) \). It heavily penalizes misclassifications (large negative \( yH(x) \)), driving the algorithm to focus on hard examples.
18ML0012	Basic	18ML-EnsembleLearning	What is the bound on the training error of the AdaBoost ensemble after M rounds?	\( \frac{1}{N} \sum_{i=1}^N \mathbb{I}(H(x_i) \ne y_i) \le \prod_{m=1}^{M} 2\sqrt{\epsilon_m(1-\epsilon_m)} \). Since \( \epsilon_m < 0.5 \), each term is \( <1 \), so the bound (and typically the error) decreases.
18ML0013	Basic	18ML-EnsembleLearning	In AdaBoost's training error bound inequality, what do the left-hand side (LHS) and right-hand side (RHS) represent?	**LHS:** \( \frac{1}{N} \sum_{i=1}^N \mathbb{I}(H(x_i) \ne y_i) \)<br>This is the *actual training error* - the fraction of training examples misclassified by the final ensemble \( H(x) \).<br><br>**RHS:** \( \prod_{m=1}^{M} 2\sqrt{\epsilon_m(1-\epsilon_m)} \)<br>This is an *upper bound* on that training error, calculated from the weighted errors \( \epsilon_m \) of each weak learner.<br><br>**Comparison:** The inequality \( \text{LHS} \le \text{RHS} \) means: "The actual training error is guaranteed to be less than or equal to this product of terms."
18ML0014	Basic	18ML-EnsembleLearning	What are key misunderstandings about AdaBoost's training error bound?	**Pitfall 1:** "The product IS the training error." ❌ Wrong! The product is an *upper bound*. Actual error is lower.<br><br>**Pitfall 2:** "If bound goes to zero, test error goes to zero." ❌ Wrong! This only applies to *training* error. Test error may increase due to overfitting.<br><br>**Pitfall 3:** "Each factor ≈ 1 means no progress." ❌ Misleading! Even if \( 2\sqrt{\epsilon(1-\epsilon)} = 0.99 \), after 100 rounds the bound is \( 0.99^{100} ≈ 0.37 \), still showing guaranteed improvement.<br><br>**Key Insight:** The inequality shows AdaBoost *must* reduce training error if weak learners are better than random, but says nothing about generalization to new data.
18ML0015	Basic	18ML-EnsembleLearning	What are the practical implications and limitations of AdaBoost's training error bound?	**Implications:**<br>1. Guarantees training error reduction if each \( \epsilon_m < 0.5 \).<br>2. Shows error decreases *exponentially* with number of rounds \( M \).<br><br>**Limitations/Pitfalls:**<br>1. **Bound vs. Reality:** The bound may be loose; actual error decreases slower.<br>2. **Test Error:** This is only for *training* error. Test error may increase due to overfitting, especially with noisy data.<br>3. **Weak Learner Quality:** If \( \epsilon_m \) approaches 0.5, the factor approaches 1, and progress slows drastically.<br>4. **Noise Sensitivity:** The bound assumes weak learners can achieve \( \epsilon_m < 0.5 \). With noisy labels, this may fail, causing poor performance.<br><br>**Key Insight:** The product bound shows AdaBoost aggressively minimizes training error, explaining its tendency to eventually overfit on noisy datasets if run for too many rounds.
18ML0016	Basic	18ML-EnsembleLearning	Is AdaBoost prone to overfitting? Under what conditions might it occur?	AdaBoost is often resistant to overfitting, but it can happen with: 1) **Very noisy data** (the algorithm keeps trying to fit outliers), 2) **Too many rounds (M)** on complex data. Remedies: Early stopping, cross-validation to choose M.
18ML0017	Basic	18ML-EnsembleLearning	Define the Bagging algorithm. What statistical property does it primarily improve?	1. Create *B* bootstrap samples (random subsets with replacement) from the training set.<br>2. Train a base learner (e.g., decision tree) on each sample.<br>3. Aggregate predictions via majority vote (classification) or averaging (regression).<br>   It primarily **reduces variance**.
18ML0018	Basic	18ML-EnsembleLearning	What condition is necessary for bagging to effectively improve performance over a single model?	The base learners must be **diverse** (make different errors). If they are identical, bagging offers no improvement. Diversity is achieved by training on different data subsets.
18ML0019	Basic	18ML-EnsembleLearning	What is an "unstable" learner, and why are they ideal for bagging?	An unstable learner is one whose output changes significantly with small changes in the training data (high variance). Bagging stabilizes them by averaging. **Decision trees** are classic unstable learners, making them ideal for bagging.
18ML0020	Basic	18ML-EnsembleLearning	How does the Random Forest algorithm extend basic bagging for decision trees to increase diversity further?	When splitting a node during tree construction, it considers only a **random subset of features** (e.g., \( \sqrt{D} \)) instead of all \( D \) features. This decorrelates the trees more than bagging alone.
18ML0021	Basic	18ML-EnsembleLearning	Compare the sensitivity of Boosting and Bagging to noisy data and outliers.	**Boosting** is more sensitive; it tries to fit hard examples, which can be noise, leading to potential overfitting. **Bagging** is more robust; averaging over bootstraps dilutes the influence of any single outlier.
18ML0022	Basic	18ML-EnsembleLearning	Correct this pitfall: "In boosting, the first weak learner uses the simplest data points, and their probability is decreased for the next round."	This is incorrect. All data points start with equal weight/probability. The algorithm doesn't pre-select "simple" points. It *discovers* hard points (those consistently misclassified) and their weights **increase** over rounds.
18ML0023	Basic	18ML-EnsembleLearning	Correct this pitfall from the notes: "Decision trees are stable classifiers because of Information Gain."	This is incorrect. Decision trees are **unstable** (high-variance) learners. Small data changes can lead to completely different splits/trees, despite using IG. This instability is *why* bagging works so well on them.