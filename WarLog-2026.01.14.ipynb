{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dad8e3",
   "metadata": {},
   "source": [
    "## Sensitibvity analysis method for backpropagation\n",
    "\n",
    "The sensitivity analysis in backpropagation involves calculating the sensitivity of the output with respect to the weights in the network. This is done by computing the partial derivatives of the output with respect to each weight. The general formulation for sensitivity analysis is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ E $ is the error (or loss) function.\n",
    "- $ w_{ij} $ is the weight connecting the $ i $-th neuron in the previous layer to the $ j $-th neuron in the current layer.\n",
    "- $ a_j $ is the activation of the $ j $-th neuron.\n",
    "- $ z_j $ is the weighted sum of inputs to the $ j $-th neuron.\n",
    "\n",
    "The first term, $ \\frac{\\partial E}{\\partial a_j} $, is the derivative of the error with respect to the activation of the neuron. The second term, $ \\frac{\\partial a_j}{\\partial z_j} $, is the derivative of the activation function with respect to the weighted sum. The third term, $ \\frac{\\partial z_j}{\\partial w_{ij}} $, is simply the input to the neuron $ a_i $.\n",
    "\n",
    "This formulation allows us to compute the sensitivity of the error with respect to each weight in the network, which is essential for updating the weights during the training process. Let me know if you have any specific questions about this formulation or would like to move on to the SSE method! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ae06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f81844e",
   "metadata": {},
   "source": [
    "I'm glad you asked for an explanation in English! Let's go through a **simple example of the Backpropagation algorithm** in a **3-layer neural network** (Input â†’ Hidden â†’ Output).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Network Structure:\n",
    "\n",
    "- **Input Layer:** 2 neurons  \n",
    "- **Hidden Layer:** 2 neurons  \n",
    "- **Output Layer:** 1 neuron  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 1: Forward Propagation\n",
    "\n",
    "Letâ€™s define the inputs and weights:\n",
    "\n",
    "- **Inputs:**  \n",
    "  $ x_1 = 0.05 $, $ x_2 = 0.10 $\n",
    "\n",
    "- **Weights (Hidden Layer):**  \n",
    "  $ w_1 = 0.15 $, $ w_2 = 0.20 $ (for hidden neuron 1)  \n",
    "  $ w_3 = 0.25 $, $ w_4 = 0.30 $ (for hidden neuron 2)\n",
    "\n",
    "- **Weights (Output Layer):**  \n",
    "  $ w_5 = 0.40 $, $ w_6 = 0.45 $\n",
    "\n",
    "- **Bias (optional):** For simplicity, we'll ignore bias in this example.\n",
    "\n",
    "- **Activation Function:** Sigmoid  \n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Hidden Layer Output:\n",
    "\n",
    "$$\n",
    "h_1 = \\sigma(w_1 x_1 + w_2 x_2) = \\sigma(0.15 \\cdot 0.05 + 0.20 \\cdot 0.10) = \\sigma(0.0375) \\approx 0.509\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\sigma(w_3 x_1 + w_4 x_2) = \\sigma(0.25 \\cdot 0.05 + 0.30 \\cdot 0.10) = \\sigma(0.0425) \\approx 0.511\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Output Layer:\n",
    "\n",
    "$$\n",
    "o = \\sigma(w_5 h_1 + w_6 h_2) = \\sigma(0.40 \\cdot 0.509 + 0.45 \\cdot 0.511) = \\sigma(0.43355) \\approx 0.606\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 2: Calculate the Error\n",
    "\n",
    "Letâ€™s say the **target output** is $ y = 0.01 $\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2}(y - o)^2 = \\frac{1}{2}(0.01 - 0.606)^2 \\approx 0.186\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 3: Backpropagation (Compute Gradients)\n",
    "\n",
    "We want to compute how much each weight contributed to the error.\n",
    "\n",
    "#### ðŸ”¸ Output Layer Gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial o} = -(y - o) = -(0.01 - 0.606) = 0.596\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial o}{\\partial z_o} = \\sigma'(z_o) = o(1 - o) = 0.606 \\cdot (1 - 0.606) \\approx 0.239\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial z_o} = \\frac{\\partial E}{\\partial o} \\cdot \\frac{\\partial o}{\\partial z_o} = 0.596 \\cdot 0.239 \\approx 0.142\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_5} = \\frac{\\partial E}{\\partial z_o} \\cdot h_1 = 0.142 \\cdot 0.509 \\approx 0.072\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_6} = \\frac{\\partial E}{\\partial z_o} \\cdot h_2 = 0.142 \\cdot 0.511 \\approx 0.0726\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¸ Hidden Layer Gradients:\n",
    "\n",
    "We now propagate the error back to the hidden layer.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial h_1} = \\frac{\\partial E}{\\partial z_o} \\cdot w_5 = 0.142 \\cdot 0.40 = 0.0568\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial z_h1} = \\sigma'(z_h1) = h_1(1 - h_1) = 0.509 \\cdot (1 - 0.509) \\approx 0.250\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial z_h1} = \\frac{\\partial E}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_h1} = 0.0568 \\cdot 0.250 \\approx 0.0142\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_1} = \\frac{\\partial E}{\\partial z_h1} \\cdot x_1 = 0.0142 \\cdot 0.05 = 0.00071\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_2} = \\frac{\\partial E}{\\partial z_h1} \\cdot x_2 = 0.0142 \\cdot 0.10 = 0.00142\n",
    "$$\n",
    "\n",
    "Repeat similar steps for $ w_3 $ and $ w_4 $.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Step 4: Update Weights\n",
    "\n",
    "Using **Gradient Descent**:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial E}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where $ \\eta $ is the learning rate (e.g., $ \\eta = 0.5 $).\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary\n",
    "\n",
    "- Forward pass computes outputs using weights and activation functions.\n",
    "- Backpropagation computes gradients of the error with respect to each weight.\n",
    "- Weights are updated using gradient descent to reduce the error.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a **Python code** example to implement this? I can write it for you! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce75ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57e1b617",
   "metadata": {},
   "source": [
    "Hey there! ðŸ˜Š  \n",
    "Thanks for your question â€” this is a great opportunity to dive deep into the **chain rule** in the context of **neural networks**, especially for the **output layer gradients**. Let's break it down step by step so you can fully understand how this works.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Chain Rule in Neural Networks\n",
    "\n",
    "The chain rule is a fundamental concept in calculus, and it becomes **crucial** in **backpropagation**, the algorithm used to train neural networks.\n",
    "\n",
    "The formula you're referring to is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "Letâ€™s go through each term and explain what it means in the context of a **neural network**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© 1. $ \\frac{\\partial E}{\\partial a_j} $:  \n",
    "This is the **derivative of the error** with respect to the **activation** of the $ j $-th neuron.\n",
    "\n",
    "- $ E $ is the **error (or loss)** function â€” it tells us how far off our prediction is from the true value.\n",
    "- $ a_j $ is the **activation** of the $ j $-th neuron, i.e., the output of the neuron after the activation function is applied.\n",
    "- This term tells us how sensitive the error is to the output of the neuron.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© 2. $ \\frac{\\partial a_j}{\\partial z_j} $:  \n",
    "This is the **derivative of the activation function** with respect to the **weighted sum** $ z_j $.\n",
    "\n",
    "- $ z_j = w_{1j}a_1 + w_{2j}a_2 + \\dots + w_{nj}a_n + b_j $, where $ b_j $ is the bias.\n",
    "- This term tells us how sensitive the output of the neuron is to changes in the **input to the neuron** (i.e., the weighted sum).\n",
    "- For example, if the activation function is the **sigmoid** function $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $, then:\n",
    "  $$\n",
    "  \\frac{\\partial a_j}{\\partial z_j} = \\sigma'(z_j) = \\sigma(z_j)(1 - \\sigma(z_j))\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© 3. $ \\frac{\\partial z_j}{\\partial w_{ij}} $:  \n",
    "This is the **derivative of the weighted sum** with respect to the **weight** $ w_{ij} $.\n",
    "\n",
    "- Since $ z_j = w_{ij}a_i + \\text{other terms} $, the derivative is simply:\n",
    "  $$\n",
    "  \\frac{\\partial z_j}{\\partial w_{ij}} = a_i\n",
    "  $$\n",
    "- This is because the derivative of $ w_{ij}a_i $ with respect to $ w_{ij} $ is just $ a_i $.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Putting It All Together\n",
    "\n",
    "So the full chain rule becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\left( \\frac{\\partial E}{\\partial a_j} \\right) \\cdot \\left( \\frac{\\partial a_j}{\\partial z_j} \\right) \\cdot \\left( \\frac{\\partial z_j}{\\partial w_{ij}} \\right)\n",
    "$$\n",
    "\n",
    "This tells us how the **error** changes with respect to a **specific weight** in the network. This is the core of **backpropagation** â€” we compute these derivatives and use them to **update the weights** in the network to reduce the error.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¸ Example: Output Layer Gradients\n",
    "\n",
    "Letâ€™s now walk through the example you provided for the **output layer**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. $ \\frac{\\partial E}{\\partial o} = -(y - o) = -(0.01 - 0.606) = 0.596 $\n",
    "\n",
    "- This is the **derivative of the error** with respect to the **output** $ o $.\n",
    "- The loss function here is **Mean Squared Error (MSE)**:  \n",
    "  $$\n",
    "  E = \\frac{1}{2}(y - o)^2 \\Rightarrow \\frac{\\partial E}{\\partial o} = -(y - o)\n",
    "  $$\n",
    "- So, the error increases when the output is far from the true value.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. $ \\frac{\\partial o}{\\partial z_o} = \\sigma'(z_o) = o(1 - o) = 0.606 \\cdot (1 - 0.606) \\approx 0.239 $\n",
    "\n",
    "- This is the **derivative of the activation function** (sigmoid) with respect to the **weighted sum** $ z_o $.\n",
    "- It tells us how sensitive the output is to changes in the input to the neuron.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. $ \\frac{\\partial E}{\\partial z_o} = \\frac{\\partial E}{\\partial o} \\cdot \\frac{\\partial o}{\\partial z_o} = 0.596 \\cdot 0.239 \\approx 0.142 $\n",
    "\n",
    "- This is the **error gradient with respect to the weighted sum** $ z_o $.\n",
    "- It combines the sensitivity of the error to the output and the sensitivity of the output to the input.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. $ \\frac{\\partial E}{\\partial w_5} = \\frac{\\partial E}{\\partial z_o} \\cdot h_1 = 0.142 \\cdot 0.509 \\approx 0.072 $\n",
    "\n",
    "- $ h_1 $ is the **activation** from the previous layer (hidden layer).\n",
    "- This is the **error gradient with respect to the weight** $ w_5 $.\n",
    "- It tells us how much the error would change if we adjusted the weight $ w_5 $.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. $ \\frac{\\partial E}{\\partial w_6} = \\frac{\\partial E}{\\partial z_o} \\cdot h_2 = 0.142 \\cdot 0.511 \\approx 0.0726 $\n",
    "\n",
    "- Same logic as above, but for weight $ w_6 $.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ Summary Table\n",
    "\n",
    "| Derivative | Meaning | Value |\n",
    "|------------|---------|-------|\n",
    "| $ \\frac{\\partial E}{\\partial o} $ | Error w.r.t. output | 0.596 |\n",
    "| $ \\frac{\\partial o}{\\partial z_o} $ | Activation derivative | 0.239 |\n",
    "| $ \\frac{\\partial E}{\\partial z_o} $ | Error w.r.t. weighted sum | 0.142 |\n",
    "| $ \\frac{\\partial E}{\\partial w_5} $ | Error w.r.t. weight $ w_5 $ | 0.072 |\n",
    "| $ \\frac{\\partial E}{\\partial w_6} $ | Error w.r.t. weight $ w_6 $ | 0.0726 |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final Thoughts\n",
    "\n",
    "This is the **core of backpropagation** â€” using the chain rule to propagate the error **backwards** through the network and compute how each weight contributes to the error. Once you have these gradients, you can **update the weights** using an optimization algorithm like **Gradient Descent**.\n",
    "\n",
    "If you'd like, I can also show you how to implement this in **Python** or help you go through the **hidden layer gradients** too. Just let me know! ðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
