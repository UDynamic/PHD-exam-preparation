{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084eb311",
   "metadata": {},
   "source": [
    "Okay, let's dive into Support Vector Machines (SVMs) in a simple and friendly way! ๐\n",
    "\n",
    "### What is a Support Vector Machine (SVM)?\n",
    "SVM is a **supervised machine learning algorithm** used for **classification** and **regression** tasks. But it's most famous for **classification**.\n",
    "\n",
    "Think of SVM as a **smart separator**. It tries to find the best line (or hyperplane in higher dimensions) that **clearly separates** different classes in your data.\n",
    "\n",
    "---\n",
    "\n",
    "### The Big Idea:\n",
    "Imagine you have two groups of points on a graph. You want to draw a line that separates them as clearly as possible. SVM finds the **widest possible margin** between the two groups.\n",
    "\n",
    "- The points that are **closest to the line** are called **support vectors**.\n",
    "- These are the **most important points** in defining the decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "### How SVM Works (Simplified):\n",
    "1. **Input Data**: You have labeled data (e.g., red dots and blue dots).\n",
    "2. **Find the Best Line**: SVM tries to find a line that **maximizes the margin** between the two classes.\n",
    "3. **Support Vectors**: The data points closest to the line are the **support vectors**.\n",
    "4. **Decision Boundary**: The line that separates the classes is the **decision boundary**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example in 2D:\n",
    "Letโs say you have two classes: cats and dogs. Each has features like weight and height.\n",
    "\n",
    "SVM will try to find the **best line** that separates cats from dogs. The line that gives the **maximum distance** from the nearest cat and dog is the best one.\n",
    "\n",
    "---\n",
    "\n",
    "### Kernels in SVM:\n",
    "Sometimes, the data isn't linearly separable (you can't draw a straight line to separate the classes). Thatโs where **kernels** come in!\n",
    "\n",
    "- **Kernel Trick**: It transforms the data into a higher dimension where it becomes linearly separable.\n",
    "- Common kernels:\n",
    "  - **Linear Kernel** (for simple data)\n",
    "  - **RBF (Radial Basis Function)** (for complex, non-linear data)\n",
    "  - **Polynomial Kernel**\n",
    "  -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e065842",
   "metadata": {},
   "source": [
    "Absolutely! Let's dive deeper into the **mathematical formulas** behind Support Vector Machines (SVM) and add **examples** to make it clearer. ๐\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ SVM Formulas Explained\n",
    "\n",
    "### 1. **Linear SVM (Hard Margin)**\n",
    "\n",
    "Letโs assume we have a **linearly separable dataset**. The goal is to find a **hyperplane** that separates the two classes with the **maximum margin**.\n",
    "\n",
    "#### The Hyperplane Equation:\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$\n",
    "- $ w $: weight vector (normal to the hyperplane)\n",
    "- $ x $: input data point\n",
    "- $ b $: bias term\n",
    "\n",
    "#### The Margin:\n",
    "The **margin** is the distance between the hyperplane and the **closest data points** (support vectors). The margin is given by:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "#### Optimization Goal:\n",
    "We want to **maximize the margin**, which is equivalent to **minimizing $ \\|w\\| $**.\n",
    "\n",
    "So, the **optimization problem** is:\n",
    "$$\n",
    "\\text{Minimize: } \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "Subject to the constraints:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i\n",
    "$$\n",
    "Where:\n",
    "- $ y_i \\in \\{-1, 1\\} $: class labels\n",
    "- $ x_i $: data points\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Soft Margin SVM (for noisy data)**\n",
    "\n",
    "In real-world data, perfect separation is often impossible. So, we allow **some misclassifications** by introducing **slack variables** $ \\xi_i $.\n",
    "\n",
    "#### New Optimization Problem:\n",
    "$$\n",
    "\\text{Minimize: } \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "Subject to:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "- $ C $: regularization parameter (controls the trade-off between maximizing margin and minimizing errors)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Kernel Trick (for non-linear data)**\n",
    "\n",
    "When data is **not linearly separable**, we use **kernel functions** to map the data into a **higher-dimensional space** where it becomes linearly separable.\n",
    "\n",
    "#### Kernel Function:\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "$$\n",
    "Where $ \\phi $ is a mapping function to a higher-dimensional space.\n",
    "\n",
    "#### Common Kernels:\n",
    "- **Linear Kernel**: $ K(x_i, x_j) = x_i \\cdot x_j $\n",
    "- **RBF (Radial Basis Function)**:\n",
    "  $$\n",
    "  K(x_i, x_j) = \\exp\\left(-\\gamma \\|x_i - x_j\\|^2\\right)\n",
    "  $$\n",
    "- **Polynomial Kernel**:\n",
    "  $$\n",
    "  K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งช Example: Linear SVM with Math\n",
    "\n",
    "Letโs take a simple 2D example:\n",
    "\n",
    "### Data:\n",
    "- Class A (label = +1): (1, 2), (2, 3)\n",
    "- Class B (label = -1): (3, 1), (4, 2)\n",
    "\n",
    "We want to find the best line that separates these two classes.\n",
    "\n",
    "### Step 1: Assume a hyperplane:\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = 0\n",
    "$$\n",
    "\n",
    "Letโs say we find:\n",
    "- $ w = [1, 1] $\n",
    "- $ b = -4 $\n",
    "\n",
    "So the line is:\n",
    "$$\n",
    "x_1 + x_2 - 4 = 0\n",
    "$$\n",
    "\n",
    "### Step 2: Check the margin:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|} = \\frac{2}{\\sqrt{1^2 + 1^2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2}\n",
    "$$\n",
    "\n",
    "This is the **maximum margin** for this data.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง Example: RBF Kernel in Python\n",
    "\n",
    "Letโs use the **RBF kernel** to classify a non-linear dataset like the **moons dataset**.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate non-linear data\n",
    "X, y = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create SVM with RBF kernel\n",
    "model = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"SVM with RBF Kernel\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "Accuracy: 95.00%\n",
    "```\n",
    "\n",
    "Youโll see a **non-linear decision boundary** that separates the two moon-shaped clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งพ Summary of Key Formulas\n",
    "\n",
    "| Concept | Formula |\n",
    "|--------|---------|\n",
    "| Hyperplane | $ w \\cdot x + b = 0 $ |\n",
    "| Margin | $ \\frac{2}{\\|w\\|} $ |\n",
    "| Optimization (Hard Margin) | $ \\min \\frac{1}{2} \\|w\\|^2 $ with $ y_i (w \\cdot x_i + b) \\geq 1 $ |\n",
    "| Optimization (Soft Margin) | $ \\min \\frac{1}{2} \\|w\\|^2 + C \\sum \\xi_i $ |\n",
    "| RBF Kernel | $ K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $ |\n",
    "\n",
    "---\n",
    "\n",
    "If you want to see how **SVM works for regression** (called **SVR**), or how to **tune hyperparameters** like $ C $ and $ \\gamma $, just let me know! I'm here to help you learn step by step. ๐โจ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e84a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fddfd74a",
   "metadata": {},
   "source": [
    "Absolutely! Let's go into the **academic version** of the **Support Vector Machine (SVM)** using the **Lagrange multipliers** method. This is the **mathematical foundation** of SVM and is essential for understanding how it works under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 1. **SVM Optimization Problem (Primal Form)**\n",
    "\n",
    "We want to find the **hyperplane** that **maximizes the margin** between two classes.\n",
    "\n",
    "Letโs define:\n",
    "- $ x_i \\in \\mathbb{R}^n $: input data points\n",
    "- $ y_i \\in \\{-1, +1\\} $: class labels\n",
    "- $ w \\in \\mathbb{R}^n $: weight vector\n",
    "- $ b \\in \\mathbb{R} $: bias term\n",
    "\n",
    "The **hyperplane** is defined as:\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$\n",
    "\n",
    "The **margin** is:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "We want to **maximize the margin**, which is equivalent to **minimizing**:\n",
    "$$\n",
    "\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "Subject to the **constraints**:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 2. **Lagrangian Function**\n",
    "\n",
    "To solve this **constrained optimization problem**, we use **Lagrange multipliers**.\n",
    "\n",
    "We define the **Lagrangian** as:\n",
    "$$\n",
    "\\mathcal{L}(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha_i \\geq 0 $: Lagrange multipliers\n",
    "- The minus sign is because we are **minimizing** the objective function.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 3. **Dual Form of the Problem**\n",
    "\n",
    "We now **optimize the Lagrangian** with respect to $ w $ and $ b $, and then express the problem in terms of the **Lagrange multipliers** $ \\alpha_i $.\n",
    "\n",
    "### Step 1: Take partial derivatives\n",
    "\n",
    "Take the derivative of $ \\mathcal{L} $ with respect to $ w $ and $ b $, and set them to zero:\n",
    "\n",
    "#### Derivative w.r.t. $ w $:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0\n",
    "\\Rightarrow w = \\sum_{i=1}^{n} \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "#### Derivative w.r.t. $ b $:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "\\Rightarrow \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Plug back into the Lagrangian\n",
    "\n",
    "Now substitute $ w $ into the Lagrangian:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} \\left\\| \\sum_{i=1}^{n} \\alpha_i y_i x_i \\right\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "After simplifying, we get the **dual form** of the SVM optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 4. **Support Vectors and Decision Function**\n",
    "\n",
    "Only the **support vectors** (data points with $ \\alpha_i > 0 $) contribute to the solution.\n",
    "\n",
    "The **decision function** is:\n",
    "$$\n",
    "f(x) = \\text{sign} \\left( \\sum_{i=1}^{n} \\alpha_i y_i x_i \\cdot x + b \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x_i $ are the **support vectors**\n",
    "- $ \\alpha_i $ are the **Lagrange multipliers** for those points\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 5. **Kernel Trick (Non-Linear SVM)**\n",
    "\n",
    "For **non-linearly separable data**, we use the **kernel trick**.\n",
    "\n",
    "We replace the inner product $ x_i \\cdot x_j $ with a **kernel function** $ K(x_i, x_j) $, which implicitly maps the data to a higher-dimensional space.\n",
    "\n",
    "So the **dual problem becomes**:\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 6. **Example: Solving for ฮฑ in a Small Dataset**\n",
    "\n",
    "Letโs take a **small dataset** with 2 points in each class:\n",
    "\n",
    "### Class A (Label = +1):\n",
    "- $ x_1 = (1, 1) $\n",
    "- $ x_2 = (2, 2) $\n",
    "\n",
    "### Class B (Label = -1):\n",
    "- $ x_3 = (1, 3) $\n",
    "- $ x_4 = (2, 4) $\n",
    "\n",
    "We want to find the **Lagrange multipliers** $ \\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4 $.\n",
    "\n",
    "Letโs assume:\n",
    "- $ \\alpha_1 = \\alpha_2 = \\alpha $, $ \\alpha_3 = \\alpha_4 = \\beta $\n",
    "\n",
    "From the constraint:\n",
    "$$\n",
    "\\sum \\alpha_i y_i = 0 \\Rightarrow \\alpha(1) + \\alpha(1) + \\beta(-1) + \\beta(-1) = 0\n",
    "\\Rightarrow 2\\alpha - 2\\beta = 0 \\Rightarrow \\alpha = \\beta\n",
    "$$\n",
    "\n",
    "Now plug into the dual objective function and solve for $ \\alpha $.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งพ Summary of Key Equations\n",
    "\n",
    "| Concept | Equation |\n",
    "|--------|----------|\n",
    "| Primal Objective | $ \\min \\frac{1}{2} \\|w\\|^2 $ |\n",
    "| Primal Constraints | $ y_i (w \\cdot x_i + b) \\geq 1 $ |\n",
    "| Lagrangian | $ \\mathcal{L} = \\frac{1}{2} \\|w\\|^2 - \\sum \\alpha_i [y_i (w \\cdot x_i + b) - 1] $ |\n",
    "| Dual Objective | $ \\max \\sum \\alpha_i - \\frac{1}{2} \\sum \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j $ |\n",
    "| Kernel Trick | $ x_i \\cdot x_j \\rightarrow K(x_i, x_j) $ |\n",
    "| Decision Function | $ f(x) = \\text{sign} \\left( \\sum \\alpha_i y_i x_i \\cdot x + b \\right) $ |\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง Final Notes\n",
    "\n",
    "- The **dual form** is used in practice because it allows for **kernelization**.\n",
    "- The **Lagrange multipliers** $ \\alpha_i $ are **non-zero only for support vectors**.\n",
    "- The **optimization is convex**, so there is a **unique global solution**.\n",
    "\n",
    "---\n",
    "\n",
    "If you want to see how this works in code (e.g., using **Sequential Minimal Optimization** or **CVXOPT**), or how to **solve it with a small dataset**, just let me know! I'm here to help you learn step by step. ๐โจ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0dd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4ca104",
   "metadata": {},
   "source": [
    "Absolutely! ๐  \n",
    "Letโs take it **step by step** and **explain the dual form** of the SVM in **simple and thorough terms**, especially focusing on **step 2**, which is the **Lagrangian function** and how we derive the **dual form**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ Step 2: Understanding the Dual Form of SVM\n",
    "\n",
    "In the **primal form**, we want to **minimize** the norm of the weight vector $ w $, subject to the constraints that the data is correctly classified. But this is a **constrained optimization problem**, and to solve it, we use **Lagrange multipliers**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 1. **Primal Problem Recap**\n",
    "\n",
    "We want to **minimize**:\n",
    "$$\n",
    "\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i\n",
    "$$\n",
    "\n",
    "This is a **constrained optimization problem**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 2. **Introducing the Lagrangian Function**\n",
    "\n",
    "To handle the constraints, we use **Lagrange multipliers** $ \\alpha_i \\geq 0 $.\n",
    "\n",
    "We define the **Lagrangian** as:\n",
    "$$\n",
    "\\mathcal{L}(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "This function combines the **objective function** (what we want to minimize) and the **constraints**.\n",
    "\n",
    "- The first term is the **primal objective**.\n",
    "- The second term is the **sum of constraints**, multiplied by the **Lagrange multipliers** $ \\alpha_i $.\n",
    "\n",
    "We subtract the constraints because we are **minimizing** the primal objective.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 3. **Optimizing the Lagrangian**\n",
    "\n",
    "Now, we want to **minimize** $ \\mathcal{L} $ with respect to $ w $ and $ b $, and **maximize** it with respect to $ \\alpha_i $.\n",
    "\n",
    "This is called the **primal-dual optimization**.\n",
    "\n",
    "### Step 1: Take derivative with respect to $ w $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0\n",
    "\\Rightarrow w = \\sum_{i=1}^{n} \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "This tells us that the **weight vector $ w $** is a **linear combination** of the data points, **weighted by their Lagrange multipliers and labels**.\n",
    "\n",
    "### Step 2: Take derivative with respect to $ b $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "\\Rightarrow \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "This is a **constraint** that the **Lagrange multipliers must satisfy**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 4. **Plug Back into the Lagrangian**\n",
    "\n",
    "Now that we have expressions for $ w $ and $ b $, we can **plug them back** into the Lagrangian to eliminate $ w $ and $ b $.\n",
    "\n",
    "We get the **dual form** of the problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง What is the Dual Form?\n",
    "\n",
    "The **dual form** is the **reformulated version** of the optimization problem, where we no longer optimize over $ w $ and $ b $, but instead over the **Lagrange multipliers $ \\alpha_i $**.\n",
    "\n",
    "### Why is this useful?\n",
    "\n",
    "- It allows us to **use kernel functions** (more on that later).\n",
    "- It reduces the problem to a **quadratic optimization** over $ \\alpha_i $, which is **easier to solve**.\n",
    "- It gives us a way to **focus only on the support vectors** (those with $ \\alpha_i > 0 $).\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 5. **Example: Small Dataset**\n",
    "\n",
    "Letโs take a **small dataset** with 2 points in each class:\n",
    "\n",
    "### Class A (Label = +1):\n",
    "- $ x_1 = (1, 1) $\n",
    "- $ x_2 = (2, 2) $\n",
    "\n",
    "### Class B (Label = -1):\n",
    "- $ x_3 = (1, 3) $\n",
    "- $ x_4 = (2, 4) $\n",
    "\n",
    "We want to find the **Lagrange multipliers** $ \\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4 $.\n",
    "\n",
    "Letโs assume:\n",
    "- $ \\alpha_1 = \\alpha_2 = \\alpha $, $ \\alpha_3 = \\alpha_4 = \\beta $\n",
    "\n",
    "From the constraint:\n",
    "$$\n",
    "\\sum \\alpha_i y_i = 0 \\Rightarrow \\alpha(1) + \\alpha(1) + \\beta(-1) + \\beta(-1) = 0\n",
    "\\Rightarrow 2\\alpha - 2\\beta = 0 \\Rightarrow \\alpha = \\beta\n",
    "$$\n",
    "\n",
    "Now plug into the dual objective function and solve for $ \\alpha $.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งพ Summary of Key Equations\n",
    "\n",
    "| Concept | Equation |\n",
    "|--------|----------|\n",
    "| Primal Objective | $ \\min \\frac{1}{2} \\|w\\|^2 $ |\n",
    "| Primal Constraints | $ y_i (w \\cdot x_i + b) \\geq 1 $ |\n",
    "| Lagrangian | $ \\mathcal{L} = \\frac{1}{2} \\|w\\|^2 - \\sum \\alpha_i [y_i (w \\cdot x_i + b) - 1] $ |\n",
    "| Dual Objective | $ \\max \\sum \\alpha_i - \\frac{1}{2} \\sum \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j $ |\n",
    "| Constraint | $ \\sum \\alpha_i y_i = 0 $, $ \\alpha_i \\geq 0 $ |\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง Final Notes\n",
    "\n",
    "- The **dual form** is used in practice because it allows for **kernelization**.\n",
    "- The **Lagrange multipliers** $ \\alpha_i $ are **non-zero only for support vectors**.\n",
    "- The **optimization is convex**, so there is a **unique global solution**.\n",
    "\n",
    "---\n",
    "\n",
    "If you want to see how this works in code (e.g., using **Sequential Minimal Optimization** or **CVXOPT**), or how to **solve it with a small dataset**, just let me know! I'm here to help you learn step by step. ๐โจ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6817bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85d5cf39",
   "metadata": {},
   "source": [
    "Absolutely, I'm here to help you **understand the transformation from the primal to the dual form of SVM** **step by step**, with **clear and explicit calculations**. I'll use a **simple example** to make it easy to follow.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 1. **Primal Problem Recap**\n",
    "\n",
    "We want to **minimize** the norm of the weight vector $ w $, subject to the constraint that the data is correctly classified.\n",
    "\n",
    "Letโs define:\n",
    "- $ x_i \\in \\mathbb{R}^n $: input data points\n",
    "- $ y_i \\in \\{-1, +1\\} $: class labels\n",
    "- $ w \\in \\mathbb{R}^n $: weight vector\n",
    "- $ b \\in \\mathbb{R} $: bias term\n",
    "\n",
    "The **primal optimization problem** is:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i\n",
    "$$\n",
    "\n",
    "This is a **constrained optimization problem**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 2. **Introducing the Lagrangian Function**\n",
    "\n",
    "To handle the constraints, we use **Lagrange multipliers** $ \\alpha_i \\geq 0 $.\n",
    "\n",
    "We define the **Lagrangian** as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "Letโs expand this:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} w^T w - \\sum_{i=1}^{n} \\alpha_i y_i (w^T x_i + b) + \\sum_{i=1}^{n} \\alpha_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 3. **Optimizing the Lagrangian**\n",
    "\n",
    "We now want to **minimize** $ \\mathcal{L} $ with respect to $ w $ and $ b $, and **maximize** it with respect to $ \\alpha_i $.\n",
    "\n",
    "### Step 1: Take derivative with respect to $ w $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0\n",
    "\\Rightarrow w = \\sum_{i=1}^{n} \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "This tells us that the **weight vector $ w $** is a **linear combination** of the data points, **weighted by their Lagrange multipliers and labels**.\n",
    "\n",
    "### Step 2: Take derivative with respect to $ b $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "\\Rightarrow \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "This is a **constraint** that the **Lagrange multipliers must satisfy**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 4. **Plug Back into the Lagrangian**\n",
    "\n",
    "Now that we have expressions for $ w $ and $ b $, we can **plug them back** into the Lagrangian to eliminate $ w $ and $ b $.\n",
    "\n",
    "Letโs compute the **first term**:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|w\\|^2 = \\frac{1}{2} \\left( \\sum_{i=1}^{n} \\alpha_i y_i x_i \\right)^T \\left( \\sum_{j=1}^{n} \\alpha_j y_j x_j \\right)\n",
    "$$\n",
    "\n",
    "This is a **double sum**:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
    "$$\n",
    "\n",
    "Now the **second term**:\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^{n} \\alpha_i y_i (w^T x_i + b) = - \\sum_{i=1}^{n} \\alpha_i y_i \\left( \\sum_{j=1}^{n} \\alpha_j y_j x_j^T x_i + b \\right)\n",
    "$$\n",
    "\n",
    "This is:\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^{n} \\alpha_i y_i b\n",
    "$$\n",
    "\n",
    "Now the **third term**:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i\n",
    "$$\n",
    "\n",
    "Now combine all three terms:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^{n} \\alpha_i y_i b + \\sum_{i=1}^{n} \\alpha_i\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^{n} \\alpha_i y_i b + \\sum_{i=1}^{n} \\alpha_i\n",
    "$$\n",
    "\n",
    "Now use the constraint $ \\sum_{i=1}^{n} \\alpha_i y_i = 0 $, so the second term is zero.\n",
    "\n",
    "So we get:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 5. **Dual Form of the Problem**\n",
    "\n",
    "Now we have the **dual form** of the SVM optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 6. **Example: Small Dataset**\n",
    "\n",
    "Letโs take a **small dataset** with 2 points in each class:\n",
    "\n",
    "### Class A (Label = +1):\n",
    "- $ x_1 = (1, 1) $\n",
    "- $ x_2 = (2, 2) $\n",
    "\n",
    "### Class B (Label = -1):\n",
    "- $ x_3 = (1, 3) $\n",
    "- $ x_4 = (2, 4) $\n",
    "\n",
    "Letโs assume:\n",
    "- $ \\alpha_1 = \\alpha_2 = \\alpha $\n",
    "- $ \\alpha_3 = \\alpha_4 = \\beta $\n",
    "\n",
    "From the constraint:\n",
    "$$\n",
    "\\sum \\alpha_i y_i = 0 \\Rightarrow \\alpha(1) + \\alpha(1) + \\beta(-1) + \\beta(-1) = 0\n",
    "\\Rightarrow 2\\alpha - 2\\beta = 0 \\Rightarrow \\alpha = \\beta\n",
    "$$\n",
    "\n",
    "Now plug into the dual objective function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{4} \\sum_{j=1}^{4} \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
    "$$\n",
    "\n",
    "Letโs compute the **inner products**:\n",
    "\n",
    "- $ x_1 \\cdot x_1 = 1^2 + 1^2 = 2 $\n",
    "- $ x_1 \\cdot x_2 = 1*2 + 1*2 = 4 $\n",
    "- $ x_1 \\cdot x_3 = 1*1 + 1*3 = 4 $\n",
    "- $ x_1 \\cdot x_4 = 1*2 + 1*4 = 6 $\n",
    "- $ x_2 \\cdot x_2 = 2^2 + 2^2 = 8 $\n",
    "- $ x_2 \\cdot x_3 = 2*1 + 2*3 = 8 $\n",
    "- $ x_2 \\cdot x_4 = 2*2 + 2*4 = 12 $\n",
    "- $ x_3 \\cdot x_3 = 1^2 + 3^2 = 10 $\n",
    "- $ x_3 \\cdot x_4 = 1*2 + 3*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5e175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9db4219",
   "metadata": {},
   "source": [
    "Great question! ๐  \n",
    "You're absolutely right โ **understanding the *concepts* behind SVM is often more important than getting lost in the complex math**, especially if you're not planning to implement it from scratch or dive into optimization theory.\n",
    "\n",
    "Letโs break down the **key conceptual points** you should understand about **SVM**, especially from a **high-level and intuitive perspective**, without getting bogged down in the solving method.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 1. **What is an SVM Trying to Do?**\n",
    "\n",
    "At its core, **SVM (Support Vector Machine)** is a **supervised learning algorithm** used for **classification** (and regression too, but weโll focus on classification here).\n",
    "\n",
    "### ๐ฏ Goal:\n",
    "> **Find the best possible hyperplane** that separates the data into classes **with the maximum margin**.\n",
    "\n",
    "- A **hyperplane** is just a line (in 2D), a plane (in 3D), or a higher-dimensional equivalent.\n",
    "- The **margin** is the distance between the hyperplane and the **closest data points** from each class.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 2. **Support Vectors**\n",
    "\n",
    "- **Support vectors** are the **data points closest to the decision boundary** (the hyperplane).\n",
    "- These are the **only points that influence the position and orientation of the hyperplane**.\n",
    "- All other points **do not affect the model** once the hyperplane is determined.\n",
    "\n",
    "> ๐ก **Key Concept**:  \n",
    "> The **Lagrange multipliers (ฮฑ)** for **non-support vectors are zero**.  \n",
    "> Only the **support vectors have ฮฑ > 0**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 3. **Hard Margin vs Soft Margin**\n",
    "\n",
    "### โ **Hard Margin SVM**:\n",
    "- Assumes the data is **linearly separable**.\n",
    "- No data points are allowed to be on the wrong side of the margin.\n",
    "- **Not robust to noise or outliers**.\n",
    "\n",
    "### ๐ค **Soft Margin SVM**:\n",
    "- Allows **some misclassifications** to make the model more **robust**.\n",
    "- Introduces a **slack variable** (ฮพ) to allow data points to be on the wrong side of the margin.\n",
    "- More practical for **real-world data**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 4. **Kernel Trick (Non-Linear SVM)**\n",
    "\n",
    "- When the data is **not linearly separable**, we use the **kernel trick**.\n",
    "- The **kernel function** maps the input data into a **higher-dimensional space**, where it becomes **linearly separable**.\n",
    "- Common kernels:\n",
    "  - **Linear**\n",
    "  - **Polynomial**\n",
    "  - **Radial Basis Function (RBF)** โ most commonly used\n",
    "  - **Sigmoid**\n",
    "\n",
    "> ๐ก **Key Concept**:  \n",
    "> The **dual form** of SVM allows us to use **kernels** by replacing the dot product $ x_i \\cdot x_j $ with $ K(x_i, x_j) $.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 5. **Why Use the Dual Form?**\n",
    "\n",
    "- The **dual form** is **mathematically equivalent** to the primal form.\n",
    "- It has **several advantages**:\n",
    "  - It allows the use of **kernel functions**.\n",
    "  - It only involves **support vectors**, which reduces the problem size.\n",
    "  - Itโs **easier to solve numerically** in practice.\n",
    "\n",
    "> ๐ก **Key Concept**:  \n",
    "> In the **dual form**, we optimize over **Lagrange multipliers (ฮฑ)**, not the **weights (w)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 6. **The Role of Lagrange Multipliers (ฮฑ)**\n",
    "\n",
    "- The **Lagrange multipliers (ฮฑ)** are **non-zero only for support vectors**.\n",
    "- They tell us **how much each support vector contributes** to the decision boundary.\n",
    "- The **larger the ฮฑ**, the more **influential** the point is.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 7. **Bias Term (b)**\n",
    "\n",
    "- The **bias term (b)** shifts the hyperplane up or down.\n",
    "- It is **not part of the weight vector (w)**.\n",
    "- It is **determined after solving for the ฮฑs** using the **KKT conditions**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 8. **KKT Conditions (KarushโKuhnโTucker)**\n",
    "\n",
    "These are the **necessary conditions** for a solution to be optimal in a constrained optimization problem.\n",
    "\n",
    "In SVM, the **KKT conditions** help us:\n",
    "- Identify **support vectors** (ฮฑ > 0).\n",
    "- Determine the **bias term (b)**.\n",
    "- Ensure the **solution is optimal**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 9. **How to Use SVM in Practice**\n",
    "\n",
    "Even if you donโt understand the math in depth, you can use SVM effectively by:\n",
    "\n",
    "- Choosing the right **kernel**.\n",
    "- Tuning **hyperparameters** like:\n",
    "  - **C (regularization parameter)** โ controls the trade-off between maximizing the margin and minimizing classification error.\n",
    "  - **ฮณ (gamma)** โ for RBF kernel, controls the **influence of a single training example**.\n",
    "- Using **cross-validation** to find the best parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 10. **When to Use SVM**\n",
    "\n",
    "- When the **number of features is small to moderate**.\n",
    "- When the **data is not too noisy**.\n",
    "- When you want a **robust model** with **good generalization**.\n",
    "- When you need **interpretability** (e.g., in medical or financial applications).\n",
    "\n",
    "---\n",
    "\n",
    "## โ Summary of Key Concepts\n",
    "\n",
    "| Concept | Explanation |\n",
    "|--------|-------------|\n",
    "| **Hyperplane** | The decision boundary that separates classes. |\n",
    "| **Support Vectors** | The data points closest to the hyperplane. Only they affect the model. |\n",
    "| **Lagrange Multipliers (ฮฑ)** | Non-zero only for support vectors. |\n",
    "| **Kernel Trick** | Maps data to a higher-dimensional space to make it linearly separable. |\n",
    "| **Dual Form** | Allows kernel use and optimization over ฮฑ. |\n",
    "| **Hard vs Soft Margin** | Hard margin = no errors allowed; Soft margin = allows some errors. |\n",
    "| **Bias Term (b)** | Shifts the hyperplane. |\n",
    "| **KKT Conditions** | Necessary conditions for optimality in constrained optimization. |\n",
    "| **Hyperparameters (C, ฮณ)** | Control model complexity and generalization. |\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฏ Final Thought\n",
    "\n",
    "You donโt need to **solve the SVM optimization problem manually** to use it effectively. What matters is **understanding the intuition and the key concepts** like:\n",
    "\n",
    "- **Support vectors**\n",
    "- **Kernels**\n",
    "- **Dual form**\n",
    "- **Lagrange multipliers**\n",
    "- **Bias and margin**\n",
    "\n",
    "Once you have that, you can **use SVM confidently in practice**, and understand whatโs going on under the hood. ๐\n",
    "\n",
    "If you want, I can also give you a **simple Python example using scikit-learn** to see how SVM works in practice. Just say the word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beae5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7b454e",
   "metadata": {},
   "source": [
    "Absolutely! I'm glad you're diving into **kernels** โ they are one of the most **powerful and elegant ideas** in SVMs. Let's go **beyond the dual form** and **deep into the world of kernels**, step by step, in simple and clear English.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 1. **What is a Kernel in SVM?**\n",
    "\n",
    "A **kernel** is a **function** that takes **two input vectors** and returns a **scalar** (a single number), representing their **similarity** in some (possibly high-dimensional) space.\n",
    "\n",
    "In SVM, the kernel is used to **transform the input data into a space where it is easier to separate** โ even if it wasn't linearly separable in the original space.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 2. **Why Use Kernels?**\n",
    "\n",
    "In the **primal form**, the decision boundary is a **hyperplane** in the original input space.\n",
    "\n",
    "But in many real-world problems, the data is **not linearly separable** โ you **canโt draw a straight line** to separate the classes.\n",
    "\n",
    "Thatโs where **kernels** come in.\n",
    "\n",
    "They allow us to **implicitly map the data into a higher-dimensional space**, where it **becomes linearly separable**, and then find a **hyperplane in that space**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 3. **The Kernel Trick**\n",
    "\n",
    "The **kernel trick** is the **magic** that makes this possible **without explicitly computing the high-dimensional mapping**.\n",
    "\n",
    "Hereโs how it works:\n",
    "\n",
    "- Instead of computing the **mapping function ฮฆ(x)** (which could be very expensive), we use a **kernel function K(x, y)** that computes the **inner product in the high-dimensional space**:\n",
    "  $$\n",
    "  K(x, y) = \\Phi(x) \\cdot \\Phi(y)\n",
    "  $$\n",
    "\n",
    "This is **computationally efficient**, and it allows us to **use SVMs in high-dimensional (even infinite-dimensional) spaces**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ 4. **How Kernels Work in the Dual Form**\n",
    "\n",
    "In the **dual form** of SVM, the **decision function** is:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign} \\left( \\sum_{i=1}^{n} \\alpha_i y_i K(x_i, x) + b \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha_i $ are the **Lagrange multipliers** (non-zero only for support vectors).\n",
    "- $ y_i $ are the **labels**.\n",
    "- $ K(x_i, x) $ is the **kernel function**.\n",
    "- $ b $ is the **bias term**.\n",
    "\n",
    "So instead of using the **original features**, we use the **kernel function** to compute similarity between the input and each support vector.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 5. **Common Kernel Functions**\n",
    "\n",
    "Letโs look at the **most commonly used kernels** in SVM:\n",
    "\n",
    "---\n",
    "\n",
    "### ๐น 1. **Linear Kernel**\n",
    "\n",
    "$$\n",
    "K(x, y) = x \\cdot y\n",
    "$$\n",
    "\n",
    "- This is the **simplest kernel**.\n",
    "- Itโs equivalent to the **primal form** of SVM.\n",
    "- Best for **linearly separable data**.\n",
    "\n",
    "---\n",
    "\n",
    "### ๐น 2. **Polynomial Kernel**\n",
    "\n",
    "$$\n",
    "K(x, y) = (x \\cdot y + c)^d\n",
    "$$\n",
    "\n",
    "- $ c $ is a constant (often 1).\n",
    "- $ d $ is the **degree** of the polynomial.\n",
    "- Can model **non-linear relationships**.\n",
    "- Example: $ (x \\cdot y + 1)^2 $\n",
    "\n",
    "---\n",
    "\n",
    "### ๐น 3. **Radial Basis Function (RBF) Kernel** โ Most Popular\n",
    "\n",
    "$$\n",
    "K(x, y) = \\exp \\left( -\\gamma \\|x - y\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "- $ \\gamma $ is a **hyperparameter** (controls the \"spread\").\n",
    "- This kernel maps the data into an **infinite-dimensional space**.\n",
    "- Very powerful for **non-linear classification**.\n",
    "- Also known as the **Gaussian kernel**.\n",
    "\n",
    "---\n",
    "\n",
    "### ๐น 4. **Sigmoid Kernel**\n",
    "\n",
    "$$\n",
    "K(x, y) = \\tanh(\\kappa x \\cdot y + c)\n",
    "$$\n",
    "\n",
    "- Inspired by **neural networks**.\n",
    "- Less commonly used in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 6. **How Kernels Make SVM Powerful**\n",
    "\n",
    "Letโs take a **simple example** to see how kernels help.\n",
    "\n",
    "---\n",
    "\n",
    "### ๐ Example: XOR Problem\n",
    "\n",
    "Imagine you have 2D data that looks like this:\n",
    "\n",
    "| x1 | x2 | y  |\n",
    "|----|----|----|\n",
    "| 0  | 0  | -1 |\n",
    "| 0  | 1  | +1 |\n",
    "| 1  | 0  | +1 |\n",
    "| 1  | 1  | -1 |\n",
    "\n",
    "This is the **XOR problem**, and itโs **not linearly separable** in 2D.\n",
    "\n",
    "But if we **map the data into 3D** using a **polynomial kernel** of degree 2:\n",
    "\n",
    "$$\n",
    "\\Phi(x_1, x_2) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)\n",
    "$$\n",
    "\n",
    "Now the data becomes **linearly separable** in 3D.\n",
    "\n",
    "And we can use a **linear SVM** in this new space.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 7. **Choosing the Right Kernel**\n",
    "\n",
    "Thereโs no one-size-fits-all kernel. Here are some **guidelines**:\n",
    "\n",
    "| Kernel | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Linear** | Linearly separable data | Fast, simple | Not good for complex patterns |\n",
    "| **Polynomial** | Medium complexity | Can model curves | Computationally heavy |\n",
    "| **RBF** | Most common | Very flexible | Can overfit if ฮณ is too high |\n",
    "| **Sigmoid** | Rare | Inspired by neural nets | Not always effective |\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 8. **Hyperparameters in Kernels**\n",
    "\n",
    "Each kernel has **hyperparameters** you can tune:\n",
    "\n",
    "- **RBF kernel**: $ \\gamma $ (controls how far the influence of a single training example reaches)\n",
    "- **Polynomial kernel**: $ d $ (degree), $ c $ (constant)\n",
    "- **C** (regularization parameter): controls the **trade-off between maximizing margin and minimizing error**\n",
    "\n",
    "These are usually tuned using **cross-validation**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 9. **Kernel vs. Feature Engineering**\n",
    "\n",
    "- **Feature engineering** is about **manually creating new features**.\n",
    "- **Kernels** are a **mathematical trick** to **automatically create new features** in a high-dimensional space.\n",
    "- Kernels can be seen as a **form of automatic feature engineering**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง 10. **When to Use Kernels**\n",
    "\n",
    "- When the data is **not linearly separable**.\n",
    "- When you want to **avoid manually designing complex features**.\n",
    "- When you have **moderate-sized datasets** (kernels can be slow for very large data).\n",
    "- When you want a **robust and interpretable model**.\n",
    "\n",
    "---\n",
    "\n",
    "## โ Summary of Kernel Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Kernel** | A function that computes similarity in a high-dimensional space. |\n",
    "| **Kernel Trick** | Allows us to compute in high-dimensional space without explicitly mapping the data. |\n",
    "| **Dual Form** | Makes kernel use possible by working with Lagrange multipliers. |\n",
    "| **Common Kernels** | Linear, Polynomial, RBF, Sigmoid. |\n",
    "| **RBF Kernel** | Most powerful and widely used. |\n",
    "| **Hyperparameters** | ฮณ (for RBF), C (regularization), d (degree for polynomial). |\n",
    "| **Use Case** | When data is not linearly separable. |\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฏ Final Thought\n",
    "\n",
    "Kernels are one of the **most elegant and powerful ideas in machine learning**. They allow SVMs to **adapt to complex data patterns** without getting bogged down in high-dimensional math.\n",
    "\n",
    "You donโt need to **compute the kernel functions manually** โ libraries like **scikit-learn** handle it for you. But understanding **what they do and when to use them** is key to using SVMs effectively.\n",
    "\n",
    "If you want, I can show you a **simple Python example using scikit-learn** with different kernels. Just say the word! ๐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfe1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78933881",
   "metadata": {},
   "source": [
    "# Covariance Matrix\n",
    "\n",
    "ุนุงู! ๐  \n",
    "ฺฉ **ูุงุชุฑุณ ฺฉููุงุฑุงูุณ ูุนุชุจุฑ** ฺูุฏ ูฺฺฏ ููู ุฏุงุฑู ฺฉู ุจุงุฏ ุฏุงุดุชู ุจุงุดู ุชุง ุฏุฑ ูุญุงุณุจุงุช ุขูุงุฑ ุง ุงุฏฺฏุฑ ูุงุดูุ ุจู ุฏุฑุณุช ฺฉุงุฑ ฺฉูู. ูู ุงูุงู ุจูุช ฺูุฏุชุง ุงุฒ ูููโุชุฑูุดูู ุฑู ุจู ุฒุจุงู ุณุงุฏู ู ุจุง ูุซุงู ูโฺฏู.\n",
    "\n",
    "---\n",
    "\n",
    "### โ 1. **ูุงุชุฑุณ ุจุงุฏ ูุชูุงุฑู ุจุงุดู**  \n",
    "- ุนู ุงฺฏุฑ ุฏุฑุงู $ S_{ij} $ ุฑู ุจุจูุ ุจุงุฏ $ S_{ij} = S_{ji} $ ุจุงุดู.  \n",
    "- **ูุซุงู**: ุงฺฏุฑ ฺฉููุงุฑุงูุณ ุจู ุงุฑุชูุงุน ู ุชุนุฏุงุฏ ุจุฑฺฏ 0.5 ุจุงุดูุ ฺฉููุงุฑุงูุณ ุจู ุชุนุฏุงุฏ ุจุฑฺฏ ู ุงุฑุชูุงุน ูู ุจุงุฏ 0.5 ุจุงุดู.\n",
    "\n",
    "---\n",
    "\n",
    "### โ 2. **ูุงุชุฑุณ ุจุงุฏ ููโูุซุจุช (Positive Semi-Definite) ุจุงุดู**  \n",
    "- ุนู ุชูุงู ููุงุฏุฑ ูฺู (Eigenvalues) ุงูู ุจุงุฏ **ุจุฒุฑฺฏุชุฑ ุง ูุณุงู ุตูุฑ** ุจุงุดู.  \n",
    "- ุงู ูฺฺฏ ูููู ฺูู ูุทูุฆู ูโฺฉูู ฺฉู ูุงุชุฑุณ ูุนฺฉูุณโูพุฐุฑู ุง ูู ู ูุงุตููโูุง ูุญุงุณุจู ุดุฏู ููุทู ุจุงุดู.\n",
    "\n",
    "---\n",
    "\n",
    "### โ 3. ** ูุทุฑ ุงุตู ุจุงุฏ ูุงุฑุงูุณ ุจุงุดู**  \n",
    "- ุฏุฑุงูโูุง ุฑู ูุทุฑ ุงุตู (Sโโ, Sโโ, ...) ุจุงุฏ **ูุงุฑุงูุณ** ูุฑ ูุชุบุฑ ุจุงุดู.  \n",
    "- **ูุซุงู**: ุงฺฏุฑ ูุชุบุฑ ุงูู ุงุฑุชูุงุน ุจุงุดูุ $ S_{11} $ ุจุงุฏ ูุงุฑุงูุณ ุงุฑุชูุงุน ุจุงุดู.\n",
    "\n",
    "---\n",
    "\n",
    "### โ 4. **ููุงุฏุฑ ุฑู ูุทุฑ ุจุงุฏ ูุซุจุช ุง ุตูุฑ ุจุงุดู**  \n",
    "- ฺูู ูุงุฑุงูุณ ููโุชููู ููู ุจุงุดู.  \n",
    "- ุงฺฏุฑ ฺฉ ุงุฒ ูุงุฑุงูุณโูุง ุตูุฑ ุจุงุดูุ ุนู ููู ููุงุฏุฑ ฺฉ ูุชุบุฑ ฺฉุณุงู ูุณุชู.\n",
    "\n",
    "---\n",
    "\n",
    "### โ 5. **ูุงุชุฑุณ ุจุงุฏ ูุฑุจุน ุจุงุดู**  \n",
    "- ุชุนุฏุงุฏ ุณุทุฑูุง ู ุณุชููโูุง ุจุงุฏ ฺฉุณุงู ุจุงุดู.  \n",
    "- ุงฺฏุฑ 3 ูุชุบุฑ ุฏุงุฑุ ูุงุชุฑุณ ฺฉููุงุฑุงูุณ ุจุงุฏ 3ร3 ุจุงุดู.\n",
    "\n",
    "---\n",
    "\n",
    "### โ 6. **ฺฉููุงุฑุงูุณ ุจู ฺฉ ูุชุบุฑ ุจุง ุฎูุฏุด ุจุฑุงุจุฑ ุจุง ูุงุฑุงูุณุด ุจุงุดู**  \n",
    "- ุนู $ S_{ii} = \\text{Var}(X_i) $\n",
    "\n",
    "---\n",
    "\n",
    "### ๐ ูุซุงู ุณุงุฏู:\n",
    "\n",
    "ูุฑุถ ฺฉู ฺฉ ูุงุชุฑุณ ฺฉููุงุฑุงูุณ ุฏุงุฑู:\n",
    "\n",
    "$$\n",
    "S = \\begin{bmatrix}\n",
    "4 & 0.5 \\\\\n",
    "0.5 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- ูุชูุงุฑู ูุณุช โ  \n",
    "- ููุงุฏุฑ ุฑู ูุทุฑ (4 ู 1) ูุงุฑุงูุณ ูุณุชู โ  \n",
    "- ฺฉููุงุฑุงูุณ ุจู ุฏู ูุชุบุฑ 0.5 ูุณุช โ  \n",
    "- ูุงุชุฑุณ ูุฑุจุน ูุณุช โ  \n",
    "- ูโุชููู ุงุฒ ุงูู ุจุฑุง ูุญุงุณุจู ูุงุตูู ูุงูุงูุงููุจุณ ุงุณุชูุงุฏู ฺฉูู โ\n",
    "\n",
    "---\n",
    "\n",
    "ุงฺฏุฑ ูโุฎูุง ู ูุงุชุฑุณ ุฑู ุจุฑุฑุณ ฺฉูู ุง ู ูุซุงู ุฏฺฏู ุฑู ูู ุจุฒููุ ููุท ุจฺฏู! ๐\n",
    "ุฎุจุ ุฎูุจ! ูู ุงูุงู ุจูุช **ุฏุฑ ููุฑุฏ ูุซุจุช ููู ูุนู ุจูุฏู (Positive Semi-Definite)** ฺฉ ูุงุชุฑุณ ุจู ุฒุจุงู ุณุงุฏู ู ุจุง ูุซุงู ุชูุถุญ ูโุฏู. ๐\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ ูุซุจุช ููู ูุนู (Positive Semi-Definite) ฺูุ\n",
    "\n",
    "ููุช ูโฺฏู ฺฉ ูุงุชุฑุณ **ูุซุจุช ููู ูุนู** ูุณุชุ ุนู:\n",
    "\n",
    "> ุจุฑุง ูุฑ ุจุฑุฏุงุฑ $ x $ (ฺฉู ุงุจุนุงุฏุด ุจุง ูุงุชุฑุณ ฺฉ ุจุงุดู)ุ ุญุงุตู $ x^T S x $ **ุจุฒุฑฺฏุชุฑ ุง ูุณุงู ุตูุฑ** ุจุงุดู.\n",
    "\n",
    "$$\n",
    "x^T S x \\geq 0\n",
    "$$\n",
    "\n",
    "ุงู ุนู ููุช ู ูุงุชุฑุณ ุฑู ุฏุฑ ู ุจุฑุฏุงุฑ ุถุฑุจ ฺฉูู ู ุจุนุฏ ุถุฑุจ ุฏุงุฎูุด ุฑู ุญุณุงุจ ฺฉููุ ุญุงุตู ููโุชููู ููู ุจุงุดู.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง ฺุฑุง ููููุ\n",
    "\n",
    "ุฏุฑ ุงุฏฺฏุฑ ูุงุดูุ ุขูุงุฑ ู ุจูููโุณุงุฒุ ูุงุชุฑุณโูุง ฺฉููุงุฑุงูุณ ุจุงุฏ **ูุซุจุช ููู ูุนู** ุจุงุดู. ฺูู:\n",
    "\n",
    "- ุงฺฏุฑ ูุงุชุฑุณ ฺฉููุงุฑุงูุณ ููู ุจุงุดูุ ููฺฉูู ูุงุตููโูุง ุง ุงูุฑฺโูุง ููู ุจู ูุธุฑ ุจุฑุณู ฺฉู ููุทู ูุณุช.\n",
    "- ูุทูุฆู ูโฺฉูู ฺฉู ูุงุชุฑุณ ูุนฺฉูุณโูพุฐุฑู (ุง ุญุฏุงูู ูุนฺฉูุณุด ุฑู ูโุชููู ุจุง ุชุบุฑุงุช ุจู ุฏุณุช ุจุงุฑู).\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ ฺุทูุฑ ูโุชููู ุจูููู ฺฉ ูุงุชุฑุณ ูุซุจุช ููู ูุนููุ\n",
    "\n",
    "### 1. **ููุงุฏุฑ ูฺู (Eigenvalues)**\n",
    "- ุงฺฏุฑ ุชูุงู ููุงุฏุฑ ูฺู ฺฉ ูุงุชุฑุณ **ุจุฒุฑฺฏุชุฑ ุง ูุณุงู ุตูุฑ** ุจุงุดูุ ูุงุชุฑุณ ูุซุจุช ููู ูุนูู.\n",
    "- ุงฺฏุฑ ุชูุงู ููุงุฏุฑ ูฺู **ุจุฒุฑฺฏุชุฑ ุงุฒ ุตูุฑ** ุจุงุดูุ ูุงุชุฑุณ **ูุซุจุช ูุนู (Positive Definite)** ูุณุช.\n",
    "\n",
    "### 2. **ูุงุชุฑุณ ูุชูุงุฑู ุจุงุดู**\n",
    "- ฺฉ ูุงุชุฑุณ ูุซุจุช ููู ูุนู ุจุงุฏ **ูุชูุงุฑู** ุจุงุดู.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฑ ูุซุงู ุณุงุฏู:\n",
    "\n",
    "ูุฑุถ ฺฉู ูุงุชุฑุณ ฺฉููุงุฑุงูุณ ุฒุฑ ุฑู ุฏุงุฑู:\n",
    "\n",
    "$$\n",
    "S = \\begin{bmatrix}\n",
    "4 & 0.5 \\\\\n",
    "0.5 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- ุงู ูุงุชุฑุณ ูุชูุงุฑู ูุณุช โ  \n",
    "- ููุงุฏุฑ ูฺู ุงูู ุฑู ุญุณุงุจ ูโฺฉูู:\n",
    "\n",
    "$$\n",
    "\\text{Eigenvalues} \\approx 4.22 \\text{ ู } 0.78\n",
    "$$\n",
    "\n",
    "- ูุฑ ุฏู **ุจุฒุฑฺฏุชุฑ ุงุฒ ุตูุฑ** ูุณุชู โ  \n",
    "- ูพุณ ุงู ูุงุชุฑุณ **ูุซุจุช ูุนู** ูุณุช (ู ุงูุจุชู ูุซุจุช ููู ูุนู ูู ูุณุช).\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ซ ูุซุงู ูุงูุนุชุจุฑ:\n",
    "\n",
    "ูุฑุถ ฺฉู ูุงุชุฑุณ ุฒุฑ ุฑู ุฏุงุฑู:\n",
    "\n",
    "$$\n",
    "S = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- ูุชูุงุฑู ูุณุช โ  \n",
    "- ููุงุฏุฑ ูฺู:\n",
    "\n",
    "$$\n",
    "\\text{Eigenvalues} \\approx 3 \\text{ ู } -1\n",
    "$$\n",
    "\n",
    "- ฺฉ ุงุฒ ููุงุฏุฑ **ููู** ูุณุช โ  \n",
    "- ูพุณ ุงู ูุงุชุฑุณ **ูุซุจุช ููู ูุนู ูุณุช**.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฉ ฺุฑุง ูุงุชุฑุณ ฺฉููุงุฑุงูุณ ูุซุจุช ููู ูุนููุ\n",
    "\n",
    "ฺูู:\n",
    "\n",
    "- ูุงุฑุงูุณ (ุฏุฑุงูโูุง ูุทุฑ) ููุดู **ุจุฒุฑฺฏุชุฑ ุง ูุณุงู ุตูุฑ** ูุณุช.\n",
    "- ฺฉููุงุฑุงูุณโูุง (ุฏุฑุงูโูุง ุบุฑ ูุทุฑ) ูโุชููู ููู ุง ูุซุจุช ุจุงุดูุ ุงูุง ูุงุชุฑุณ ฺฉููุงุฑุงูุณ ุจู ุทูุฑ ุทุจุน **ูุชูุงุฑู ู ูุซุจุช ููู ูุนู** ูโูููู.\n",
    "\n",
    "---\n",
    "\n",
    "ุงฺฏุฑ ูโุฎูุง ู ูุซุงู ุฏฺฏู ุฑู ุจุง ุงุนุฏุงุฏ ูุงูุน ุจุฒูู ุง ู ูุงุชุฑุณ ุฑู ุจุฑุฑุณ ฺฉููุ ููุท ุจฺฏู! ๐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729e0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd24ad41",
   "metadata": {},
   "source": [
    "# Eigenvalues\n",
    "\n",
    "ุนุงู! ๐  \n",
    "**ููุงุฏุฑ ูฺู (Eigenvalues)** ฺฉ ุงุฒ ููุงูู ุงุณุงุณ ุฏุฑ ุฌุจุฑ ุฎุท ูุณุชู ู ุฏุฑ ูุจุงุญุซ ูุฎุชูู ูุซู ุงุฏฺฏุฑ ูุงุดูุ ุขูุงุฑุ ูุฒฺฉ ู ูููุทูุฑ ุฏุฑ ูุญุงุณุจู ูุงุตูู ูุงูุงูุงููุจุณ ฺฉุงุฑุจุฑุฏ ุฒุงุฏ ุฏุงุฑู. ูู ุงูุงู ุจูุช ุจู ุฒุจุงู ุณุงุฏู ุชูุถุญ ูโุฏู ฺฉู ฺ ูุณุชู ู ฺุฑุง ูููู.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ ููุฏุงุฑ ูฺู ฺูุ\n",
    "\n",
    "ูุฑุถ ฺฉู ู ูุงุชุฑุณ $ A $ ุฏุงุฑู. ู ุนุฏุฏ $ \\lambda $ ุฑู **ููุฏุงุฑ ูฺู** ูโูุงูู ุงฺฏุฑ ู ุจุฑุฏุงุฑ ุบุฑุตูุฑ $ v $ ูุฌูุฏ ุฏุงุดุชู ุจุงุดู ฺฉู:\n",
    "\n",
    "$$\n",
    "A v = \\lambda v\n",
    "$$\n",
    "\n",
    "ุงู ูุนุงุฏูู ุนู: ููุช ูุงุชุฑุณ $ A $ ุฑู ุฏุฑ ุจุฑุฏุงุฑ $ v $ ุถุฑุจ ฺฉููุ ููุท **ุทูู** ุจุฑุฏุงุฑ $ v $ ุชุบุฑ ูโฺฉููุ **ุฌูุช**ุด ุชุบุฑ ููโฺฉูู.  \n",
    "ุนุฏุฏ $ \\lambda $ ูููู **ููุฏุงุฑ ูฺู** ูุณุช.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ง ฺุฑุง ููููุ\n",
    "\n",
    "### 1. **ุฏุฑ ููุฑุฏ ูุงุชุฑุณ ฺฉููุงุฑุงูุณ:**\n",
    "- ููุฏุงุฑ ูฺูโูุง ูุดูู ูโุฏู ฺฉู ฺูุฏุฑ ุฏุงุฏูโูุง ุฏุฑ ุฌูุชโูุง ูุฎุชูู **ูพุฑุงฺฉูุฏู** ูุณุชู.\n",
    "- ุงฺฏุฑ ู ููุฏุงุฑ ูฺู **ุตูุฑ** ุจุงุดูุ ุนู ุฏุงุฏูโูุง ุฏุฑ ู ุฌูุช **ฺฉุงููุงู ฺฉุณุงู** ูุณุชู (ุจุฏูู ูพุฑุงฺฉูุฏฺฏ).\n",
    "\n",
    "### 2. **ุฏุฑ ููุฑุฏ ูุซุจุช ููู ูุนู ุจูุฏู:**\n",
    "- ุงฺฏุฑ ุชูุงู ููุงุฏุฑ ูฺู **ุจุฒุฑฺฏุชุฑ ุง ูุณุงู ุตูุฑ** ุจุงุดูุ ูุงุชุฑุณ **ูุซุจุช ููู ูุนู** ูุณุช.\n",
    "- ุงฺฏุฑ ุชูุงู ููุงุฏุฑ ูฺู **ุจุฒุฑฺฏุชุฑ ุงุฒ ุตูุฑ** ุจุงุดูุ ูุงุชุฑุณ **ูุซุจุช ูุนู** ูุณุช.\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฎ ฺุทูุฑ ูโุชููู ููุงุฏุฑ ูฺู ุฑู ูพุฏุง ฺฉููุ\n",
    "\n",
    "ุจุฑุง ู ูุงุชุฑุณ $ A $ุ ููุงุฏุฑ ูฺู ุงุฒ ุญู ูุนุงุฏูู ุฒุฑ ุจู ุฏุณุช ูโุขู:\n",
    "\n",
    "$$\n",
    "\\text{det}(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "- $ \\text{det} $: ุฏุชุฑููุงู  \n",
    "- $ I $: ูุงุชุฑุณ ููุงู  \n",
    "- $ \\lambda $: ููุฏุงุฑ ูฺู\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ฑ ูุซุงู ุณุงุฏู:\n",
    "\n",
    "ูุฑุถ ฺฉู ูุงุชุฑุณ ุฒุฑ ุฑู ุฏุงุฑู:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "4 & 0.5 \\\\\n",
    "0.5 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "ุจุฑุง ูพุฏุง ฺฉุฑุฏู ููุงุฏุฑ ูฺูุ ูุนุงุฏูู ุฒุฑ ุฑู ุญู ูโฺฉูู:\n",
    "\n",
    "$$\n",
    "\\text{det}(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{det} \\left( \\begin{bmatrix}\n",
    "4 - \\lambda & 0.5 \\\\\n",
    "0.5 & 1 - \\lambda\n",
    "\\end{bmatrix} \\right) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(4 - \\lambda)(1 - \\lambda) - (0.5)^2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(4 - \\lambda)(1 - \\lambda) - 0.25 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(4 - \\lambda)(1 - \\lambda) = 0.25\n",
    "$$\n",
    "\n",
    "ุงฺฏุฑ ุงู ุฑู ุญู ฺฉููุ ููุงุฏุฑ ูฺู ุฑู ุจู ุฏุณุช ูโุงุฑู.  \n",
    "ุฏุฑ ุงู ูุซุงูุ ููุงุฏุฑ ูฺู ุชูุฑุจุงู:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\approx 4.22, \\quad \\lambda_2 \\approx 0.78\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ๐งฉ ฺู ฺฉุงุฑ ูโุชููู ุจุง ููุงุฏุฑ ูฺู ุจฺฉููุ\n",
    "\n",
    "- **PCA (ุชุญูู ูููููโูุง ุงุตู)**: ุงุฒ ููุงุฏุฑ ูฺู ุจุฑุง ฺฉุงูุด ุงุจุนุงุฏ ุงุณุชูุงุฏู ูโฺฉูู.\n",
    "- **ุจุฑุฑุณ ูุซุจุช ููู ูุนู ุจูุฏู**: ุงฺฏุฑ ููุงุฏุฑ ูฺู ููู ุจุฒุฑฺฏุชุฑ ุง ูุณุงู ุตูุฑ ุจุงุดูุ ูุงุชุฑุณ ูุซุจุช ููู ูุนูู.\n",
    "- **ูุญุงุณุจู ูุนฺฉูุณ ูุงุชุฑุณ**: ุงฺฏุฑ ู ููุฏุงุฑ ูฺู ุตูุฑ ุจุงุดูุ ูุงุชุฑุณ ูุนฺฉูุณโูพุฐุฑ ูุณุช.\n",
    "\n",
    "---\n",
    "\n",
    "ุงฺฏุฑ ูโุฎูุง ู ูุซุงู ุฏฺฏู ุฑู ุจุง ุงุนุฏุงุฏ ูุงูุน ุจุฒูู ุง ู ูุงุชุฑุณ ุฑู ุจุฑุฑุณ ฺฉููุ ููุท ุจฺฏู! ๐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75b8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d7fd830",
   "metadata": {},
   "source": [
    "# Mahalonobis distance\n",
    "\n",
    "ุณูุงู! ๐\n",
    "\n",
    "ุฎู ุฎูุจู ฺฉู ูโุฎูุง ููุงุตู ูุฎุชูู ุฑู ุจูุฏ ุจุงุด. ูุงุตูู ูุงูุงูุงููุจุณ ููุท ฺฉ ุงุฒ ุงููุงุน ููุงุตู ูุณุช ฺฉู ุฏุฑ ุจุฑุฎ ูุณุงุฆู ุฎุงุต ูุซู ุชุญูู ุฏุงุฏูโูุง ุจุง ุงุจุนุงุฏ ูุฎุชูู ุง ุฏุงุฏูโูุง ุจุง ููุงุณโูุง ูุฎุชูู ุงุณุชูุงุฏู ูโุดู. ุฏุฑ ุงุฏุงูู ฺูุฏ ูุงุตูู ููู ุฏฺฏู ุฑู ุจูุช ูุนุฑู ูโฺฉูู ฺฉู ูโุชููู ููุฏ ุจุงุดู:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **ูุงุตูู ุงููุฏุณ (Euclidean Distance)**\n",
    "ุงู ูุงุตูู ฺฉ ุงุฒ ูุนุฑููโุชุฑู ู ูพุฑฺฉุงุฑุจุฑุฏุชุฑู ููุงุตู ูุณุช. ูุงุตูู ุจู ุฏู ููุทู ุฏุฑ ูุถุง ุฑู ุจู ุตูุฑุช ุฑุดู ูุฑุจุน ูุฌููุน ูุฑุจุนุงุช ุชูุงุถู ูุฎุชุตุงุชุดูู ูุญุงุณุจู ูโฺฉูู.\n",
    "\n",
    "**ูุฑููู:**\n",
    "$$ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **ูุงุตูู ูููุชู (Manhattan Distance)**\n",
    "ุงู ูุงุตูู ฺฏุงู \"ูุงุตูู ุจููฺฉ\" ูู ูุงูุฏู ูโุดู. ุฏุฑ ุงู ูุงุตููุ ููุท ูุฌููุน ูุฏุฑ ูุทูู ุชูุงุถู ูุฎุชุตุงุช ุฑู ูุญุงุณุจู ูโฺฉูู.\n",
    "\n",
    "**ูุฑููู:**\n",
    "$$ d(x, y) = \\sum_{i=1}^{n} |x_i - y_i| $$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **ูุงุตูู ฺุจุดู (Chebyshev Distance)**\n",
    "ุงู ูุงุตูู ุจุดุชุฑ ุฏุฑ ูุณุงุฆู ฺฉุงุฑุจุฑุฏ ุฏุงุฑู ฺฉู ุญุฑฺฉุช ุฏุฑ ุชูุงู ุฌูุงุช ฺฉุณุงูู (ูุซู ุญุฑฺฉุช ุดุงู ุฏุฑ ุดุทุฑูุฌ). ูุงุตูู ุจุฑุงุจุฑ ุจุง ุจุฒุฑฺฏุชุฑู ูุฏุฑ ูุทูู ุชูุงุถู ูุฎุชุตุงุช ูุณุช.\n",
    "\n",
    "**ูุฑููู:**\n",
    "$$ d(x, y) = \\max_{i} |x_i - y_i| $$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **ูุงุตูู ููฺฉููุณฺฉ (Minkowski Distance)**\n",
    "ุงู ูุงุตูู ฺฉ ูุฑู ุนููู ุงุฒ ูุงุตูู ุงููุฏุณ ู ูููุชู ูุณุช. ุจุง ุชุบุฑ ูพุงุฑุงูุชุฑ $ p $ ูโุชููู ุจู ุงู ุฏู ูุงุตูู ู ููุงุตู ุฏฺฏู ุฌุงุจูโุฌุง ุจุดู.\n",
    "\n",
    "**ูุฑููู:**\n",
    "$$ d(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p} $$\n",
    "\n",
    "- ุงฺฏุฑ $ p = 1 $: ูุงุตูู ูููุชู\n",
    "- ุงฺฏุฑ $ p = 2 $: ูุงุตูู ุงููุฏุณ\n",
    "- ุงฺฏุฑ $ p \\to \\infty $: ูุงุตูู ฺุจุดู\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **ูุงุตูู ููโุงูุฒูู (Cosine Similarity / Cosine Distance)**\n",
    "ุงู ูุงุตูู ุจุดุชุฑ ุจุฑุง ููุงุณู ุฌูุช ุฏู ุจุฑุฏุงุฑ (ูุซู ุฏู ูุชู ุง ุฏู ุชุตูุฑ) ุงุณุชูุงุฏู ูโุดู. ูุงุตูู ููโุงูุฒูู ุฑู ุจุง 1 ูููุง ุถุฑุจ ููุจุณุชฺฏ ฺฉุณููุณ ูุญุงุณุจู ูโฺฉูู.\n",
    "\n",
    "**ูุฑููู:**\n",
    "$$ \\text{Cosine Similarity} = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} $$\n",
    "$$ \\text{Cosine Distance} = 1 - \\text{Cosine Similarity} $$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **ูุงุตูู ููโุงูุฒูู ุฌูุชโุฏุงุฑ (Jaccard Distance)**\n",
    "ุงู ูุงุตูู ุจุดุชุฑ ุจุฑุง ูุฌููุนูโูุง ู ุฏุงุฏูโูุง ุจุงูุฑ (0 ู 1) ุงุณุชูุงุฏู ูโุดู. ูุงุตูู ุฌฺฉุงุฑุฏ ุจุฑุงุจุฑ ุจุง 1 ูููุง ุถุฑุจ ุดุจุงูุช ุฌฺฉุงุฑุฏ ูุณุช.\n",
    "\n",
    "**ูุฑููู:**\n",
    "$$ d(x, y) = 1 - \\frac{|x \\cap y|}{|x \\cup y|} $$\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **ูุงุตูู ููโุงูุฒูู ุงุญุชูุงู (Hellinger Distance)**\n",
    "ุงู ูุงุตูู ุจุฑุง ููุงุณู ุฏู ุชูุฒุน ุงุญุชูุงู ุงุณุชูุงุฏู ูโุดู. ูุซูุงู ุฏุฑ ุชุญูู ุชุตูุฑ ุง ุฏุงุฏูโูุง ุขูุงุฑ.\n",
    "\n",
    "**ูุฑููู:**\n",
    "$$ d(x, y) = \\frac{1}{\\sqrt{2}} \\sqrt{ \\sum_{i} ( \\sqrt{x_i} - \\sqrt{y_i} )^2 } $$\n",
    "\n",
    "---\n",
    "\n",
    "ุงฺฏุฑ ูโุฎูุง ฺฉ ุงุฒ ุงู ูุงุตููโูุง ุฑู ุจุง ฺฉุฏ ูพุงุฏูโุณุงุฒ ฺฉููุ ููุท ุจฺฏู ฺฉุฏูู ูุงุตูู ุฑู ูโุฎูุง ู ฺู ุฒุจุงู ุฑู ุฏูุณุช ุฏุงุฑ (ูุซู ูพุงุชูู ุง ุฌุงูุงุงุณฺฉุฑูพุช)ุ ูู ููุฑุงู ฺฉุฏุด ุฑู ูโููุณู ุจุฑุงุช! โจ\n",
    "\n",
    "ุงฺฏุฑ ุณูุงู ุฏฺฏู ุฏุงุดุชุ ุฎูุดุญุงู ูโุดู ฺฉูฺฉ ฺฉูู. ๐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
