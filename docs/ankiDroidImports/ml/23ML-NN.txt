#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6

23ML0001	Basic	23ML-NN	What are the three fundamental components of a single artificial neuron's forward computation?	1. Weighted sum: \(z = \sum_{i} w_i x_i + b\) (bias term, often absorbed as \(w_0\) with \(x_0=1\))<br>2. Activation: \(a = f(z)\)<br>3. Output: \(a\) (passed to next layer or used as prediction)<br><br>\(z = \sum_i w_i x_i + b, \quad a = f(z)\)
23ML0002	Basic	23ML-NN	Write the formula for the sigmoid activation function and its key property regarding output range. Is sigmoid output zero-centered?	The sigmoid function squashes real-valued input to a range (0, 1).<br><br>\(\sigma(z) = \frac{1}{1 + e^{-z}}\)<br><br><b>Zero-centered?</b> No. Sigmoid outputs are strictly positive (0 to 1). This causes gradient updates to move weights in the same sign direction, leading to inefficient zig-zagging optimization.<br><br>Property: \(\sigma(z) \in (0, 1)\), monotonic and smooth.
23ML0003	Basic	23ML-NN	Derive the compact formula for the derivative of the sigmoid function \(\sigma(z)\) in terms of \(\sigma(z)\) itself.	Let \(f = \sigma(z) = \frac{1}{1+e^{-z}}\).<br>Then \(\frac{df}{dz} = f \cdot (1 - f)\).<br><br>\(\sigma'(z) = \sigma(z) \cdot (1 - \sigma(z))\)<br><br>This form is computationally efficient as it reuses the forward pass value.
23ML0004	Basic	23ML-NN	Write the formula for tanh activation, its output range, zero-centered property, and its derivative.	Tanh is a scaled and shifted sigmoid, output range (-1, 1).<br><br>\(\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\sigma(2z) - 1\)<br><br><b>Attributes:</b> Zero-centered (outputs negative and positive values), mitigates the sign-constraint issue of sigmoid.<br><br><b>Derivative:</b><br><br>\(\frac{d}{dz}\tanh(z) = 1 - \tanh^2(z)\)
23ML0005	Basic	23ML-NN	Write the formula for ReLU, its derivative, and its key advantages/disadvantages.	Rectified Linear Unit (ReLU):<br><br>\(\text{ReLU}(z) = \max(0, z)\)<br><br><b>Derivative:</b><br><br>\(\text{ReLU}'(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}\)<br><br><b>Attributes:</b><br><br>- <b>Faster convergence:</b> Non-saturating gradient (1) for \(z>0\)<br>- <b>Computationally efficient:</b> Simple thresholding<br>- <b>Dead Neurons:</b> If \(z \leq 0\), gradient = 0; neuron never recovers (dying ReLU problem)
23ML0006	Basic	23ML-NN	Write the formula for ELU, its derivative, and its key advantages over ReLU.	Exponential Linear Unit (ELU):<br><br>\(\text{ELU}(z) = \begin{cases} z & \text{if } z > 0 \\ \alpha(e^z - 1) & \text{if } z \leq 0 \end{cases}\)<br><br>Where \(\alpha > 0\) is a hyperparameter (typically \(\alpha = 1.0\)).<br><br><b>Derivative:</b><br><br>\(\text{ELU}'(z) = \begin{cases} 1 & \text{if } z > 0 \\ \text{ELU}(z) + \alpha & \text{if } z \leq 0 \end{cases}\)<br><br><b>Attributes:</b> Smooth negative region, closer to zero-centered, reduces dead neurons, but computationally more expensive than ReLU.
23ML0007	Basic	23ML-NN	Describe Maxout activation, its parameter count, and its derivative.	Maxout generalizes ReLU by taking the maximum over \(k\) linear functions.<br><br>\(\text{Maxout}(z) = \max(z_1, z_2, ..., z_k)\)<br><br>Where each \(z_i = w_i^T x + b_i\).<br><br><b>Parameter Count:</b> \(k\) times more parameters than a standard neuron (each unit requires \(k\) weight vectors and \(k\) biases).<br><br><b>Derivative:</b><br><br>\(\frac{\partial}{\partial z_i}\text{Maxout} = \begin{cases} 1 & \text{if } z_i = \max(z_1,...,z_k) \\ 0 & \text{otherwise} \end{cases}\)<br><br><b>Attributes:</b> No saturation, no dead neurons, universal approximator of convex functions, but high parameter cost.
23ML0008	Basic	23ML-NN	Write the squared error loss function for a single training example.	Measures the squared difference between target \(y\) and predicted output \(o\).<br><br>\(E = \frac{1}{2}(y - o)^2\)<br><br>The \(\frac{1}{2}\) simplifies the derivative, canceling the factor of 2.
23ML0009	Basic	23ML-NN	Describe the perceptron algorithm, its activation function, and its decision boundary.	<b>Model:</b> \(f(x) = \text{sign}(w^T x + b)\), output ∈ {+1, -1}<br><br><b>Perceptron Criterion:</b> Minimize number of misclassifications.<br><br><b>Update Rule:</b> For each misclassified example \((x^i, y^i)\) where \(y^i \in \{+1, -1\}\):<br><br>- If \(w^T x^i > 0\) but \(y^i = -1\): \(w \leftarrow w - \eta x^i\)<br>- If \(w^T x^i < 0\) but \(y^i = +1\): \(w \leftarrow w + \eta x^i\)<br><br><b>Unified update:</b> \(w \leftarrow w + \eta \cdot y^i \cdot x^i\) (only for misclassified points)<br><br><b>Decision Boundary:</b> Linear hyperplane \(w^T x + b = 0\). Perceptron can only solve <b>linearly separable</b> problems.
23ML0010	Basic	23ML-NN	Write the perceptron cost function \(J_p(w)\) and its relationship to gradient descent.	The perceptron criterion is defined over misclassified samples \(M\):<br><br>\(J_p(w) = -\sum_{i \in M} y^i (w^T x^i)\)<br><br>Where \(y^i \in \{-1, +1\}\). For misclassified points, \(y^i(w^T x^i) < 0\), making the sum negative; the negative sign makes \(J_p(w) > 0\).<br><br><b>Gradient:</b><br><br>\(\nabla J_p(w) = -\sum_{i \in M} y^i x^i\)<br><br><b>Gradient Descent Update:</b><br><br>\(w \leftarrow w - \eta \nabla J_p(w) = w + \eta \sum_{i \in M} y^i x^i\)<br><br><b>Note:</b> Standard perceptron updates <b>per misclassified sample</b> (stochastic), not full batch gradient.
23ML0011	Basic	23ML-NN	State three key properties and conditions for perceptron convergence.	1. <b>Linear Separability:</b> Perceptron converges to zero training error <b>if and only if</b> the data is linearly separable.<br>2. <b>Update Frequency:</b> Perceptron updates weights <b>once per misclassified example</b> (not per epoch).<br>3. <b>Learning Rate:</b> With fixed \(\eta > 0\), convergence rate is independent of \(\eta\) (scales decision boundary). However, for non-separable data, \(\eta\) should be <b>decreased over iterations</b> to aid convergence.<br>4. <b>Output:</b> Perceptron uses <b>binary sign output</b> (±1), not probabilistic outputs.<br><br><b>Pitfall:</b> Perceptron does not converge on non-separable data; weights will oscillate.
23ML0012	Basic	23ML-NN	Decompose the gradient \(\frac{\partial E}{\partial w_{ij}}\) using the chain rule through pre-activation \(z_j\) and activation \(a_j\).	\(\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}\)<br><br>- \(\frac{\partial z_j}{\partial w_{ij}} = a_i\) (input from previous neuron)<br>- \(\frac{\partial a_j}{\partial z_j} = f'(z_j)\) (activation derivative)
23ML0013	Basic	23ML-NN	Define the local error signal \(\delta_j\) for neuron \(j\).	\(\delta_j\) represents the sensitivity of the total error \(E\) to the pre-activation input \(z_j\) of neuron \(j\).<br><br>\(\delta_j \equiv \frac{\partial E}{\partial z_j} = \frac{\partial E}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j}\)<br><br>This simplifies the weight gradient to a product of \(\delta_j\) and the input \(a_i\):<br><br>\(\frac{\partial E}{\partial w_{ij}} = \delta_j \cdot a_i\)
23ML0014	Basic	23ML-NN	State the Gradient Descent update rule for a weight \(w_{ij}\) using learning rate \(\eta\).	\(w_{ij}^{\text{new}} = w_{ij}^{\text{old}} - \eta \cdot \frac{\partial E}{\partial w_{ij}}\)<br><br>Equivalently, the weight change \(\Delta w_{ij}\) is:<br><br>\(\Delta w_{ij} = -\eta \cdot \delta_j \cdot a_i\)<br><br><b>Learning Rate Scheduling:</b> For convergence, \(\eta\) is often decreased over iterations (learning rate decay).
23ML0015	Basic	23ML-NN	Derive \(\delta_k\) for an output neuron \(k\).	For output neuron \(k\), \(a_k = o_k\). Using squared error loss:<br><br>1. \(\frac{\partial E}{\partial a_k} = -(y_k - a_k)\) (from \(E = \frac{1}{2}(y_k - a_k)^2\))<br>2. \(\frac{\partial a_k}{\partial z_k} = f'(z_k)\)<br><br>\(\delta_k = \frac{\partial E}{\partial a_k} \cdot \frac{\partial a_k}{\partial z_k} = -(y_k - a_k) \cdot f'(z_k)\)<br><br><i>Note: Some texts omit the negative sign and adjust the update rule; the core concept is the product of error and derivative.</i>
23ML0016	Basic	23ML-NN	Derive \(\delta_j\) for a hidden layer neuron \(j\) connected to downstream neurons \(k\).	Using the chain rule through all outgoing connections:<br><br>\(\delta_j = \frac{\partial E}{\partial z_j} = \sum_k \frac{\partial E}{\partial z_k} \cdot \frac{\partial z_k}{\partial z_j}\)<br><br>Since \(\frac{\partial z_k}{\partial z_j} = w_{jk} \cdot f'(z_j)\) and \(\delta_k = \frac{\partial E}{\partial z_k}\):<br><br>\(\delta_j = \left( \sum_k \delta_k \cdot w_{jk} \right) \cdot f'(z_j)\)
23ML0017	Basic	23ML-NN	List the 4 main steps of the backpropagation algorithm.	1. <b>Forward Pass:</b> Compute \(z_j\) and \(a_j\) for all neurons layer by layer.<br>2. <b>Output δ:</b> Compute \(\delta_k = (y_k - a_k) \cdot f'(z_k)\) for all output neurons.<br>3. <b>Backward Pass:</b> Propagate \(\delta\) backwards: \(\delta_j = (\sum_k \delta_k w_{jk}) \cdot f'(z_j)\) for hidden layers.<br>4. <b>Update Weights:</b> \(\Delta w_{ij} = -\eta \cdot \delta_j \cdot a_i\); update all weights.
23ML0018	Basic	23ML-NN	How are bias weights typically handled in the weight update equations?	A bias is treated as a weight \(w_{0j}\) connected to a constant input \(a_0 = +1\).<br><br>- Forward: \(z_j = \sum_{i=1}^n w_{ij} a_i + b_j = \sum_{i=0}^n w_{ij} a_i\) (with \(a_0=1\), \(w_{0j}=b_j\))<br>- Update: \(\Delta w_{0j} = -\eta \cdot \delta_j \cdot 1 = -\eta \cdot \delta_j\)
23ML0019	Basic	23ML-NN	Compare vanishing gradient susceptibility across activation functions.	<table><br>        <tr><br>            <th>Activation</th><br>            <th>Vanishing Risk</th><br>            <th>Mechanism</th><br>        </tr><br>        <tr><br>            <td><b>Sigmoid</b></td><br>            <td><b>High</b></td><br>            <td>Derivative ≤ 0.25; saturates at large|z|</td><br>        </tr><br>        <tr><br>            <td><b>Tanh</b></td><br>            <td><b>High</b></td><br>            <td>Derivative ≤ 1.0; saturates at large|z|</td><br>        </tr><br>        <tr><br>            <td><b>ReLU</b></td><br>            <td><b>Low</b> (z>0)</td><br>            <td>Gradient = 1 for z>0; 0 for z≤0</td><br>        </tr><br>        <tr><br>            <td><b>ELU</b></td><br>            <td><b>Low</b> (z>0)</td><br>            <td>Gradient = 1 for z>0; α for negative region</td><br>        </tr><br>        <tr><br>            <td><b>Maxout</b></td><br>            <td><b>Low</b></td><br>            <td>Gradient = 1 for max unit; piecewise linear</td><br>        </tr><br>    </table><br><br><b>Exploding Gradient:</b> Can occur with any activation if weights are large; mitigated by gradient clipping, batch norm, careful initialization.
23ML0020	Basic	23ML-NN	Explain the "dead neuron" (dying ReLU) problem and how ELU/Maxout address it.	<b>Dead ReLU:</b> When \(z \leq 0\), ReLU output = 0 and gradient = 0. If a neuron consistently receives negative input, it never updates and remains permanently inactive.<br><br><b>Mitigations:</b><br><br>- <b>ELU:</b> For \(z \leq 0\), output is \(\alpha(e^z - 1)\) with <b>non-zero gradient</b> \(\text{ELU}(z) + \alpha\), allowing recovery.<br>- <b>Maxout:</b> Never saturates; always selects a linear unit with gradient 1 for the winner.<br>- <b>Leaky ReLU:</b> Small positive slope (e.g., 0.01) for \(z<0\).
23ML0021	Basic	23ML-NN	How does parameter count relate to overfitting in neural networks, and how do activations affect this?	<b>Parameter Count:</b><br><br>- Each weight and bias is a trainable parameter.<br>- Fully connected layer: \((n_{in} + 1) \times n_{out}\) parameters.<br><br><b>Activation Impact:</b><br><br>- <b>Maxout:</b> \(k \times\) parameters of standard layer (significant overfitting risk).<br>- <b>ReLU/ELU/Tanh/Sigmoid:</b> No additional parameters.<br><br><b>Overfitting:</b><br><br>- High parameter count → high capacity → memorization risk on small data.<br>- <b>Regularization:</b> Dropout, weight decay, early stopping.<br>- <b>Architecture:</b> Parameter efficiency improves generalization.<br><br><b>Rule of thumb:</b> More parameters require more data or stronger regularization.
23ML0022	Basic	23ML-NN	Explain Dropout as an approximation to bagging (bootstrap aggregating).	<b>Dropout:</b> During training, randomly drop neurons with probability \(p\) (or keep with probability \(1-p\)).<br><br><b>As Bagging:</b><br><br>- Each forward/backward pass trains a different <b>thinned network</b> (unique subset of weights).<br>- Weight sharing across models (unlike true bagging with independent models).<br>- At test time, use <b>all neurons</b> with weights scaled by \(p\) (weight scaling inference rule).<br><br><b>Effect:</b><br><br>- Prevents co-adaptation of neurons.<br>- Reduces overfitting in high-parameter-count networks.<br>- Approximates ensemble averaging.
23ML0023	Basic	23ML-NN	Explain the vanishing gradient problem in the context of sigmoid activation.	When \(|z|\) is large, \(\sigma(z) \approx 0\) or \(1\), and \(\sigma'(z) = \sigma(z)(1-\sigma(z)) \approx 0\).<br><br>- <b>Pitfall:</b> Deep networks using sigmoid suffer from near-zero gradients in early layers (\(\delta_j\) shrinks during backpropagation).<br>- <b>Result:</b> Weights in early layers stop updating; network fails to learn long-range dependencies.<br>- <b>Mitigation:</b> Use ReLU, careful initialization, or batch normalization.<br><br>For Tanh: \(\tanh'(z) = 1 - \tanh^2(z) \leq 1.0\), also saturates at large |z| causing vanishing gradients.
23ML0024	Basic	23ML-NN	What happens when perceptron is trained on non-linearly separable data?	<b>Pitfall:</b> Perceptron does <b>not converge</b>; weights will oscillate indefinitely as decision boundary flips to correct different misclassified points.<br><br><b>Misconception:</b> Decreasing learning rate helps but does not guarantee convergence to zero error.<br><br><b>Truth:</b> Perceptron only guarantees convergence (to zero training error) <b>if data is linearly separable</b>. For non-separable data, use logistic regression or SVM with soft margin.
23ML0025	Basic	23ML-NN	What happens if all weights in a layer are initialized to the same constant value?	<b>Pitfall:</b> All neurons receive identical gradients and update identically. They remain symmetric and learn identical features.<br><br><b>Result:</b> Layer capacity collapses to single neuron.<br><br><b>Requirement:</b> Random initialization (Xavier/Glorot for sigmoid/tanh, He for ReLU) to break symmetry.
23ML0026	Basic	23ML-NN	Common sign convention error in \(\delta_k\) derivation.	- <b>Your note:</b> \(\delta_k = (y_k - a_k) \times f'(z_k)\) (No negative sign)<br>- <b>Standard:</b> \(\delta_k = \frac{\partial E}{\partial z_k} = -(y_k - a_k) \cdot f'(z_k)\)<br><br><b>Pitfall:</b> Inconsistency leads to gradient <b>ascent</b> if update rule is \(\Delta w = -\eta \delta a\).<br><br><b>Solution:</b> Be consistent. If using \(\delta_k = -(y_k - a_k)f'(z_k)\), then \(\Delta w = -\eta \delta a\) yields gradient descent.
23ML0027	Basic	23ML-NN	Common mistake when applying the derivative chain rule for \(\frac{\partial a_j}{\partial z_j}\).	- <b>Correct:</b> \(a_j = f(z_j)\), so \(\frac{\partial a_j}{\partial z_j} = f'(z_j)\).<br>- <b>Incorrect:</b> Mistaking \(f'(a_j)\) for \(f'(z_j)\). For sigmoid, \(f'(z) = f(z)(1-f(z)) = a(1-a)\), which is numerically equal but conceptually distinct.<br>- <b>Pitfall:</b> Miswriting the backpropagation formula for \(\delta_j\) as \((\sum \delta_k w_{jk}) \times f'(a_j)\) instead of \(f'(z_j)\).
23ML0028	Basic	23ML-NN	Distinguish between weight update frequency in SGD vs Batch GD in backprop.	- <b>Stochastic (SGD):</b> Update weights after every single training example. \(\Delta w = -\eta \delta_j a_i\).<br>- <b>Batch GD:</b> Accumulate gradients over entire dataset, then update once.<br>- <b>Mini-batch:</b> Average gradients over a subset.<br>- <b>Pitfall:</b> Using SGD notation but implementing batch averaging without adjusting learning rate or accumulation logic.
23ML0029	Basic	23ML-NN	Why is cross-entropy loss often preferred over squared error for sigmoid output units?	For sigmoid output and squared error, \(\delta_k = (y_k - a_k) \cdot a_k(1-a_k)\). When \(a_k\) is very wrong (close to 0 when \(y_k=1\)), the \((1-a_k)\) term saturates the gradient.<br><br>For cross-entropy loss \(E = -[y \ln a + (1-y) \ln(1-a)]\), the derivative cancels the sigmoid derivative:<br><br>\(\delta_k = a_k - y_k\)<br><br><b>Advantage:</b> Gradient is <b>proportional to error</b>, not attenuated by \(a_k(1-a_k)\). No vanishing gradient even at saturation.
23ML0030	Basic	23ML-NN	Why should learning rate often be decreased over iterations?	<b>Rationale:</b><br><br>- <b>Early training:</b> Large \(\eta\) for fast progress and escaping poor local minima.<br>- <b>Late training:</b> Small \(\eta\) for fine-tuning and convergence stability.<br><br><b>Methods:</b><br><br>- Step decay: Reduce by factor every \(k\) epochs.<br>- Exponential decay: \(\eta = \eta_0 e^{-kt}\)<br>- 1/t decay: \(\eta = \eta_0 / (1 + kt)\)<br><br><b>Perceptron context:</b> For non-separable data, decreasing \(\eta\) can help dampen oscillations, though zero error is not guaranteed.
23ML0031	Basic	23ML-NN	Why are zero-centered activations (tanh) preferred over non-zero-centered (sigmoid)?	<b>Sigmoid:</b> Outputs ∈ (0,1), always positive.<br><br>- Gradients for weights connected to a given neuron are <b>all same sign</b> (sign of \(\delta_j \times\) positive input).<br>- Causes zig-zagging optimization: weights must decrease/increase together before direction can reverse.<br><br><b>Tanh:</b> Outputs ∈ (-1,1), zero-centered.<br><br>- Gradients can be positive or negative.<br>- More efficient, stable optimization.<br><br><b>Modern solution:</b> Batch normalization centers activations, reducing dependence on activation function zero-centering.